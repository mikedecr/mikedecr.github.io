<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Non-Flat Implications of Flat Priors | Michael DeCrescenzo</title>
  <meta name="description" content="[SITE UNDER CONSTRUCTION.] I am a statistical researcher working in quantitative/high-frequency finance. This site contains some info about me, some notes about my work, and blog posts about stats and statistical programming.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Non-Flat Implications of Flat Priors" />
<meta property="og:description" content="Understanding the &ldquo;implied prior&rdquo; for functions of parameters" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/nonflat-implications/" />
<meta property="article:published_time" content="2020-06-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-06-30T00:00:00+00:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Non-Flat Implications of Flat Priors"/>
<meta name="twitter:description" content="Understanding the &ldquo;implied prior&rdquo; for functions of parameters"/>

  
  
    
  
  
  <link rel="stylesheet" href="/css/style-classic.css">
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="/images/favicon.ico" />

  
  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/about">About</a></li>
         
        <li><a href="/posts">Blog</a></li>
         
        <li><a href="/research">Public Research</a></li>
         
        <li><a href="/teaching">Teaching Resources</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" /posts/package-reinstall/">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=%2fposts%2fnonflat-implications%2f">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=%2fposts%2fnonflat-implications%2f&text=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=%2fposts%2fnonflat-implications%2f&is_video=false&description=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Non-Flat%20Implications%20of%20Flat%20Priors&body=Check out this article: %2fposts%2fnonflat-implications%2f">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.stumbleupon.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-stumbleupon " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://digg.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-digg " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=%2fposts%2fnonflat-implications%2f&name=Non-Flat%20Implications%20of%20Flat%20Priors&description=Understanding%20the%20%26ldquo%3bimplied%20prior%26rdquo%3b%20for%20functions%20of%20parameters">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=%2fposts%2fnonflat-implications%2f&t=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    <div id="toc">
      
    </div>
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        Non-Flat Implications of Flat Priors
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2020-06-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">2020-06-30</time>
          
        </div>
        
        <div class="article-category">
            <i class="fas fa-archive"></i>
            
            
            <a class="category-link" href="/categories/methods">Methods</a>
            
        </div>
        
        
        <div class="article-tag">
            <i class="fas fa-tag"></i>
            
            
            <a class="tag-link" href="/tags/computational-methods" rel="tag">Computational Methods</a>
            
             ,  
            <a class="tag-link" href="/tags/bayesian-statistics" rel="tag">Bayesian Statistics</a>
            
        </div>
        
      </div>
    </header>

  
    <div class="content" itemprop="articleBody">
      
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>Many researchers, when they’re introduced to Bayesian methods, are nervous about the possibility that their prior distributions will corrupt their posterior inferences.
Since they know that the posterior distribution is a precision-weighted average of the prior and the data (or “likelihood”), it initially makes sense to err toward a flatter, more diffuse prior density for model parameters.
These diffuse densities let the model put relatively more weight on data, which feels safer.</p>
<p>The purpose of this post is to highlight a few areas where this “default tendency” to use flat priors runs into unexpected consequences.
We show how functions of model parameters have <em>implied priors</em>: density functions of their own that inherit the prior uncertainty about the parameters that compose the function.
These implied priors can have strange shapes that you wouldn’t anticipate based on the raw parameters.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
We then show two cases where these strange shapes appear.
The first case comes from <a href="/publication/nonvoters">my own published work on voter ID in Wisconsin</a>.
The second case is a hypothetical experiment where we “accidentally” create a non-flat prior for the treatment effect in a randomized experiment where we weren’t expecting it.</p>
<p>Together, these exercises give concrete examples for the way flat priors and “uninformative” don’t necessarily mean the same thing.</p>
</div>
<div id="implied-priors" class="section level2">
<h2>Implied priors</h2>
<p>We’ll begin with a notion of the <em>implied prior</em>.
With some random variable <span class="math inline">\(\theta\)</span>, we can construct a function <span class="math inline">\(f\left(\theta\right)\)</span>, which is necessarily a random variable as well.
If <span class="math inline">\(\theta\)</span> has a probability distribution <span class="math inline">\(p\left(\theta\right)\)</span>, <span class="math inline">\(f\left(\theta\right)\)</span> will have some probability distribution <span class="math inline">\(p\left(f\left(\theta\right)\right)\)</span>.</p>
<p>A simple example.
Imagine some standard normal variable <span class="math inline">\(\nu\)</span> that is distributed <span class="math inline">\(\mathrm{Normal}\left(0, 1\right)\)</span> prior.
If we have some function <span class="math inline">\(f(\nu) = \mu + \sigma\nu\)</span>, then <span class="math inline">\(f(\nu)\)</span> will have a probability distribution.
In this case, it is straightforward to see that this prior would be <span class="math inline">\(\mathrm{Normal}(\mu, \sigma)\)</span>, but in more complicated examples it won’t be so easy to glean the implied prior directly.
We can create this example using code, setting <span class="math inline">\(\mu = 4\)</span> and <span class="math inline">\(\sigma = 2\)</span>, to reassure you that I’m telling the truth.</p>
<pre class="r"><code>library(&quot;tidyverse&quot;)
library(&quot;hrbrthemes&quot;)
library(&quot;latex2exp&quot;)
library(&quot;viridisLite&quot;)

theme_ipsum(base_family = &quot;Fira Sans&quot;) %+replace%
  theme(
    panel.grid.minor = element_blank(),
    axis.title.x.bottom = element_text(
      margin = margin(t = 0.35, unit = &quot;cm&quot;),
      size = rel(1.5)
    )
  ) %&gt;%
  theme_set()

accent &lt;- viridis(n = 1, begin = 0.5, end = 0.5)</code></pre>
<pre class="r"><code># set mu and sigma values
mu &lt;- 4
sigma &lt;- 2

# simulate nu and f(nu)
normal_example &lt;- 
  tibble(
    nu = rnorm(100000),
    f_nu = mu + (nu * sigma)
  ) %&gt;%
  print(n = 4) 
## # A tibble: 100,000 x 2
##       nu  f_nu
##    &lt;dbl&gt; &lt;dbl&gt;
## 1 -1.02  1.95 
## 2  0.908 5.82 
## 3 -1.72  0.556
## 4  0.184 4.37 
## # … with 99,996 more rows

# implied prior is Normal(mu, sigma)
ggplot(normal_example) +
  aes(x = f_nu) +
  geom_histogram(binwidth = .1, fill = accent) +
  geom_vline(
    xintercept = c(mu - sigma, mu + sigma),
    linetype = &quot;dashed&quot;,
    size = 0.25
  ) +
  geom_vline(xintercept = mu) +
  scale_x_continuous(
    breaks = seq(mu - 5*sigma, mu + 5*sigma, sigma)
  ) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = &quot;Implied Prior for Transformed Normal&quot;,
    subtitle = &quot;Histogram of prior samples&quot;,
    x = TeX(&quot;$f(\\nu) = \\mu + \\sigma\\nu, \\; \\nu \\sim N(0, 1)$&quot;),
    y = NULL
  )</code></pre>
<p><img src="/posts/nonflat-implied-priors/index_files/figure-html/implied-normal-1.png" width="100%" /></p>
<p>Bayesians will recognize this as a “non-centered parameterization” of a Normal distribution, or a normal distribution that sneaks the mean and standard deviation values out of the random variable.
Bayesian modelers invoke this trick all the time in hierarchical models, since sampling <span class="math inline">\(\nu\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma\)</span> separately is easier for a computer to do than sampling a distribution for <span class="math inline">\(f(\nu)\)</span> that itself contains <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.
Parameterizations that de-correlate these parameters are generally easier to sample <em>and</em>, conveniently, more manageable to set priors for.</p>
</div>
<div id="flat-priors-meet-nonlinear-transformations" class="section level2">
<h2>Flat priors meet nonlinear transformations</h2>
<p>Suppose we have a parameter <span class="math inline">\(\pi\)</span> has a flat prior in the <span class="math inline">\([0, 1]\)</span> interval, and we calculate some function <span class="math inline">\(g(\pi)\)</span>.
Will <span class="math inline">\(g(\pi)\)</span> have a flat distribution?
It depends.</p>
<p>I encountered an example of this in my work with Ken Mayer on <a href="/publication/nonvoters">voter ID in Wisconsin</a> in Wisconsin,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
although I didn’t put this lesson about prior flatness in the paper.
We wanted to estimate the <em>number of eligible registered voters</em> in two Wisconsin counties for whom Wisconsin’s voter ID requirement hindered their voting in 2016.
This estimate contains three ingredients.</p>
<ul>
<li><span class="math inline">\(N\)</span>: the number of individual records in the registered voter file for the target population.
This is an observed constant.</li>
<li><span class="math inline">\(\epsilon\)</span>: the proportion of the population in the voter file that was eligible to vote in 2016.
This is relevant because voter files contain individuals who moved, died, or who were otherwise ineligible, that have to be cleaned out of voter files periodically.
The population size must be penalized by <span class="math inline">\(\epsilon\)</span> to remove these ineligible records from our estimate.
This is a random variable, estimated by coding a finite sample of the voter file.</li>
<li><span class="math inline">\(\pi\)</span>: the proportion of eligible registrants who experienced ID-related difficulty voting.
This is a random variable, estimated using a survey sample of registrants in the voter file.</li>
</ul>
<p>The quantity we want to estimate is <span class="math inline">\(N\times \epsilon \times \pi\)</span>, an eligibility-penalized population estimate for the number of voters affected by the voter ID requirement.
Suppose that we know <span class="math inline">\(N = 229,625\)</span> from the voter file, but <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\epsilon\)</span> are proportions that must be estimated.
We give each proportion a flat <span class="math inline">\(\mathrm{Beta}(1, 1)\)</span> prior on the <span class="math inline">\([0, 1]\)</span> interval.
What is our implied prior for the population estimate?</p>
<pre class="r"><code>tibble(
  pi = rbeta(10000, 1, 1),
  epsilon = rbeta(10000, 1, 1),
  N = 229625,
  pop_estimate = N * epsilon * pi
) %&gt;%
  ggplot() +
  aes(x = pop_estimate) +
  geom_histogram(fill = accent, bins = 100, boundary = 0) +
  labs(
    title = &quot;Implied Prior for Population Estimate&quot;,
    subtitle = TeX(&quot;Histogram of prior simulations&quot;),
    x = TeX(&quot;Population estimate $= N \\epsilon \\pi$&quot;),
    y = NULL
  ) +
  scale_x_continuous(labels = scales::comma)</code></pre>
<p><img src="/posts/nonflat-implied-priors/index_files/figure-html/voter-id-1.png" width="100%" /></p>
<p>Seriously, what?
If I had plopped this graphic into my paper and said that it was my prior for this population quantity, I would have been in trouble.
Look at how swoopy that prior looks!
How can that be an uninformative prior?
Well, we know that the random components have vague priors, so this is the natural result of sending these parameters through a nonlinear function.
If you don’t like it, you should think about how this quantity is parameterized and whether some other priors make more sense.</p>
<p>Bayesians may recognize this feature of prior distributions as well, where nonlinear functions of parameters have a density that does not simply reflect a shifting or scaling of the original density.
This happens because nonlinear transformations of parameters can “squish” or “stretch” probability mass into different areas/volumes than they were previously, thereby changing probability density.
If we wanted to write out the new density, we would need to start with the old density and use (spooky voice) <em><strong>the Jacobian</strong></em>.</p>
</div>
<div id="honey-i-shrunk-my-treatment-effect" class="section level2">
<h2>Honey, I shrunk my treatment effect</h2>
<p>When we think about causal inference, we are thinking about methods that want to be light on their assumptions.
If we imagine a Bayesian interpretation of an experiment (even if we don’t specify the Bayesian <em>model</em> per se), it makes sense that we want vague priors on important quantities in order to “let the data speak” instead of deriving results from the prior.
This turns out to be less straightforward than you would think.</p>
<p>Imagine an experiment where we treat individuals with an advertisement or we don’t, <span class="math inline">\(z_{i} \in \{0, 1\}\)</span>, and then we measure whether they intend to vote for the Democratic candidate or not, <span class="math inline">\(y_{i} \in \{0, 1\}\)</span>.
My treatment effect <span class="math inline">\(\tau\)</span> is the comparison between the Democratic vote proportion in the control group, <span class="math inline">\(\mu_{z = 0}\)</span>, and the Democratic vote proportion in the treatment group, <span class="math inline">\(\mu_{z = 1}\)</span>.
<span class="math display">\[\begin{align}
  \tau = \mu_{1} - \mu_{0}
\end{align}\]</span></p>
<p>If we estimate this effect with a linear model, we have a choice about how to parameterize the regression function.
We could use a constant and a treatment effect with error term <span class="math inline">\(u_{i}\)</span>,
<span class="math display">\[\begin{align}
  y_{i} = \mu_{0} + \tau z_{i} + u_{i}  
\end{align}\]</span>
or we have two intercepts for each condition.
<span class="math display">\[\begin{align}
  y_{i} = \mu_{z[i]} + u_{i}
\end{align}\]</span>
Bayesians think it makes more sense to use the second parameterization.
Why?
If we want to set the same prior on both groups, it’s easier to do that when we can directly set priors on each mean instead of a prior on one mean and then a prior on the difference in means.
So let’s take that approach, giving each vote share a flat prior that says any vote share for both groups is <em>a priori</em> equally likely. I will write these as flat Beta densities, but you could imagine them as standard Uniform densities as well.
<span class="math display">\[\begin{align}
  \mu_{1}, \mu_{0} \sim \mathrm{Beta}(1, 1)
\end{align}\]</span>
What is the prior for the treatment effect (the difference-in-means)?
Not flat!
We will again simulate to see the effect of combining parameters into a single function.</p>
<pre class="r"><code># simulate means and calculate difference
tibble(
  mu_0 = rbeta(100000, 1, 1), 
  mu_1 = rbeta(100000, 1, 1),
  trt = mu_1 - mu_0 
) %&gt;%
  ggplot() +
  aes(x = trt) +
  geom_histogram(fill = accent, binwidth = .05, boundary = 0) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = &quot;Implied Prior for Treatment Effect&quot;,
    subtitle = &quot;Histogram of prior samples&quot;,
    x = &quot;Difference in means&quot;,
    y = NULL
  )</code></pre>
<p><img src="/posts/nonflat-implied-priors/index_files/figure-html/rct-1.png" width="100%" /></p>
<p>What happened to my vague prior beliefs?
Why do I have this non-flat prior for something I thought I wanted to have vague information for?<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>It still is a vague prior, but we’re wrong to expect it to be flat.
Why?
Well, averaging over my prior uncertainty in both groups, my expected difference in means ought to be <em>mean</em> zero (natch).
But more than that, the reason why we get a <em>mode</em> at zero is that there are many more ways to produce differences near zero with my raw means than ways to produce differences far from zero.
The only way to get big differences (near <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>) is for both means to be simultaneously far apart, which isn’t as likely to happen randomly with a flat prior on each group mean.
When we think about the treatment effect prior in this way, we can understand why this actually feels <em>less informed</em> than a direct flat prior for the treatment effect.
Putting a flat prior on the treatment effect is saying that we think big differences are just as likely as small differences.
This is like a prior that says my group means should be negatively correlated, effectively upweighting bigger differences from what we’d otherwise expect.
Weird!
I’d rather set reasonable priors for my means and let my treatment prior do what it do.</p>
</div>
<div id="flat-priors-often-have-non-flat-implications" class="section level2">
<h2>Flat priors often have non-flat implications</h2>
<p>These implications feel strange at first, but they are all around us whether or not we notice them.
The flatness of a prior (or any shape, flat or not) is a <em>relative</em> feature of a model parameterization or a quantity of interest, not an <em>absolute</em> one.
Inasmuch as we believe priors are at work even when we don’t want to think about them—i.e. we accept Bayesian models as generalizations of likelihood models—we should respect how transforming a likelihood affects <a href="https://www.mdpi.com/1099-4300/19/10/555">which parameters are exposed to the researcher, and which spaces those parameters are defined in</a>.
We should know that flat doesn’t imply uninformative, and that non-flat doesn’t imply informative.
What we’re seeing here is that flatness begets non-flatness in tons of circumstances, and that’s totally ordinary.
And more examples of how prior predictive checks show us what our model thinks about key quantities of interest.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>
These strange shapes tend to be extremely useful in practice.
For example, it is straightforward to create Bayesian versions of “L1” and “L2” by combining parameters with particular densities.
Topic for a future post maybe.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>
Or see the <a href="https://www.liebertpub.com/doi/10.1089/elj.2018.0536">published version</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>
You’ve probably seen this phenomenon previously in our stats education.
If you keep adding and subtracting more uniform variables, we would approach a Normal distribution.<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>

    </div>
  </article>

  
  




  
    <div class="blog-post-comments">
        <div id="disqus_thread">
          <script type="text/javascript">
          
          (function() {
              
              
              
              
          
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              var disqus_shortname = 'example';
              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          <a href="https://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
    </div>

  


  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about">About</a></li>
         
          <li><a href="/posts">Blog</a></li>
         
          <li><a href="/research">Public Research</a></li>
         
          <li><a href="/teaching">Teaching Resources</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      
    </div>

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=%2fposts%2fnonflat-implications%2f">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=%2fposts%2fnonflat-implications%2f&text=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=%2fposts%2fnonflat-implications%2f&is_video=false&description=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Non-Flat%20Implications%20of%20Flat%20Priors&body=Check out this article: %2fposts%2fnonflat-implications%2f">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.stumbleupon.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://digg.com/submit?url=%2fposts%2fnonflat-implications%2f&title=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-digg fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=%2fposts%2fnonflat-implications%2f&name=Non-Flat%20Implications%20of%20Flat%20Priors&description=Understanding%20the%20%26ldquo%3bimplied%20prior%26rdquo%3b%20for%20functions%20of%20parameters">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=%2fposts%2fnonflat-implications%2f&t=Non-Flat%20Implications%20of%20Flat%20Priors">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left" align=right>
    Copyright  &copy; 2020  Michael DeCrescenzo 
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>


  


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</html>
