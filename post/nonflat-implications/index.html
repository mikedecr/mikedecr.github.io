
<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Michael DeCrescenzo">

  
  
  
    
  
  <meta name="description" content="Understanding the &#34;implied prior&#34; for functions of parameters">

  
  <link rel="alternate" hreflang="en-us" href="/post/nonflat-implications/">

  


  

  
  
  
  <meta name="theme-color" content="#268bd2">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Serif+Pro:400,700|Fira+Sans:500|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.dd75c6b920d285055d76459426bdfc45.css">

  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/nonflat-implications/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@mikedecr">
  <meta property="twitter:creator" content="@mikedecr">
  
  <meta property="og:site_name" content="Michael DeCrescenzo">
  <meta property="og:url" content="/post/nonflat-implications/">
  <meta property="og:title" content="Non-Flat Implications of Flat Priors | Michael DeCrescenzo">
  <meta property="og:description" content="Understanding the &#34;implied prior&#34; for functions of parameters"><meta property="og:image" content="/img/circular-telles.png">
  <meta property="twitter:image" content="/img/circular-telles.png"><meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-06-30T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2020-06-30T00:00:00&#43;00:00">
  

  


  





  <title>Non-Flat Implications of Flat Priors | Michael DeCrescenzo</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/"><img src="/img/circular-telles.png" alt="Michael DeCrescenzo"></a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/teaching"><span>Teaching</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/code"><span>Code Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Non-Flat Implications of Flat Priors</h1>

  
  <p class="page-subtitle">Understanding the &ldquo;implied prior&rdquo; for functions of parameters</p>
  

  
    



<meta content="2020-06-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2020-06-30 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemprop="author name" itemtype="http://schema.org/Person"><a href="/authors/admin/">Michael DeCrescenzo</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>2020-06-30</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    <a href="/categories/methods/">Methods</a></span>
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>Many researchers, when they’re introduced to Bayesian methods, are nervous about the possibility that their prior distributions will corrupt their posterior inferences.
Since they know that the posterior distribution is a precision-weighted average of the prior and the data (or “likelihood”), it initially makes sense to err toward a flatter, more diffuse prior density for model parameters.
These diffuse densities let the model put relatively more weight on data, which feels safer.</p>
<p>The purpose of this post is to highlight a few areas where this “default tendency” to use flat priors runs into unexpected consequences.
We show how functions of model parameters have <em>implied priors</em>: density functions of their own that inherit the prior uncertainty about the parameters that compose the function.
These implied priors can have strange shapes that you wouldn’t anticipate based on the raw parameters.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
We then show two cases where these strange shapes appear.
The first case comes from <a href="/publication/nonvoters">my own published work on voter ID in Wisconsin</a>.
The second case is a hypothetical experiment where we “accidentally” create a non-flat prior for the treatment effect in a randomized experiment where we weren’t expecting it.</p>
<p>Together, these exercises give concrete examples for the way flat priors and “uninformative” don’t necessarily mean the same thing.</p>
</div>
<div id="implied-priors" class="section level2">
<h2>Implied priors</h2>
<p>We’ll begin with a notion of the <em>implied prior</em>.
With some random variable <span class="math inline">\(\theta\)</span>, we can construct a function <span class="math inline">\(f\left(\theta\right)\)</span>, which is necessarily a random variable as well.
If <span class="math inline">\(\theta\)</span> has a probability distribution <span class="math inline">\(p\left(\theta\right)\)</span>, <span class="math inline">\(f\left(\theta\right)\)</span> will have some probability distribution <span class="math inline">\(p\left(f\left(\theta\right)\right)\)</span>.</p>
<p>A simple example.
Imagine some standard normal variable <span class="math inline">\(\nu\)</span> that is distributed <span class="math inline">\(\mathrm{Normal}\left(0, 1\right)\)</span> prior.
If we have some function <span class="math inline">\(f(\nu) = \mu + \sigma\nu\)</span>, then <span class="math inline">\(f(\nu)\)</span> will have a probability distribution.
In this case, it is straightforward to see that this prior would be <span class="math inline">\(\mathrm{Normal}(\mu, \sigma)\)</span>, but in more complicated examples it won’t be so easy to glean the implied prior directly.
We can create this example using code, setting <span class="math inline">\(\mu = 4\)</span> and <span class="math inline">\(\sigma = 2\)</span>, to reassure you that I’m telling the truth.</p>
<pre class="r"><code>library(&quot;tidyverse&quot;)
library(&quot;hrbrthemes&quot;)
library(&quot;latex2exp&quot;)
library(&quot;viridisLite&quot;)

theme_ipsum(base_family = &quot;Fira Sans&quot;) %+replace%
  theme(
    panel.grid.minor = element_blank(),
    axis.title.x.bottom = element_text(
      margin = margin(t = 0.35, unit = &quot;cm&quot;),
      size = rel(1.5)
    )
  ) %&gt;%
  theme_set()
accent &lt;- viridis(n = 1, begin = 0.5, end = 0.5)</code></pre>
<pre class="r"><code># set mu and sigma values
mu &lt;- 4
sigma &lt;- 2

# simulate nu and f(nu)
normal_example &lt;- 
  tibble(
    nu = rnorm(100000),
    f_nu = mu + (nu * sigma)
  ) %&gt;%
  print(n = 4) 
## # A tibble: 100,000 x 2
##        nu  f_nu
##     &lt;dbl&gt; &lt;dbl&gt;
## 1  0.669   5.34
## 2 -0.448   3.10
## 3  0.0987  4.20
## 4  0.591   5.18
## # … with 99,996 more rows

# implied prior is Normal(mu, sigma)
ggplot(normal_example) +
  aes(x = f_nu) +
  geom_histogram(binwidth = .1, fill = accent) +
  geom_vline(
    xintercept = c(mu - sigma, mu + sigma),
    linetype = &quot;dashed&quot;,
    size = 0.25
  ) +
  geom_vline(xintercept = mu) +
  scale_x_continuous(
    breaks = seq(mu - 5*sigma, mu + 5*sigma, sigma)
  ) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = &quot;Implied Prior for Transformed Normal&quot;,
    subtitle = &quot;Histogram of prior samples&quot;,
    x = TeX(&quot;$f(\\nu) = \\mu + \\sigma\\nu, \\; \\nu \\sim N(0, 1)$&quot;),
    y = NULL
  )</code></pre>
<p><img src="/post/nonflat-implied-priors/index_files/figure-html/implied-normal-1.png" width="100%" /></p>
<p>Bayesians will recognize this as a “non-centered parameterization” of a Normal distribution, or a normal distribution that sneaks the mean and standard deviation values out of the random variable.
Bayesian modelers invoke this trick all the time in hierarchical models, since sampling <span class="math inline">\(\nu\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma\)</span> separately is easier for a computer to do than sampling a distribution for <span class="math inline">\(f(\nu)\)</span> that itself contains <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.
Parameterizations that de-correlate these parameters are generally easier to sampler <em>and</em>, conveniently, more manageable to set priors for.</p>
</div>
<div id="flat-priors-meet-nonlinear-transformations" class="section level2">
<h2>Flat priors meet nonlinear transformations</h2>
<p>Suppose we have a parameter <span class="math inline">\(\pi\)</span> has a flat prior in the <span class="math inline">\([0, 1]\)</span> interval, and we calculate some function <span class="math inline">\(g(\pi)\)</span>.
Will <span class="math inline">\(g(\pi)\)</span> have a flat distribution?
It depends.</p>
<p>I encountered an example of this in my work with Ken Mayer on <a href="/publication/nonvoters">voter ID in Wisconsin</a> in Wisconsin,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
although I didn’t put this lesson about prior flatness in the paper.
We wanted to estimate the <em>number of eligible registered voters</em> in two Wisconsin counties for whom Wisconsin’s voter ID requirement hindered their voting in 2016.
This estimate contains three ingredients.</p>
<ul>
<li><span class="math inline">\(N\)</span>: the number of individual records in the registered voter file for the target population.</li>
<li><span class="math inline">\(\epsilon\)</span>: the proportion of the population in the voter file that was eligible to vote in 2016.
This is relevant because voter files contain individuals who moved, died, or who were otherwise ineligible, that have to be cleaned out of voter files periodically.
The population size must be penalized by <span class="math inline">\(\epsilon\)</span> to remove these ineligible records from our estimate.
Estimated by coding a finite sample of the voter file.</li>
<li><span class="math inline">\(\pi\)</span>: the proportion of eligible registrants who experienced ID-related difficulty voting.
Estimated using a survey sample of registrants in the voter file.</li>
</ul>
<p>The quantity we want to estimate is <span class="math inline">\(N\epsilon\pi\)</span>, an eligibility-penalized population estimate for the number of voters affected by the voter ID requirement.
Suppose that we know <span class="math inline">\(N = 229,625\)</span> from the voter file, but <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\epsilon\)</span> are proportions that must be estimated.
We give each proportion a flat <span class="math inline">\(\mathrm{Beta}(1, 1)\)</span> prior on the <span class="math inline">\([0, 1]\)</span> interval.
What is our implied prior for the population estimate?</p>
<pre class="r"><code>tibble(
  pi = rbeta(10000, 1, 1),
  epsilon = rbeta(10000, 1, 1),
  N = 229625,
  pop_estimate = N * epsilon * pi
) %&gt;%
  ggplot() +
  aes(x = pop_estimate) +
  geom_histogram(fill = accent, bins = 100, boundary = 0) +
  labs(
    title = &quot;Implied Prior for Population Estimate&quot;,
    subtitle = TeX(&quot;Histogram of prior simulations&quot;),
    x = TeX(&quot;Population estimate $= N \\epsilon \\pi$&quot;),
    y = NULL
  ) +
  scale_x_continuous(labels = scales::comma)</code></pre>
<p><img src="/post/nonflat-implied-priors/index_files/figure-html/voter-id-1.png" width="100%" /></p>
<p>Seriously, what?
If I had plopped this graphic into my paper and said that it was my prior for this population quantity, I would have been in trouble.
Look at how swoopy that prior looks!
How can that be an uninformative prior?
Well, we know that the random components have vague priors, so this is the natural result of sending these parameters through a nonlinear function.
If you don’t like it, you should think about how this quantity is parameterized and whether some other priors make more sense.</p>
<p>Bayesians may recognize this feature of prior distributions as well, where nonlinear functions of parameters have a density that does not simply reflect a shifting or scaling of the original density.
This happens because nonlinear transformations of parameters can “squish” or “stretch” probability mass into different areas/volumes than they were previously, thereby changing probability density.
If we wanted to write out the new density, we would need to start with the old density and use (spooky voice) <em><strong>the Jacobian</strong></em>.</p>
</div>
<div id="honey-i-shrunk-my-treatment-effect" class="section level2">
<h2>Honey, I shrunk my treatment effect</h2>
<p>When we think about causal inference, we are thinking about methods that want to be light on their assumptions.
If we imagine a Bayesian interpretation of this experiment (even if we don’t specify the Bayesian <em>model</em> per se), it makes sense that we want vague priors on important quantities in order to “let the data speak” instead of deriving results from the prior.
This turns out to be less straightforward than you would think.</p>
<p>Imagine an experiment where we treat individuals with an advertisement or we don’t, <span class="math inline">\(z_{i} \in \{0, 1\}\)</span>, and then we measure whether they intend to vote for the Democratic candidate or not, <span class="math inline">\(y_{i} \in \{0, 1\}\)</span>.
My treatment effect <span class="math inline">\(\tau\)</span> is the comparison between the Democratic vote proportion in the control group, <span class="math inline">\(\mu_{z = 0}\)</span>, and the Democratic vote proportion in the treatment group, <span class="math inline">\(\mu_{z = 1}\)</span>.
<span class="math display">\[\begin{align}
  \tau = \mu_{1} - \mu_{0}
\end{align}\]</span></p>
<p>If we estimate this effect with a linear model, we have a choice about how to parameterize the regression function.
We could use a constant and a treatment effect with error term <span class="math inline">\(u_{i}\)</span>,
<span class="math display">\[\begin{align}
  y_{i} = \mu_{0} + \tau z_{i} + u_{i}  
\end{align}\]</span>
or we have two intercepts for each condition.
<span class="math display">\[\begin{align}
  y_{i} = \mu_{z[i]} + u_{i}
\end{align}\]</span>
Bayesians think it makes more sense to use the second parameterization.
Why?
If we want to set the same prior on both groups, it’s easier to do that when we can directly set priors on each mean instead of on one mean and the difference-in-means.
So let’s take that approach, giving each vote share a flat prior that says any vote share for both groups is <em>a priori</em> equally likely. I will write these as flat Beta densities, but you could imagine them as standard Uniform densities as well.
<span class="math display">\[\begin{align}
  \mu_{1}, \mu_{0} \sim \mathrm{Beta}(1, 1)
\end{align}\]</span>
What is the prior for the treatment effect (the difference-in-means)?
Not flat!
We will again simulate to see the effect of combining parameters into a single function.</p>
<pre class="r"><code># simulate means and calculate difference
tibble(
  mu_0 = rbeta(100000, 1, 1), 
  mu_1 = rbeta(100000, 1, 1),
  trt = mu_1 - mu_0 
) %&gt;%
  ggplot() +
  aes(x = trt) +
  geom_histogram(fill = accent, binwidth = .05, boundary = 0) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = &quot;Implied Prior for Treatment Effect&quot;,
    subtitle = &quot;Histogram of prior samples&quot;,
    x = &quot;Difference in means&quot;,
    y = NULL
  )</code></pre>
<p><img src="/post/nonflat-implied-priors/index_files/figure-html/rct-1.png" width="100%" /></p>
<p>What happened to my vague prior beliefs?
Why do I have this non-flat prior for something I thought I wanted to have vague information for?<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>It still is a vague prior, but we’re wrong to expect it to be flat.
Why?
Well, averaging over my prior uncertainty in both groups, my expected difference in means ought to be <em>mean</em> zero (natch).
But more than that, the reason why we get a <em>mode</em> at zero there are many more to produce differences near zero with my raw means than differences far from zero.
The only way to get big differences (near <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>) is for both means to be far apart, which isn’t as likely to happen randomly as two means that are closer together in the prior.
When we think about the treatment effect prior in this way, we can understand why this actually feels <em>less informed</em> than a direct flat prior for the treatment effect.
Putting a flat prior on the treatment effect is saying that we think big differences are just as likely as small differences.
This is like a prior that says my group means should be negatively correlated, effectively upweighting bigger differences from what we’d otherwise expect.
Weird!
I’d rather set reasonable priors for my means and let my treatment prior do what it do.</p>
</div>
<div id="flat-priors-often-have-non-flat-implications" class="section level2">
<h2>Flat priors often have non-flat implications</h2>
<p>These implications feel strange at first, but they are all around us whether or not we notice them.
The flatness of a prior (or any shape, flat or not) is a <em>relative</em> feature of a model parameterization or a quantity of interest, not an <em>absolute</em> one.
Inasmuch as we believe priors are at work even when we don’t want to think about them—i.e. we accept Bayesian models as generalizations of likelihood models—we should respect how transforming a likelihood affects <a href="https://www.mdpi.com/1099-4300/19/10/555">which parameters are exposed to the researcher, and which spaces those parameters are defined in</a>.
We should know that flat doesn’t imply uninformative, and that non-flat doesn’t imply informative.
What we’re seeing here is that flatness begets non-flatness in tons of circumstances, and that’s totally ordinary.
And more examples of how prior predictive checks show us what our model thinks about key quantities of interest.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>
These strange shapes tend to be extremely useful in practice.
For example, it is straightforward to create Bayesian versions of “L1” and “L2” by combining parameters with particular densities.
Topic for a future post maybe.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>
Or see the <a href="https://www.liebertpub.com/doi/10.1089/elj.2018.0536">published version</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>
You’ve probably seen this phenomenon previously in our stats education.
If you keep adding and subtracting more uniform variables, we would approach a Normal distribution.<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/computational-methods/">Computational Methods</a>
  
  <a class="badge badge-light" href="/tags/bayesian-statistics/">Bayesian Statistics</a>
  
</div>



    
      








  
  
    
  
  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar-grace_hu05a9fa9b849008cfd0c58ea39efc7f26_83291_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/">Michael DeCrescenzo</a></h5>
      <h6 class="card-subtitle">Ph.D. Candidate, Political Science</h6>
      <p class="card-text" itemprop="description">U.S. Electoral Politics, Bayesian Analysis, Open Source Research</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="/#contact" >
              <i class="fas fa-paper-plane"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/mikedecr" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://twitter.com/mikedecr" target="_blank" rel="noopener">
              <i class="fab fa-twitter"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/unavoidable-modeling/">Avoidable and Unavoidable Modeling</a></li>
          
          <li><a href="/post/learning-splines/">Teaching Myself Splines In Public</a></li>
          
          <li><a href="/post/bayes-mediation/">Causal Mediation, Bayesianly</a></li>
          
          <li><a href="/post/package-reinstall/">How do I reinstall my packages for R 4.0 when many of them came from Github?</a></li>
          
          <li><a href="/post/visualizing-what-matters/">Plotting What Matters</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/tex.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/bib.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/txt.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.23e2fabfaf271b1b075bb9e0280085b7.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    <a href="/privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    © Michael DeCrescenzo 2020 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>
  
  
<script type="text/javascript">
var sc_project=11382424; 
var sc_invisible=1; 
var sc_security="647b3843"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="free hit
counter" href="https://statcounter.com/"
target="_blank"><img class="statcounter"
src="https://c.statcounter.com/11382424/0/647b3843/1/"
alt="free hit counter"></a></div></noscript>


</body>
</html>
