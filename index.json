[{"authors":["admin"],"categories":null,"content":"I am a (very new) quantitative researcher at DRW Holdings, which means I use statistics and machine learning to analyze financial markets.\nI hold a Ph.D. in political science from the University of Wisconsin–Madison. My dissertation developed Bayesian statistical techniques for measuring political ideology in the mass electorate and its role in congressional primary elections. Other research focuses on voter identification requirements and the dynamics of intergroup voting blocs. My research is published in the Election Law Journal and The Forum and has been covered by the New York Times, the Washington Post, FiveThirtyEight, and more.\n","date":1593475200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1605378230,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a (very new) quantitative researcher at DRW Holdings, which means I use statistics and machine learning to analyze financial markets.\nI hold a Ph.D. in political science from the University of Wisconsin–Madison. My dissertation developed Bayesian statistical techniques for measuring political ideology in the mass electorate and its role in congressional primary elections. Other research focuses on voter identification requirements and the dynamics of intergroup voting blocs. My research is published in the Election Law Journal and The Forum and has been covered by the New York Times, the Washington Post, FiveThirtyEight, and more.","tags":null,"title":"Michael DeCrescenzo","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1561762571,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Welcome to the online lessons for PS 811: Introduction to Statistical Computing for Political Science. This series of posts contain instructional notes for using R for political science research.\nThis current post contains a few introductory remarks about R, how I intend to teach R, and the structure of our coming lessons.\nSchedule: Read this before our first in-class R lesson!\nA little about R R is not the only program out there for doing statistical programming, so let\u0026rsquo;s begin by talking about what makes it unique.\nCompared to other popular statistical software for the social scientists (such as Stata), R stands out as a more general programming language. It isn\u0026rsquo;t just a set of off-the-shelf commands that are punched into a program. It is a collection of functions and routines that can be used to build more complex and powerful tools for a broader set of computational uses. This has some trade-offs; it\u0026rsquo;s more complicated and tougher to learn, but you can do a lot more with it.\nR is an \u0026ldquo;object-oriented\u0026rdquo; language, which essentially means that everything you interact with\u0026mdash;datasets, variables, variable names, functions, etc.\u0026mdash;is an \u0026ldquo;object\u0026rdquo; that contains either data or code to manipulate data. This makes R extremely flexible for use in statistics, as we will see throughout these lessons.\nWhere other statistical software constraints you to work out of one data table, R allows you to manipulate multiple data tables. This will allow more flexibility in your work flow, but it requires you to manage more data tables.\nR and Stata All softwares are better suited to certain tasks than others. For example, Stata has historically been a stronger program than R for time series analysis (though time series analysis in R has seen dramatic improvement in recent years). However, R has much more flexibility than Stata to modify what your analysis is doing, allowing more explicit control over data manipulation, modeling, graphics, and workflow integration across programs.\nMore specifically, Stata will make stricter assumptions about what your analysis will look like than R will. This makes it easier to perform typical routines using Stata than using R. On the other hand, should you ever want to extend your analysis beyond typical routines, R provides more infrastructure for making that possible, but the user has to know what they want R to do. Said slightly differently: anything you can do in Stata, you can also do in R, but things that seem simple in Stata may seem complicated in R because R will make fewer assumptions for you.\nHere is a common example that many users encounter. Robust standard errors. In Stata, they are easy: reg y x, robust. In R, they are hard, because R will make you confront the fact that there are many types of robust standard errors. Stata picks an estimator by default and hides this decision from you. R forces you to make the decision for yourself. As a result, political science is full of robust standard errors that reflect Stata\u0026rsquo;s automatic choice.\nThe difference between Stata and R is kind of like the corny quote that you recognize from Spiderman: with great power comes great responsibility. R lets you go farther, but you need to know how not to break everything. This can be a good thing, depending on your style of analysis. I like it, personally, but not every body does, and loads of brilliant social scientists use Stata to enormous success. As you learn more about statistics, about the research you intend to do, and about the tools available to you, (and perhaps about yourself), you will have to make these software decisions for yourself\u0026mdash;or someone else will make the choice for you!\nBottom-up theory Using R sometimes requires some familiarity with general programming concepts. This course will touch on some of these concepts as we go, but rather than hammering them to death, I will instead aim to provide resources for further learning along the way.\nSome of these concepts may seem tedious at first, e.g. variables and functions. We discuss these not because you cannot be trusted to understand these concepts in general, but because these concepts relate to programming in not-always-obvious ways. (For example, what is the difference between a \u0026ldquo;global\u0026rdquo; and \u0026ldquo;local\u0026rdquo; variable? Or, how can a function return \u0026ldquo;random\u0026rdquo; results even if you keep the inputs the same?) The mathematical world and the programming world exhibit many similarities, and understanding how these realms relate will help you become a more skilled programmer, a stronger mathematical thinker, and a better statistical analyst.\nPedagogical note Because R is open-source and very flexible, there are often many different ways to accomplish the same task. Instructors, being experienced R users, see the diversity of options as an inherit benefit of R and its open-source origins, so they are often tempted to teach about the diversity. But for new R users, the diversity of R tools can be daunting and confusing. When you\u0026rsquo;re learning a new language, sometimes you just want to be told what works.\nWe have a short time to cover as much quality R as we can, so we have to make some choices about what material to emphasize. Here is the approach we will take. (I confess that my thinking is heaviliy influenced by some blog posts by David Robinson about teaching R, including but not limited to here).\n Rather than give you abstract exercises to demonstrate concepts in a vacuum, we\u0026rsquo;ll create realistic scenarios using real data as much as we can. I won\u0026rsquo;t force you to slog through difficult problems using underpowered tools for the gratuitous purpose of \u0026ldquo;getting you to appreciate the finer points of the language\u0026rdquo; or whatever. Nor will we contrast old and new tools or painstakingly catalog how multiple approaches lead to the same end result. Instead, we will jump quickly into tools that I believe you will actually use in the future. Relatedly, this means we\u0026rsquo;ll be following a philosophy of doing as much as we can with as few powerful tools as possible. Let\u0026rsquo;s define a \u0026ldquo;powerful\u0026rdquo; tool as one that performs a complicated task with a simple interface. That\u0026rsquo;s the sweet spot. Hopefully this makes R exciting to learn because we can do real heavy lifting with it soon, rather than \u0026ldquo;graduating\u0026rdquo; to fun stuff after boring you with annoying stuff.  To help us achieve these goals together, I have made the strategic decision to focus this course on the tidyverse suite of data manipulation packages, including ggplot2 for graphics. There are several other \u0026ldquo;families\u0026rdquo; of data manipulation and graphics approaches, including \u0026ldquo;base R,\u0026rdquo; which is where most R courses begin. Those courses then shift to other approaches after students have been beaten into submission by the inefficiency of base R. My attitude is that you are highly unlikely to use weak tools once you discover powerful tools, so I don\u0026rsquo;t want to exhaust you with exercises for weak tools. Put simply, the Tidyverse is the fastest route to accomplish my goals for this course\u0026mdash;to teach you R that is useful, realistic, simple, powerful, and fun.\nCourse structure We will work through this course using three tools: online notes, in-class lectures, and take-home exercises.\nOnline notes will be the \u0026ldquo;most complete\u0026rdquo; accounting of useful R concepts, functions, and lessons. I hope that these will be a permanent resource that you can always come back to. Online notes will be easiest to read on this website, but source code for all lessons will also be available on my Github page as .Rmd files. I would recommend you download the source files should they ever disappear from the web in the future. We\u0026rsquo;ll divide the material for this course across three lessons but using four documents (not including this page). You should review each lesson before its corresponding lecture day.\n For the first week of R lecture, read this overview of some basic R material and this lesson on data manipulation. The first document covers the absolute basics\u0026mdash;installing R, simple commands\u0026mdash;and the second document covers data manipulation using tidyverse routines. For the second week, we will cover graphics, with a particular focus on ggplot2. Week 3 covers statistical analysis, including model estimation, generating model summaries and post-estimation analysis, and discussing how to incorporate your results into your written work using workflow tools. This lesson will also contain some material on advanced R tools and routines.  Lecture will be a guided walk-through of each week\u0026rsquo;s material. They will cover incomplete selections from the online notes, so it is important to preview the notes before class. We will try to leave as much in-class time to work on take-home exercises as possible.\nThe take-home exercises will reinforce each week\u0026rsquo;s material using an extended analysis of one dataset. These exercises are required and should be submitted on the Thursday following each lesson. Solutions will be made available the following day, so your work should be completed on time.\nDatasets The dataset we will use for online notes and in-class lectures is the American National Election Study, which is a long-running academic survey of U.S. voting and public opinion. Specifically, we will work with the \u0026ldquo;cumulative data file,\u0026rdquo; which contains interviews from survey respondents for presidential and midterm campaign seasons since 1948. To obtain the data, make an account on electionstudies.org and download the cumulative data file before the first in-class lesson. Make sure your download includes the codebook. You should take care of this well in advance of our first R lecture. If you encounter difficulties, get in touch with me after trying to troubleshoot your problem using the website\u0026rsquo;s instructions ;-) .\nTake-home exercises will use the Database on Ideology, Money in Politics, and Elections, which contains information on campaign contributions for U.S. elections organized at by candidate-cycles. There are many data files in the DIME, but you should only download the \u0026ldquo;recipients\u0026rdquo; data file (and the codebook, of course).\nSeeking help, or, My role as instructor My goal is to get you started with R, but I cannot make you an expert. You learn R mostly by doing it, and over the long run, you will learn far more from the internet than you will from me. This is as it should be\u0026mdash;the internet is your friend. Being \u0026ldquo;good at R\u0026rdquo; means (to a certain extent) knowing what to do when you mess up, learning how to Google the error message, and learning to avoid past mistakes. To that end, when you encounter problems with R, I encourage you to start by seeking your own help online. Training yourself to find online solutions is an invaluable skill for R (and any software), so you should practice while you can. Of course, you can contact me if you find online resources to be confusing.\nSources like Stack Overflow are great for working through R problems. So is Twitter. The most prominent R developers are active on both.\n","date":1517961600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1562688722,"objectID":"1bfd166717634437824afe0a116a5f6b","permalink":"/courses/811/","publishdate":"2018-02-07T00:00:00Z","relpermalink":"/courses/811/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Abstract As partisanship increasingly drives general election competition in the U.S., party nomination contests seem increasingly important in determining who represents a district in Congress. At the same time, many ideas about how primaries affect representation in the U.S. are not supported by empirical research. Do primary elections faithfully represent the policy preferences of local partisan voters?\nPolitical scientists believe that candidates for office face a trade-off in the way they position themselves ideologically. To win the nomination, candidate must champion their party\u0026rsquo;s values, but winning the general election requires the support of a broader cross-section of the electorate. Political scientists have called termed this the \u0026ldquo;strategic positioning dilemma\u0026rdquo;—the candidate must be partisan enough to win the nomination, but not so partisan that they lose the general election.\nThis theory of primaries rests on shaky ground both theoretically and empirically. Theoretically, primary elections present voters with high informational demands. Learning the policy positions of different candidates is costly, especially when all candidates share the same party label. Moreover, a candidate\u0026rsquo;s support among informal networks of party-aligned interest groups may be more important to their nominations than their ideological \u0026ldquo;fit\u0026rdquo; with partisan voters in the district. Empirically, research of representation in primaries is held back by a limited understanding of local partisans\u0026rsquo; policy preferences. Most existing research measures voters\u0026rsquo; preferences using coarse proxies that do not distinguish between Republican and Democratic groups or may not meaningfully capture policy preferences at all.\nThis dissertation seeks to improve the measurement of the policy preferences of partisan voters at the local level and apply these new measures to the study of representation in primary elections. Along the way, it exposes and clarifies key causal assumptions at work in prior research, and it explores new methodological tools for the statistical analysis of a broader range of causal inference problems in political science.\nGoals of this project:   Create novel estimates for the policy ideology (ideal points) of partisan groups within Congressional districts—435 districts x 2 parties = 870 groups—using custom Bayesian IRT modeling approach.\n  Apply novel estimates of local partisan preferences to test key theoretical claims about representation in primary elections: does the extremism/moderation of local ideology (X) meaningfully affect the extremism/moderation of primary candidates for Congress (Y1) and the extremism/moderation of the candidate eventually selected to run in the general election (Y2)?\n  Explore a Bayesian framework for causal inference in political science: formal notation, theoretical clarity on the meaning and application of priors, and practical guidance for pragmatic causal inference with Bayesian value-added.\n  Extend Bayesian framework to computational causal inference approaches using machine learning methods, with a focus on structural causal models (DAGs) and neural networks.\n  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601785957,"objectID":"1cbc55d83b7ab3ca4061fc93cc8f5b5a","permalink":"/research/dissertation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/research/dissertation/","section":"research","summary":"Dissertation project","tags":["Elections","Methods"],"title":"Do Primaries Work? Bayesian Causal Models of Partisan Ideology and Congressional Nominations\n","type":"research"},{"authors":null,"categories":null,"content":"Quantitative Finance Work After finishing my Ph.D. in 2020, I joined DRW Holdings as a quantitative researcher. I use statistics and machine learning to analyze and trade options, futures, and other derivative assets.\nPolitical Science Research I received a Ph.D. in political science from the University of Wisconsin, where I was a student affiliate of the Elections Research Center. My research focuses on statistical modeling in American electoral politics with a substantive focus on the performance and functioning of elections: how well elections represent citizen views, how elections are administered, and how citizens experience the task of voting.\nMy dissertation developed Bayesian models for measurement and causal models to study ideological competition and representation in congressional primary elections. Other projects analyze voter identification requirements in Wisconsin, the gender gap and other group dynamics in electoral coalitions, how voters understand with uncertainty in election polls and forecasts, and Bayesian causal inference. Most of this work is open-source.\nFor more, see my publications and projects in progress.\nTeaching I have taught quantitative research courses and workshops at the undergraduate and graduate level. These courses focused on causal inference, predictive modeling, statistical computing, survey research, and essential mathematics for social sciences.\nI was recognized as an Honored Instructor at UW–Madison in 2019.\nFor more, see my teaching page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605378230,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"Quantitative Finance Work After finishing my Ph.D. in 2020, I joined DRW Holdings as a quantitative researcher. I use statistics and machine learning to analyze and trade options, futures, and other derivative assets.\nPolitical Science Research I received a Ph.D. in political science from the University of Wisconsin, where I was a student affiliate of the Elections Research Center. My research focuses on statistical modeling in American electoral politics with a substantive focus on the performance and functioning of elections: how well elections represent citizen views, how elections are administered, and how citizens experience the task of voting.","tags":null,"title":"About Me","type":"page"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":null,"content":" Read this before our first R lecture.\nHow to follow along Follow the instructions in this lesson in your own .R script.\n Objectives This document describes how to get started with R and understand its basics. We discuss installing R, running simple commands, and some elementary programming concepts such as indexing, logic, and functions. We close with some higher-level discussion about the differences between R and Stata.\nNote: Although I promised concrete lessons, this one unfortunately has to be a little more “abstract” than others. This is because we exploring R’s general behavior rather than a concrete application of R workflow. Luckily, this will be short, and we get much more concrete in future lessons.\nYou should take this lesson slowly at first if you have not played with R before. Mess around with some of the code if it helps you understand what it’s doing. Type it yourself bit by bit, so you can see what each component does. If you encounter material that does not make sense, please take note of it so we can review it in class before progressing too far.\n How to read these lessons Pages in this series contain code blocks that you can paste into R. Some of the code blocks are included with their results.\n\u0026quot;Code appears in blocks like this.\u0026quot; \u0026quot;You can paste this code directly into R.\u0026quot; ## [1] \u0026quot;Results appear on this page with two \u0026#39;##\u0026#39; signs on the left\u0026quot;  Installing R If you have not set up R already, click here to download R for your operating system (if you haven’t already).\nA little bit about what’s going on here: R is open source and is distributed from various “mirrors,” which are clone websites that contain essentially identical information. The CRAN (Comprehensive R Archive Network) hosts these mirrors all over the world. It is generally recommended that you install R (and related software like R packages) using mirrors in nearby locations. Any mirror in the U.S. should be fine for our purposes. The link at the top of this section uses the Iowa State University mirror.\nOnce R is downloaded, make sure that it is fully installed. You can run R using the GUI app (R.app on OSX or Rgui.exe on Windows), using the Rstudio application (which we discuss below), or as a program in your computer’s command-line shell (which I do, for technical reasons).\nIf you have a Chromebook or some other computing machine that makes it impossible to install R, try the Rstudio Cloud platform.\n Using R The most essential part of using R is the console. This is where results of all commands are displayed. There is a prompt at the bottom of the console where commands can be submitted directly. Yours won’t be the same colors as mine, but it looks sort of like this:\nAlthough you can type commands directly into the console, most commands should be written in a script file. Script files serve the same purpose as they do in Stata; they provide a record of all commands you want to run in your analysis. This lets you replicate your analysis the next day, the next week, the next year, or whenever (presuming your code does not become obsolete for some reason).\n Editing an R script file Script files in R are similar to Stata. They are a place where your code should live, and you should execute code by sending it to the console to be evaluated.\nUse whatever program you desire to open a script file.\n The basic R application has a script editor (accessed using the File menu or a keyboard shortcut). Many newcomers enjoy Rstudio an application that provides tons of tools for R. Rstudio has many advanced features as well, so it isn’t only for newcomers. You can also use third-party editors, but you will need to figure out how to send commands to the R console. I use Sublime Text with the R-Box package to send commands to R. Windows users may find Notepad++ or TextMate to be useful text editors. Advanced programmers may use Emacs or Vim with some package (such as ESS for Emacs) to speak to an R console.  I would recommend any external editor that can send commands to the R console and, as you become more comfortable with R, that contains some keyboard shortcuts for writing code quickly. Rstudio, it so happens, comes standard with many such shortcuts.\n What goes in an R file (code and comments) Script files should begin with some description of the script’s purpose. You can include comments (text that will not be executed as R commands) after the # symbol. For example, your script file for this lesson might contain some comments at the top like…\n# ------------------------------------------------------------ # PS 811: statistical computing for political science # Lesson 1: Basics of R # ------------------------------------------------------------ Comments are great for describing what code is doing, planning an analysis, writing notes to yourself, and so on. Comments are also great for pseudocode, which are notes to yourself that “translate” what code is doing into English. We will see some examples of this as the course progresses.\nAn R script file may begin with some common operations:\n Setting a working directory (see the next lesson) setting up a log file (if desired… I don’t do this though!) Loading packages for extra functionality Setting other project-wide options (e.g. graphics options)  Although a script file can be imperfect while a project is being developed, a well-written script file for a finished project should be organized, easy to read, and run from top to bottom without any errors. This is important for ensuring that you can retrace the steps of your analysis.\nWe should save this file in a designated location for this course. Navigate to your ps811 folder on your computer (which you should already have…), create a subfolder called R, and then within the R folder, create another lessons folder. Save this document in the ps811/R/lessons/ folder. (This syntax represents folder pathways in your computer’s file system. We’ll be seeing more of this later.)\n Executing commands Just like Stata, R processes commands one at a time. Commands are compiled while they are executed, so if something is wrong with your code, you may not realize it until you try to run it.\nJust to demonstrate some basic R behavior, let’s run a few commands.\nFirst, R works as a calculator. It can handle mathematical expressions. Paste or type the following lines of code DIRECTLY into the console.\n1 + 1 2 + 2 100 * 2 500 / 4 Ignore the [1] that prints in the results for now. We will explain it later.\nNow take the above block of code and paste the commands into your script file. Try to run the commands from the script file using the appropriate keyboard shortcut. Most computers and interfaces use super + enter, where super refers to Ctrl on PCs or Cmd on Macs. Windows machines may use a different shortcut, such as Ctrl + R. Take this opportunity to look up the appropriate keyboard shortcut on your own, before coming to lecture! :-)\n Variables and assignment Most of your R code will work with variables rather than raw numbers. Variables generally work the same way they do with math. The expression \\(x + 4\\) can take different values, depending on the value of \\(x\\).\nData must be “assigned” to a variable using the assignment operator \u0026lt;-. The \u0026lt;- is a combination of a less-than sign \u0026lt; and a hyphen -.\nFor example, assign the value of 5 to a variable called x.\nx \u0026lt;- 5 After this variable has been created, x and 5 mean the same thing as far as R is concerned.\nYou can read the \u0026lt;- operator as “gets.” A left-hand side object name “gets” the results of whatever operation is on the right-hand side. So in the above example, x “gets” the value of 5.\n Printing the contents of an object We can display the contents of any object in R by simply typing the object name.\nx ## [1] 5 That is to say, R “evaluated” the statement x, and the result is 5. We could have written some more complex statement for R to evaluate, like a mathematical expression using x…\nx + 4 ## [1] 9  Vectors: multi-element variables Most of the variables you will work with in R (such as the variables in a data set) have multiple values. That is, they are vectors.\nWe can create our own vectors with c(), which assigns a series of values to one variable. It can be helpful to think of c() as “combine.”\ny \u0026lt;- c(2, 4, 6) The y object now contains 2, 4, and 6.\ny ## [1] 2 4 6 If you wrap an assignment statement inside of parentheses, R will assign the values and print the result.\n(y \u0026lt;- c(2, 4, 6)) ## [1] 2 4 6 Vectors are useful for performing operations on every element in the vector. Just as we could do math with a vector…\n\\(\\begin{align} 2 \\cdot \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 8 \\\\ 12 \\end{bmatrix} \\end{align}\\)\nwe can also do similar things in R.\n2 * y ## [1] 4 8 12 A note about multiplying vectors: by default, R does element-wise multiplication (the “Hadamard product”). To do matrix-style multiplication (dot product/inner product) with vectors, we have a different operator.\n# element-wise product: y * y ## [1] 4 16 36 # dot product y %*% y ## [,1] ## [1,] 56  Everything is an Object In R, everything is an object. I can store any value as a variable, which is an object. A text string is an object. A dataset. A variable within a dataset is an object within an object. It’s objects all the way down. This makes R very flexible.\n Indexing You can access individual elements in a vector using indexing notation with square brackets. Using the variable y from above, what are the first, second, and third elements?\n# entire y vector y ## [1] 2 4 6 # first element of y y[1] ## [1] 2 # second element of y y[2] ## [1] 4 # third element of y y[3] ## [1] 6 This is akin to indexing in math, where we could write \\(y_{1} = 2\\), \\(y_{2} = 4\\), and so on.\nThis is why the console prints a little [1] next to all results. The [1] indicates that the adjacent element is the first element of the results. If we print a longer object, this becomes clearer. For example, 30 repetitions of the word \u0026quot;hello\u0026quot;.\n# repeat \u0026quot;hello\u0026quot; 30 times rep(\u0026quot;hello\u0026quot;, 30) ## [1] \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; ## [9] \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; ## [17] \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; ## [25] \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;hello\u0026quot; Indexing works with higher-dimensional data as well. Here is a two-dimensional array of numbers.\n# row-bind separate vectors m \u0026lt;- rbind(c(1, 2), c(3, 4), c(5, 6)) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 (Notice how rbind() binds vectors together as rows. The cbind() function would bind vectors as columns.)\nTo index a multi-dimensional object, use commas to separate the dimensions. The notation object[a, b] would reference the value in row a and column b.\nm[2, 2] ## [1] 4 If we specify a row but don’t specify a column, R returns every column from the specified row.\nm[1, ] ## [1] 1 2 If we specify a column but don’t specify a row, R returns every row from the specified column.\nm[ , 2] ## [1] 2 4 6 Even though m[ , 2] is the second column of m, the results don’t print as a vertical column. Why? Because once R evaluates m[ , 2], it simply returns an object. R no longer remembers that it comes from m. The result is nothing more than a vector floating there. As far as R is concerned, m[ , 2] is exactly equivalent to a vector c(2, 4, 6). This is actually good; it’s exactly the kind of flexibility that will be valuable in the future.\nR is one-indexed R is one-indexed, meaning the first element of a vector is element 1. Some languages (like C) are zero-indexed. Even though R is one-indexed, calling the 0’th element of a variable does not throw an error, so be careful.\n Indexing in practice The code we use in this course (heavily derived from the tidyverse suite of packages) does not require much explicit indexing. However, you should understand the intuition of indexing for a few reasons.\n It has useful mathematical parallels, where \\(y_{i}\\) is analogous to y[i]. This sometimes comes in handy for dealing with the results from statistical models. You will encounter it when searching through message boards for R help.    Logic Logic as a concept in programming is less daunting than it sounds. At it’s core, we’re working with statements that evaluate to either TRUE or FALSE.\nExample: is 1 greater than 0?\n1 \u0026gt; 0 ## [1] TRUE Let’s make a new vector, a sequence of values from 1 to 10.\n(g \u0026lt;- 1:10) ## [1] 1 2 3 4 5 6 7 8 9 10 Now that we have this vector, we can derive logical statements about it. Which elements are greater than 5?\ng \u0026gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE The statement g \u0026gt; 5 is a “logical vector,” meaning that its a vector whose values are either TRUE or FALSE. TRUE and FALSE are keywords in R (and must be capital). R asks if g is greater than 5, and since g is a vector, R returns a vector of logicals with each element corresponding to the original elements in g.\nThere are many logical operators. Here are some:\n# g \u0026quot;is equal to\u0026quot; 5 (DOUBLE EQUALS) g == 5 ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE # g \u0026quot;is greater than\u0026quot; 5 g \u0026gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE # g \u0026quot;is less than\u0026quot; 5 g \u0026lt; 5 ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE # g \u0026quot;is greater than or equal to\u0026quot; 5 g \u0026gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE # g \u0026quot;is less than or equal to\u0026quot; 5 g \u0026lt;= 5 ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE This seems a little abstract right now, but here’s how you may see others use logic for indexing. Let’s say that I have some data from countries around the world, and I want to look only at the European data. You might see someone online limit the data like so.\nworld_data[region == \u0026quot;Europe\u0026quot;, ] Translated: retrieve the world_data object, but only the rows (notice the comma in the indexing) where region is equal to \u0026quot;Europe\u0026quot;.\nMore literally, the statement region == \u0026quot;Europe\u0026quot; is either TRUE or FALSE for every observation in the data. The above statement returns the rows from world_data where region == \u0026quot;Europe\u0026quot; evaluates to TRUE.\nAs mentioned above, we will perform this kind of case selection with a more intuitive syntax that is easier to wrap your brain around.\n Data types There are a few different types of data in R.\n Logical: TRUEs and FALSEs. Numeric: including integers and doubles Strings: text strings that are contained in \u0026quot;quotes\u0026quot;. R calls these “character” type variables. Factors: ordered categories with text labels  Factors are akin to labelled variables in Stata. They are somewhat like numbers, somewhat like text strings. They are like strings because they havea text label, and you can’t do math with them. They are like numbers because you can put them in order. Put another way, factors are ordered categories that have textual metadata.\nIt is possible to coerce data from type to another. We cover this in our final lesson.\nNote: data types are different from object classes. Data type describes the values, but object classes describes the organization of values. Classes include vectors, matrices, data frames, tables, lists, and so on.\n Missing data Vectors might contain missing data. The missing data code in R is capital NA, without quotes.\nMissing data take the same data type as the vector in which they are located. For example:\nh \u0026lt;- c(1, 2, NA, 4) # what data type is h? str(h) ## num [1:4] 1 2 NA 4 By placing NA inside a numeric vector, what we’ve told R is that we know that the missing data is numeric, we just don’t know what the number is. If some function requires that we know every value, it will probably fail. Try the following.\nmean(h) ## [1] NA This doesn’t cause an error. Instead, the result is a missing value. What this means is, we know that there should be a numeric mean (which is why there is no error), we just don’t know what the mean is because we don’t know what all of h is. When we want to calculate this value irrespective of the missing values, we often need to tell R to skip over missing values.\nmean(h, na.rm = TRUE) ## [1] 2.333333 Many functions have na.rm (“remove NA”) arguments that can be flipped on, such as sum(), sd(), and so on.\n Functions In math, you can manipulate a variable with a function, \\(f(x)\\). There are tons of functions that you can use in R.\nThere are three things you need to use a function.\nThe function name itself. Self-explanatory. Parentheses. This tells R that the object name is meant to be a function. Arguments inside the parentheses. These may be optional, depending on the function and its default behaviors. Arguments can be data that you pass to the function (as inputs), settings that you modify, and so on.  Some easy, common functions.\n# 10 thousand random samples from a normal distribution x \u0026lt;- rnorm(n = 10000, mean = 0, sd = 1) # sum sum(x)  ## [1] 163.006 # number of elements in an object length(x)  ## [1] 10000 # mean mean(x)  ## [1] 0.0163006 # variance var(x)  ## [1] 1.006162 # standard deviation sd(x)  ## [1] 1.003076 It is easy to write your own functions in R. We will cover this in the final lesson.\n Help files Get help on any function using ?function_name or help(function_name).\n External Packages (for non-base functions) Base R only contains a small number of functions. Most analyses require more complicated tools that are not standard with R. For these tasks, there are hundreds of packages available for download.\nOfficial R packages are hosted on CRAN and can be installed with code. For example, the following code will install the tidyverse package. Because the package depends on other packages, all required packages will be downloaded if not already on your system. As a result, this command would take a while to execute (but you should execute it…we will use this package in class).\ninstall.packages(\u0026quot;tidyverse\u0026quot;) Install multiple packages using c() to combine package names. Run this command as well. We will use all of these packages throughout our lessons.\ninstall.packages(c(\u0026quot;magrittr\u0026quot;, \u0026quot;haven\u0026quot;, \u0026quot;labelled\u0026quot;, \u0026quot;broom\u0026quot;, \u0026quot;ggplot2\u0026quot;, \u0026quot;stargazer\u0026quot;, \u0026quot;texreg\u0026quot;, \u0026quot;xtable\u0026quot;, \u0026quot;Rmisc\u0026quot;, \u0026quot;mvtnorm\u0026quot;))  Moving on If you have made it this far, take a break!\nI want to remind you that future lessons will be far less abstract than this one. Many concepts in this lesson are important in the sense that you will see them again during your R career, but they are not all essential for surviving this course. Future lessons will be far more concrete and dataset-driven.\nContinue with the next lesson: Data Manipulation\n ","date":1518048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518048000,"objectID":"aca7fbadac4c7b15a389b3aeddc4e8d8","permalink":"/courses/811/811-02-basics/","publishdate":"2018-02-08T00:00:00Z","relpermalink":"/courses/811/811-02-basics/","section":"courses","summary":"Read this before our first R lecture.\nHow to follow along Follow the instructions in this lesson in your own .R script.\n Objectives This document describes how to get started with R and understand its basics. We discuss installing R, running simple commands, and some elementary programming concepts such as indexing, logic, and functions. We close with some higher-level discussion about the differences between R and Stata.\nNote: Although I promised concrete lessons, this one unfortunately has to be a little more “abstract” than others.","tags":null,"title":"Getting Started","type":"docs"},{"authors":null,"categories":null,"content":" Read this before our first R lecture.\nOverview How to follow along A script file walking through some of these commands is available here.\n Objectives The goal of this lesson is to simulate some data manipulation for a research project. This requires…\n setting up the project on your computer Get data onto the computer (the ANES should already be downloaded!) Load the data into R Cleaning and modifying the data Doing some simple calculations  Future lessons pick up where this one leaves off, so you must complete this lesson!. We will be making graphics and doing statistical analysis that depend on the changes we make in this document.\n  Setting up a project Each projects should have a dedicated folder on your computer. These folders should be internally organized: separate folders for data, R scripts, writing, other documentation, and so on.\nHere is an example of one of my project folders.\nFor this class, set up your class folder like this:\n You should already have a dedicated ps811 folder Inside of ps811, create an R folder for all of your R materials. Inside of R, create folders for data, lessons, and exercises. Put all data in the data folder, and put any R files corresponding to online lessons and lecture in the lessons folder. (Exercise files will go in the exercises folder).  Tips for project folders  When you name your folders and files, use hyphens or underscores to separate words, not spaces. Something like 811-data-lesson.R is better than 811 data lesson.R. Use short folder and file names names when you can. This makes your life easier in the long run (e.g. with Git, which I recommend learning). Avoid setting up the folders in ways that would require you to navigate up the directory tree. See below for more about what I mean.    Directories Just as you can look around your computer in the file browser (“Finder” on Mac and “Explorer” on Windows), R looks around your computer as well, but it needs your help knowing where to look.\nFind the current working directory with getwd().\n# \u0026quot;get working directory\u0026quot; # prints the current directory pathway (here is mine) getwd() ## [1] \u0026quot;/Users/michaeldecrescenzo/Box Sync/site/ac/content/courses/811\u0026quot; To find your project folder, set your working directory to the appropriate path (the ps811/R/lessons folder).\n# setwd() = \u0026quot;set working directory\u0026quot; # changes the directory to the specified file path # the \u0026quot;~\u0026quot; is a shortcut that means \u0026quot;the top of my user profile\u0026quot; setwd(\u0026quot;~/pathway/to/ps811/R/lessons\u0026quot;) # confirm directory location getwd() As with many R functions, you know setwd() worked when it gives you no feedback whatsoever.\nYou should always set your directory to the top of the project folder (also called the “project root” of the project). Do not set it all the way into the data/ folder. This is because it is always easier to navigate down into the data folder than it is to navigate up back into the project folder. That’s because the keyword for doing up a folder, .., is very uninformative.\n Getting started Load some packages so they can be used in this R session. These should already be downloaded from the previous lesson.\n# package loading is done using library(). # Quotes around the package name aren\u0026#39;t necessary # but recommended for technical reasons library(\u0026quot;magrittr\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.2 ## ✔ tibble 2.1.2 ✔ dplyr 0.8.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ tidyr::extract() masks magrittr::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ purrr::set_names() masks magrittr::set_names() When you load tidyverse, you should see a warning that it “masks” some names in the magrittr and other packages. This means some of the function names in the packages are the same. This is generally something you should be watching out for, since your code may not work as intended. It won’t be a problem right now, though.\nRead external data With your directory set at ps811/R/lessons, confirm that the ANES data file are where they should be using list.files(). You can look inside folders by adding the folder path as an argument. The ANES data should be in the data/ folder.\n# this is similar to typing `ls` in your computer\u0026#39;s shell / command line list.files() # you should see the \u0026quot;data\u0026quot; folder. # Now look inside the \u0026quot;data\u0026quot; folder; should see the ANES file list.files(\u0026quot;data\u0026quot;) R has many functions for “reading” datasets into memory, depending on the file format. They typically follow a format of read.xyz() or read_xyz(), where xyz refers to the file type.\nWe want to read a Stata file, which uses the .dta file extension. Although many tutorials will recommend using the foreign package for Stata files, foreign is outdated and can fail with newer Stata files. I usually use the haven package.\n# should already be installed library(\u0026quot;haven\u0026quot;) # read_dta(\u0026quot;path/to/data-file.dta\u0026quot;) anes \u0026lt;- read_dta(\u0026quot;data/anes_timeseries_cdf.dta\u0026quot;) This should take a few moments to load because the ANES cumulative file is a large file. Don’t worry.\nYou may want to check out the rio package for more generic file-reading functions.\n  Data frames Data sets in R are known as “data frames.” Data frames are tabular objects where the columns are variables with variable names. This is just like Stata, but what makes R different is that we can have many data frames in R at one time.\nPrint the data to see what the table looks like.\nanes Technically speaking, the anes object isn’t an ordinary data frame. It’s a “tibble”, which is a modified data frame object, the main differences being that tibbles are often faster and prettier when printed to the console.\nYou can always print a full data frame by coercing a tibble to the standard data frame class, but be warned: the results can be ugly if R tries to print a full data frame.\n# as.data.frame() coerces an object to be data.frame class # (there are many different as.class() functions for coercing data) # You should run this command to see how it works, but be warned: # this is gonna look gnarsty as.data.frame(anes) You can create your own data frame using data_frame(). An example:\n# create x and y variables, # then use x and y to create other variables # (inside the same function!) data_frame(x = 1:3, y = 4:6, z = x + y, abc = z * z) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. ## # A tibble: 3 x 4 ## x y z abc ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 4 5 25 ## 2 2 5 7 49 ## 3 3 6 9 81 Learning about data frames Some functions for learning about your data:\nFind the number of rows (observations). In this case, rows are survey respondents.\n# number of rows nrow(anes)  ## [1] 55674 Find the number of columns (variables).\n# number of columns ncol(anes) ## [1] 952 Print the top or bottom of a data frame to get a glance of it.\n# top 6 and bottom 6 rows # can set n != 6 if you desire head(anes) tail(anes) Print a vector of variable names.\n# vector of variable names names(anes)  Other functions for viewing data There are two for interacting directly with a data table as if it were a spreadsheet: View() and edit(). I recommend against using these, because these functions often make R freak out and freeze. The only time this has ever worked for me is when I was using Rstudio, which has a handy spread sheet window for datasets.\nIf you must edit data as a spread sheet, you may want to save it as a csv file and open it in Numbers, Excel, or some other similar program.\n  Variables in a data frame Here is a funky but extremely important R thing. Variables in a dataset cannot be accessed by typing only the variable name.\nTry to print VCF0004 variable. The codebook (which you should have!) tells us that it’s the election cycle variable.\nVCF0004 You should get an error. To prevent the error, use dataset$variable.\n# this should work anes$VCF0004 The $ symbol tells R that the variable is inside the anes dataset. This is necessary because we may have multiple datasets in R with the same variable names, so we have to be specific when we want one particular variable. Yes, this is more complicated than Stata, but it lets us do a lot of cool stuff that Stata makes difficult or tedious.\n DO NOT attach data Others may have advised you that you can sidestep the dataset$variable syntax by attaching data (attach(dataset)). Ignore that advice. You should not attach data. It does not do what you think it does, and it creates so many more (invisible) problems than it solves. Attaching data is playing with fire.\nMy policy for this class is that attaching is forbidden. Do not attach data for your homework. Do not attach data for your final project. Do not attach data in a house or with a mouse. If you are used to attaching data, I’m sorry, but you will thank me later.\nLuckily for us (and all of the R community), the tidyverse package provides many tools that make attaching unnecessary.\n The Tidyverse Now, the good stuff. The Tidyverse describes itself as…\n …an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs.\n In short There are lots of R packages out there. Some of them exist under umbrellas of shared use and interface. You can think about the Tidyverse as an umbrella of similar packages that are designed to have similar interfaces and intuitions.\nThe “Tidyverse” as a concept refers to the collection of packages, including tidyr, dplyr, and so on. The tidyverse package unto itself is a relatively recent entity. It is simply a bundling of the most-used Tidyverse tools (not every tool, however).\nThe packages in the Tidyverse are all designed to clean and manipulate data according to the principle of “tidy data” (described below). Moreover, the tools are designed with a coherent syntax that makes them easy for beginners to learn, easy to understand (when you read the code), easy to integrate into complex analysis, and easy for visualizing data. It is hard for me, as someone who has been using R for years, to describe to newcomers just how much the Tidyverse has improved the world of R in recent years.\n In medium The Tidyverse is based on a philosophy is based on tidy data, which refers to data organization where (1) rows contain cases, (2) columns contain variables, and (3) cells contain variable values.\nAlthough this sounds simple, there are many data formats that do not fit that pattern. Legislative data, for example, is often represented as a vote matrix with legislators in rows and bills in columns. The cells, in turn, would contain 1s and 0s to indicate legislators’ Yea and Nay votes, respectively, on each bill. A tidy legislative voting dataset, on the other hand, would have a legislator variable, a bill variable, and a vote variable. Both datasets contain the same information, but the organization of the data allow for different sorts of manipulations. And it so happens that the tidy organization is particularly helpful for doing a lot of powerful stuff with R with very little code.\nFor more resources about the Tidyverse (aside from this course), you can visit a webpage devoted to such resources, which links you to the R 4 Data Science book, a book on ggplot2, and various cheatsheets for the constituent packages of the Tidyverse. I got my start using this blog post, which describes the concept of “piping data” (which we will cover later) and two of the most important Tidyverse packages: dplyr and tidyr. We will cover the main points of the Tidyverse, but I would bookmark these resources for later browsing.\nOne last note before really jumping in: It is a conscious decision to focus on the Tidyverse at the expense of other approaches (namely, “base R”). As I wrote in the online introduction to these R lessons, this is because we have a limited time to make R fun and accessible, and base R is not ideal for that purpose. That being said, you may encounter base R online and in replication materials for other studies, so you should be open to learning a little bit about it on your own time. But I would encourage against making it your “default” mode of doing R, because it is quite old and inefficient compared to the tools we’ll learn in class. Hopefully you’ll see how easy the Tidyverse is as we proceed!\n  How tidyverse functions work Tidyverse literature refers to its core functions as “verbs.” These are functions that take a data frame and modify it.\nWhen you call one of these verbs, you declare the dataset name in the function, so you don’t need to use the $ to reference a variable (nice!). The function assumes that the variable is located in the declared dataset, and if not, it will check the global R memory for objects of the same name. A generic example:\n# overwrite the old dataset with the results # declare the dataset as the first argument, then specify any arguments dataset \u0026lt;- verb_name(dataset, verb_arguments = ...) We will review the following tidyverse functions from the dplyr package. The dplyr tools are designed to manipulate data that is already in a tidy format.\n rename(): renaming variables mutate(): create and recode variables select(): keep/eliminate variables (columns) filter(): keep/eliminate observations (rows) summarize(): collapse/aggregate data (e.g. summary statistics, group means…) group_by(): create groups out of your data (e.g. for summarizing within group) arrange(): sorting data various join functions: merging data  The tidyr package is designed to take un-tidy data and make it tidy. We’ll cover these functions:\n gather(): turn many columns into one column (wide to long) spread(): turn one column into many columns (long to wide)  Most data manipulation tasks fall under the umbrella of one or more of these functions. This is what makes the Tidyverse so useful: it has a small number of very powerful tools that facilitate nearly all common data munging tasks!\n Renaming The rename() function takes a data frame with one set of names, and it returns a data frame with different names.\nHere, we rename the following variables:\n election year respondents’ ratings of the two major US parties on a 7-pt ideological scale (1 “extremely liberal” to 7 “extremely conservative”) respondents’ ratings of themselves on the same scale and a 7-pt index of party ID (Strong Democrat, Weak Democrat, Independent leaning Democrat, True Independent, Leaning Republican, and so on)  # rename(dataset, # new_name = old_name, # new_name2 = old_name2) anes \u0026lt;- rename(anes, cycle = VCF0004, libcon_demparty = VCF0503, libcon_repparty = VCF0504, libcon_self = VCF0803, pid7 = VCF0301) Be warned: this command overwrites the original anes dataset with a new version with different variables names. If you want to keep the original variables, copy the old variable into a new variable with a new name. How would you do that? Well…\n Modifying data Modify the columns of a dataset with mutate(). We can create new variables or modify existing variables (a.k.a. “recoding”).\nHow it works: declare an existing data frame, modify variables within it, and the result is a new data frame with the specified changes.\nLet’s test it out. Let’s look at the ideological self-placement variable.\ntable(anes$libcon_self, exclude = NULL) ## ## 0 1 2 3 4 5 6 7 9 \u0026lt;NA\u0026gt; ## 1748 750 3138 3603 9873 5304 5379 995 9568 15316 We only care about the values 1 through 7. We want to recode everything else as missing (NA). Here are the two methods I use most often to modify variables.\nThe ifelse() function The ifelse() function follows the following psueocode intuition:\nif (this) then {that} else {something else} You may have seen these sorts of if-else statements in other programming languages. In R, we can use the ifelse() function to apply an if-else statement to an entire vector (variable).\nA dummy example:\nmy_result \u0026lt;- ifelse(condition, A, B) The ifelse() function checks a condition in the data. This condition is a logical statement: is something equal to something, greater than something, and so on. If the condition applies (meaning, if the statement evaluates to TRUE), the result is A, and if the condition does not apply, the result is B.\nWe will use it within the mutate() function. I have to get creative with code indentation to make this example more legible, so bear with me.\n# translation: # modify anes # recode libson_self, the result of ifelse() # ifelse(logical test, result if TRUE, result if FALSE) # and recode demparty # Notes: # notice how I can break lines after the \u0026lt;- symbol # this is because \u0026lt;- needs a left-side and right-side object # if there is no right-side object, R looks for on next line # this works for all \u0026quot;binary operators\u0026quot; such as +, -, *, and so on # In general, R will look at the next line until a statement is complete # so you should be careful to close all \u0026quot;quotes\u0026quot; and (parentheses) anes \u0026lt;- mutate(anes, libcon_self = ifelse(libcon_self == 0 | libcon_self \u0026gt;= 8 | is.na(libcon_self), NA, libcon_self), libcon_demparty = ifelse(libcon_demparty %in% 1:7, libcon_demparty, NA)) We recode two variables in this mutate() call. We use ifelse() in slightly different ways. Here is the translation:\n If libcon_self is 0 or greater than or equal to 8 or NA, then recode to NA, else recode to the existing value of libcon_self (i.e. no change). If libcon_demparty is an integer value 1 through 7, keep it the way it is, else recode to NA. We’ll return to the %in% operator at the end of this section.  Read the binary logical operations as follows:\n A == B: A is equal to B A \u0026gt; B: A is greater than to B A \u0026lt; B: A is less than to B A \u0026gt;= B: A is greater than or equal to B A \u0026lt;= B: A is less than or equal to B A | B: A or B A \u0026amp; B: A and B A %in% c(B, C, D): A is equal to any of the following: B, C, or D. Or, A is some element of the set {B, C, D}. is.na(A): A is NA. !is.na(A): A is not NA.  Heads up: nothing can be equal to NA. That’s because NA isn’t a value; it’s a stand-in for an unknown value. This is why we use the is.na() function.\nAlso: the %in% operator is a godsend because…\n x == (1 | 2 | 3) doesn’t work x == 1 | x == 2 | x == 3 works but is annoying  Instead, it is easier to type x %in% c(1, 2, 3) (or x %in% 1:3 if we want to match adjacent integers).\n The case_when() function Whenever we have multiple conditions that we want to check in the same function, we could nest several ifelse() functions within each other or use a very complicated logical test, but it gets quickly gets ugly and is easy to mess up.\nIn situations like this, you should use the case_when() function, which is a more flexible version of ifelse(), but it’s a little different. See the comments in the code chunk below for how it works.\n# case_when(test ~ result if test is true, # test2 ~ result if test2 is true, # test3 ~ you get the idea) # NOTE: all non-matched cases are given an NA by default # to override this, use the catch-all replacement: TRUE ~ x # case_when(test ~ result, # test2 ~ result2, # test3 ~ result3, # TRUE ~ result4) # read as \u0026quot;and everything else should be recoded as result4\u0026quot; anes \u0026lt;- mutate(anes, libcon_repparty = case_when( libcon_repparty == 1 ~ libcon_repparty, libcon_repparty == 2 ~ libcon_repparty, libcon_repparty %in% c(3, 4) ~ libcon_repparty, libcon_repparty %in% (5:7) ~ libcon_repparty), pid7 = case_when( (pid7 %in% 1:7) ~ pid7) ) You may find it helpful to place the tests in parentheses, as I do in the pid7 example. It doesn’t affect the code, but is sometimes easier to read, especially if the logical test is complicated or lengthy.\nThe above code, translated:\n If libcon_repparty is equal to 1, replace with libcon_repparty (i.e. keep the same). If it is 2, keep the same. Same with values 3 and 4, and 5 through 7. The only reason I break these into different conditions is to show you different ways to do this logical matching. If the pid7 variable is an integer value 1 through 7, keep it the same.   General tips for recoding Depending on what you’re doing, you may not want to overwrite the original data. Instead you might create a new variable that modifies the original.\nWhen you recode new variables, you can compare the new and old variables using the table() function.\nI try to consolidate all of my data cleaning tasks into as few calls to mutate() as possible. This makes it easy to retrace your steps. In fact, I usually devote an entire .R file in my project solely to cleaning the original data. Once the data are clean, I save a cleaned dataset, and then load the cleaned dataset in a separate .R file for subsequent analyses.\n Variables from other variables Intuitively, you can create variables from other variables.\nHere, we calculate the ideological distance between one’s self placement and their placement of the two parties. We also calculate how far apart they perceive the parties to be. The sign of the result (\\(+/-\\)) indicates the ideological direction of the difference. Negative values indicate that respondents find themselves to be more liberal than a party, or that they find the Republican Party to be more liberal than the Democratic Party (which would be weird, but hey, that’s survey data for you).\nanes \u0026lt;- mutate(anes, dem_distance = libcon_self - libcon_demparty, rep_distance = libcon_self - libcon_repparty, party_distance = libcon_repparty - libcon_demparty) mutate() is great because you can create a variable in one line and use it in another line, without ending the mutate() call.\n Manipulating factors and strings You should check out the forcats and stringr packages. They are tools for manipulating factors and character variables (a.k.a. “strings”), respectively, and are loaded when you load the tidyverse package. We will revisit these when we discuss graphics in the next lesson, but the functions from these packages that I use the most are (using the pkg::function notation)…\n fct_relevel(): reorder the levels in a factor fct_recode(): recoding a factor, but case_when works here also str_sub(): extract a pattern from a string str_split(): split a string at a certain character str_detect(): returns TRUE if a string contains a pattern str_replace(): replace a pattern in a string with a different pattern  Check online or investigate the help files to learn about how they’re used. We’ll see some examples next week when we do graphics.\n  Selecting columns Maybe you don’t need all these variables for something. You can select specific variables using the select() function.\n# from the anes dataset, grab only the cycle variable select(anes, cycle) ## # A tibble: 55,674 x 1 ## cycle ## \u0026lt;dbl\u0026gt; ## 1 1948 ## 2 1948 ## 3 1948 ## 4 1948 ## 5 1948 ## 6 1948 ## 7 1948 ## 8 1948 ## 9 1948 ## 10 1948 ## # … with 55,664 more rows There are various “select helper” functions that aid us in selecting variables.\n# select a series of variables in the data frame using `:` # grab cycle, and all variables between dem_distance and party_distance select(anes, cycle, dem_distance:party_distance) # select cycle and any variable containing \u0026quot;libcon\u0026quot; in the variable name select(anes, cycle, contains(\u0026quot;libcon\u0026quot;)) # drop variables using negative sign `-` # matches(\u0026quot;.\u0026quot;) means \u0026quot;all remaining variables\u0026quot; # drop cycle, keep all remaining variables select(anes, -cycle, matches(\u0026quot;.\u0026quot;)) Learn more about “select helper functions” in the help file (?select).\n Filtering rows (subsetting) Let’s say we only want some of the observations. Use the filter() function and a logical test to identify the cases you want.\n# keep cases from 2012 cycle filter(anes, cycle == 2012) ## # A tibble: 5,914 x 955 ## Version cycle VCF0006 VCF0006a VCF0009x VCF0010x VCF0011x VCF0009y ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ANES_c… 2012 1 20120001 0.380 0.380 0.380 0 ## 2 ANES_c… 2012 2 20120002 0.547 0.547 0.547 0 ## 3 ANES_c… 2012 3 20120003 0.498 0.498 0.498 0 ## 4 ANES_c… 2012 4 20120004 0.255 0.255 0.255 0 ## 5 ANES_c… 2012 5 20120005 0.603 0.603 0.603 0 ## 6 ANES_c… 2012 6 20120006 0.286 0.286 0.286 0 ## 7 ANES_c… 2012 7 20120007 0.195 0.195 0.195 0 ## 8 ANES_c… 2012 8 20120008 0.394 0.394 0.394 0 ## 9 ANES_c… 2012 9 20120009 0.506 0.506 0.506 0 ## 10 ANES_c… 2012 10 20120010 2.51 2.51 2.51 0 ## # … with 5,904 more rows, and 947 more variables: VCF0010y \u0026lt;dbl\u0026gt;, ## # VCF0011y \u0026lt;dbl\u0026gt;, VCF0009z \u0026lt;dbl\u0026gt;, VCF0010z \u0026lt;dbl\u0026gt;, VCF0011z \u0026lt;dbl\u0026gt;, ## # VCF0012 \u0026lt;dbl\u0026gt;, VCF0012a \u0026lt;dbl\u0026gt;, VCF0012b \u0026lt;dbl\u0026gt;, VCF0013 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0014 \u0026lt;dbl+lbl\u0026gt;, VCF0015a \u0026lt;dbl+lbl\u0026gt;, VCF0015b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0016 \u0026lt;dbl+lbl\u0026gt;, VCF0017 \u0026lt;dbl+lbl\u0026gt;, VCF0018a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0018b \u0026lt;dbl+lbl\u0026gt;, VCF0019 \u0026lt;dbl+lbl\u0026gt;, VCF0050a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0050b \u0026lt;dbl+lbl\u0026gt;, VCF0070a \u0026lt;dbl+lbl\u0026gt;, VCF0070b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0071a \u0026lt;dbl+lbl\u0026gt;, VCF0071b \u0026lt;dbl+lbl\u0026gt;, VCF0071c \u0026lt;dbl+lbl\u0026gt;, ## # VCF0071d \u0026lt;dbl+lbl\u0026gt;, VCF0072a \u0026lt;dbl+lbl\u0026gt;, VCF0072b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0101 \u0026lt;dbl+lbl\u0026gt;, VCF0102 \u0026lt;dbl+lbl\u0026gt;, VCF0103 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0104 \u0026lt;dbl+lbl\u0026gt;, VCF0105a \u0026lt;dbl+lbl\u0026gt;, VCF0105b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0106 \u0026lt;dbl+lbl\u0026gt;, VCF0107 \u0026lt;dbl+lbl\u0026gt;, VCF0108 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0109 \u0026lt;dbl+lbl\u0026gt;, VCF0110 \u0026lt;dbl+lbl\u0026gt;, VCF0111 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0112 \u0026lt;dbl+lbl\u0026gt;, VCF0113 \u0026lt;dbl+lbl\u0026gt;, VCF0114 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0115 \u0026lt;dbl+lbl\u0026gt;, VCF0116 \u0026lt;dbl+lbl\u0026gt;, VCF0117 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0118 \u0026lt;dbl+lbl\u0026gt;, VCF0119 \u0026lt;dbl+lbl\u0026gt;, VCF0120 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0121 \u0026lt;dbl+lbl\u0026gt;, VCF0122 \u0026lt;dbl+lbl\u0026gt;, VCF0123 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0124 \u0026lt;dbl+lbl\u0026gt;, VCF0125 \u0026lt;dbl+lbl\u0026gt;, VCF0126 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0126a \u0026lt;dbl+lbl\u0026gt;, VCF0126b \u0026lt;dbl+lbl\u0026gt;, VCF0126c \u0026lt;dbl+lbl\u0026gt;, ## # VCF0127 \u0026lt;dbl+lbl\u0026gt;, VCF0127a \u0026lt;dbl+lbl\u0026gt;, VCF0127b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0128 \u0026lt;dbl+lbl\u0026gt;, VCF0128a \u0026lt;dbl+lbl\u0026gt;, VCF0128b \u0026lt;dbl+lbl\u0026gt;, ## # VCF0129 \u0026lt;dbl+lbl\u0026gt;, VCF0130 \u0026lt;dbl+lbl\u0026gt;, VCF0130a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0131 \u0026lt;dbl+lbl\u0026gt;, VCF0132 \u0026lt;dbl+lbl\u0026gt;, VCF0133 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0134 \u0026lt;dbl+lbl\u0026gt;, VCF0135 \u0026lt;dbl+lbl\u0026gt;, VCF0136 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0137 \u0026lt;dbl+lbl\u0026gt;, VCF0138 \u0026lt;dbl+lbl\u0026gt;, VCF0138a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0138b \u0026lt;dbl+lbl\u0026gt;, VCF0138c \u0026lt;dbl+lbl\u0026gt;, VCF0138d \u0026lt;dbl+lbl\u0026gt;, ## # VCF0138e \u0026lt;dbl+lbl\u0026gt;, VCF0139 \u0026lt;dbl+lbl\u0026gt;, VCF0140 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0140a \u0026lt;dbl+lbl\u0026gt;, VCF0141 \u0026lt;dbl+lbl\u0026gt;, VCF0142 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0143 \u0026lt;dbl+lbl\u0026gt;, VCF0144 \u0026lt;dbl+lbl\u0026gt;, VCF0145 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0146 \u0026lt;dbl+lbl\u0026gt;, VCF0147 \u0026lt;dbl+lbl\u0026gt;, VCF0148 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0148a \u0026lt;dbl+lbl\u0026gt;, VCF0149 \u0026lt;dbl+lbl\u0026gt;, VCF0150 \u0026lt;dbl+lbl\u0026gt;, ## # VCF0151 \u0026lt;dbl+lbl\u0026gt;, VCF0152 \u0026lt;dbl+lbl\u0026gt;, VCF0153a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0153b \u0026lt;dbl+lbl\u0026gt;, VCF0153c \u0026lt;dbl+lbl\u0026gt;, VCF0154a \u0026lt;dbl+lbl\u0026gt;, ## # VCF0154b \u0026lt;dbl+lbl\u0026gt;, …  Summarizing summarize() will process multiple observations into summary statistics. This is also known as “collapsing” or “aggregating.”\nFind the mean ideological distance between the two parties (as judged by the respondents).\n# data contain NAs so we use na.rm = TRUE summarize(anes, mean_party_distance_na = mean(party_distance, na.rm = TRUE)) ## # A tibble: 1 x 1 ## mean_party_distance_na ## \u0026lt;dbl\u0026gt; ## 1 1.92  Grouping and summarizing Most of the time you will use summarize() in conjunction with group_by(), which groups the data by some selection of variables. It doesn’t modify the cells in any way; it only implicitly partitions the data for later calculations.\nFor instance, let’s say we want to do find the number of observations within each election year the above calculation but within each election year. The n() function, when used inside of summarize() or mutate(), will find the number of observations.\n# group by cycle s \u0026lt;- group_by(anes, cycle) # summarize the grouped data summarize(s, n = n(), mean_party_distance = mean(party_distance, na.rm = TRUE)) ## # A tibble: 30 x 3 ## cycle n mean_party_distance ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1948 662 NaN ## 2 1952 1899 NaN ## 3 1954 1139 NaN ## 4 1956 1762 NaN ## 5 1958 1450 NaN ## 6 1960 1181 NaN ## 7 1962 1297 NaN ## 8 1964 1571 NaN ## 9 1966 1291 NaN ## 10 1968 1557 NaN ## # … with 20 more rows A data frame can later be ungrouped with (wait for it…) ungroup().\n Sorting data Sort data frames using arrange(). By default, sorting is done in ascending order. Sort variables in descending order using the desc() function within arrange(). You can also sort by multiple variables at once.\n# keep cycle and party distance variables # sort by cycle (descending order) and then by party distance arr \u0026lt;- select(anes, cycle, party_distance) arrange(arr, desc(cycle), party_distance) ## # A tibble: 55,674 x 2 ## cycle party_distance ## \u0026lt;dbl\u0026gt; \u0026lt;dbl+lbl\u0026gt; ## 1 2012 -6 ## 2 2012 -6 ## 3 2012 -6 ## 4 2012 -6 ## 5 2012 -6 ## 6 2012 -6 ## 7 2012 -6 ## 8 2012 -6 ## 9 2012 -6 ## 10 2012 -6 ## # … with 55,664 more rows Get a sense for the sorting precedence with this toy example.\n# create some data to sort d \u0026lt;- data_frame(x = c(1, 1, 2, 2), y = c(1, 2, 1, 2)) d ## # A tibble: 4 x 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 ## 2 1 2 ## 3 2 1 ## 4 2 2 # sort by y and then by x (meaning, x within y) arrange(d, y, x) ## # A tibble: 4 x 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 ## 2 2 1 ## 3 1 2 ## 4 2 2  Joining (merging) Merging is done with join functions.\nBecause the ANES is so big, this concept will be easier to demonstrate with a toy example. Create two datasets with some overlapping cases and some non-overlapping cases:\n# cases 2 and 3 appear in both data sets # cases 1 and 4 exist in one dataset but not the other (data1 \u0026lt;- data_frame(case = 1:3, var1 = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;))) ## # A tibble: 3 x 2 ## case var1 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a ## 2 2 b ## 3 3 c (data2 \u0026lt;- data_frame(case = 2:4, var2 = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;))) ## # A tibble: 3 x 2 ## case var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 2 x ## 2 3 y ## 3 4 z We want to put these datasets together into one table, matching data to the appropriate cases.\nThe full_join() function keeps all cases from both datasets. Non-matching cells are filled with NA by default, but you can specify replacement values for non-matches if you wish.\n# join all cases, keep non-matches full_join(data1, data2, by = \u0026quot;case\u0026quot;) ## # A tibble: 4 x 3 ## case var1 var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a \u0026lt;NA\u0026gt; ## 2 2 b x ## 3 3 c y ## 4 4 \u0026lt;NA\u0026gt; z You can merge along multiple variables using by = c(\u0026quot;var1\u0026quot;, \u0026quot;var2\u0026quot;, ...).\nleft_join() keeps all cases from left dataset. If the right dataset can’t fill a cell in the left dataset, the result is NA. If the right dataset has other cases that aren’t present in the left, they disappear.\n# keep left data in tact, # merge only matching cases from right data left_join(data1, data2, by = \u0026quot;case\u0026quot;) ## # A tibble: 3 x 3 ## case var1 var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a \u0026lt;NA\u0026gt; ## 2 2 b x ## 3 3 c y right_join() is the mirror of left_join.\nright_join(data1, data2, by = \u0026quot;case\u0026quot;) ## # A tibble: 3 x 3 ## case var1 var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2 b x ## 2 3 c y ## 3 4 \u0026lt;NA\u0026gt; z inner_join() keeps only cases with matches in both datasets.\n# keep only matching cases, drop everything else inner_join(data1, data2, by = \u0026quot;case\u0026quot;) ## # A tibble: 2 x 3 ## case var1 var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2 b x ## 2 3 c y anti_join() is a little funky and different. It keeps only the unmatched cases from the left dataset only. This is helpful for diagnosing a problem with an imperfect attempt to join two data frames.\n# what doesn\u0026#39;t match? anti_join(data1, data2, by = \u0026quot;case\u0026quot;) ## # A tibble: 1 x 2 ## case var1 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a anti_join(data2, data1, by = \u0026quot;case\u0026quot;) ## # A tibble: 1 x 2 ## case var2 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 4 z  Tabulating Here are two ways to tabulate data: table() and count(). table() produces a table object; count() produces a data frame.\ntable() First, table(). The exclude = NULL argument will force R to print the number of missing values (which it does not do by default).\ntable(anes$libcon_self, exclude = NULL) ## ## 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 750 3138 3603 9873 5304 5379 995 26632 Turn frequencies into proportions by wrapping a table object with prop.table().\nprop.table(table(anes$libcon_self, exclude = NULL)) ## ## 1 2 3 4 5 6 ## 0.01347128 0.05636383 0.06471603 0.17733592 0.09526889 0.09661601 ## 7 \u0026lt;NA\u0026gt; ## 0.01787190 0.47835614 You can round these proportions to get more manageable values.\n# round to 3 decimal positions round(prop.table(table(anes$libcon_self, exclude = NULL)), 3) ## ## 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 0.013 0.056 0.065 0.177 0.095 0.097 0.018 0.478 Notice how these are nested functions, like \\(f(g(h(x)))\\). This is the kind of flexibility that we like about R.\nTo make two-way tables, the first variable prints as rows, and the second as columns.\n# pid7 as rows, libcon_self as columns table(anes$pid7, anes$libcon_self, exclude = NULL) ## ## 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 1 360 1392 848 1696 455 433 131 5490 ## 2 109 643 1018 2109 799 431 72 6080 ## 3 165 609 795 1552 451 251 42 2531 ## 4 64 188 287 1552 502 337 84 3395 ## 5 17 115 257 1083 991 847 135 1937 ## 6 14 92 280 1275 1362 1009 107 3274 ## 7 16 78 97 550 725 2049 418 2421 ## \u0026lt;NA\u0026gt; 5 21 21 56 19 22 6 1504 By default, prop.table() estimates proportions out of the entire table. To estimate proportions within rows or columns, use the margin argument (margin = 1 for the fraction within a row, margin = 2 for the fraction within a column).\n# two-way table tab \u0026lt;- table(anes$pid7, anes$libcon_self, exclude = NULL) # proportions as fractions of each row ptab \u0026lt;- prop.table(tab, margin = 1) # round the result round(ptab, 3) ## ## 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 1 0.033 0.129 0.078 0.157 0.042 0.040 0.012 0.508 ## 2 0.010 0.057 0.090 0.187 0.071 0.038 0.006 0.540 ## 3 0.026 0.095 0.124 0.243 0.071 0.039 0.007 0.396 ## 4 0.010 0.029 0.045 0.242 0.078 0.053 0.013 0.530 ## 5 0.003 0.021 0.048 0.201 0.184 0.157 0.025 0.360 ## 6 0.002 0.012 0.038 0.172 0.184 0.136 0.014 0.442 ## 7 0.003 0.012 0.015 0.087 0.114 0.322 0.066 0.381 ## \u0026lt;NA\u0026gt; 0.003 0.013 0.013 0.034 0.011 0.013 0.004 0.909  count() If you want your table to be organized as a data frame (which you often do want), use count(). This is nice for doing further calculations, exporting results (e.g. to \\({\\mathrm{\\LaTeX}}\\)), and so on.\n# from the anes data, count the intersections of cycle and party ID count(anes, cycle, pid7) ## # A tibble: 233 x 3 ## cycle pid7 n ## \u0026lt;dbl\u0026gt; \u0026lt;dbl+lbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1948 NA 662 ## 2 1952 1 392 ## 3 1952 2 435 ## 4 1952 3 173 ## 5 1952 4 83 ## 6 1952 5 128 ## 7 1952 6 237 ## 8 1952 7 241 ## 9 1952 NA 210 ## 10 1954 1 248 ## # … with 223 more rows This contains the same information as table(anes\\$cycle, anes\\$pid7), but the result is a “tidy” data frame.\nTo get proportions, mutate the resulting table as desired. Here we get the proportion of each partisan identity within each election cycle. Group on the cycle and then dividing each count by the number of individuals in each cycle.\n# tabulate these variables, party by cycle (tab \u0026lt;- count(anes, cycle, pid7)) ## # A tibble: 233 x 3 ## cycle pid7 n ## \u0026lt;dbl\u0026gt; \u0026lt;dbl+lbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1948 NA 662 ## 2 1952 1 392 ## 3 1952 2 435 ## 4 1952 3 173 ## 5 1952 4 83 ## 6 1952 5 128 ## 7 1952 6 237 ## 8 1952 7 241 ## 9 1952 NA 210 ## 10 1954 1 248 ## # … with 223 more rows # group by cycle # find n per cycle # divide counts by n_in_cycle # round mutate(group_by(tab, cycle), n_in_cycle = sum(n, na.rm = TRUE), p = n / n_in_cycle, p = round(p, 3)) ## # A tibble: 233 x 5 ## # Groups: cycle [30] ## cycle pid7 n n_in_cycle p ## \u0026lt;dbl\u0026gt; \u0026lt;dbl+lbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1948 NA 662 662 1 ## 2 1952 1 392 1899 0.206 ## 3 1952 2 435 1899 0.229 ## 4 1952 3 173 1899 0.091 ## 5 1952 4 83 1899 0.044 ## 6 1952 5 128 1899 0.067 ## 7 1952 6 237 1899 0.125 ## 8 1952 7 241 1899 0.127 ## 9 1952 NA 210 1899 0.111 ## 10 1954 1 248 1139 0.218 ## # … with 223 more rows The count() function also handles sample weights.\n# keep only elections since 2000 # tabulate cycle and party # apply sample weights count(filter(anes, cycle \u0026gt;= 2000), cycle, pid7, wt = VCF0009z) ## # A tibble: 40 x 3 ## cycle pid7 n ## \u0026lt;dbl\u0026gt; \u0026lt;dbl+lbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2000 1 345. ## 2 2000 2 274. ## 3 2000 3 275. ## 4 2000 4 237. ## 5 2000 5 231. ## 6 2000 6 207. ## 7 2000 7 221. ## 8 2000 NA 16.5 ## 9 2002 1 246. ## 10 2002 2 250. ## # … with 30 more rows The “counts” are no longer whole numbers, thanks to the survey weights (some people count as “partial observations” due to sample design).\n  Tidyr functions This concludes today’s foray into the dplyr family of functions. Now we’ll switch to the tidyr functions. The main difference is that dplyr tends to change data while tidyr simply moves it around.\nWe’ll talk about “wide” and “long” data.\n Wide data might be data from multiple time periods, where variables from different time periods are represented as different columns. So we might have different x and y variables from three different time periods as columns named x1, x2, x3, y1, y2, y3. Long data would have the same information as the wide data, but instead of different variables for time periods, we stack the time periods on top of one another into one variable. So we would have variables for x and y as well as a time_period variable to indicate which observations come from which wave.  When we shape data, what we’re really doing is moving data around (also called “reshaping”) to make it long (elongating) or wide (widening).\n Reshape wide to long, with gather() Gathering will take multiple columns and stack the cells into one variable (with an accompanying variable for labeling).\nHere is an example using the ideological distance variables from above. First we have to prep some data so we can see how this works.\n# keep only certain variables d \u0026lt;- select(anes, cycle, dem_distance, rep_distance) # keep certain election years d \u0026lt;- filter(d, cycle %in% c(2004, 2008, 2012)) # get mean in each year d \u0026lt;- summarize(group_by(d, cycle), dem_distance = mean(dem_distance, na.rm = TRUE), rep_distance = mean(rep_distance, na.rm = TRUE)) # show results d ## # A tibble: 3 x 3 ## cycle dem_distance rep_distance ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 1.41 -1.05 ## 2 2008 1.09 -1.01 ## 3 2012 1.54 -1.23 We’ll gather the two distance variables into one variable, with another variable to indicate which party we’re contrasting from.\n# gather(data, resulting key, resulting value, initial varlist) l \u0026lt;- gather(d, key = party, value = distance, dem_distance, rep_distance) l ## # A tibble: 6 x 3 ## cycle party distance ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 dem_distance 1.41 ## 2 2008 dem_distance 1.09 ## 3 2012 dem_distance 1.54 ## 4 2004 rep_distance -1.05 ## 5 2008 rep_distance -1.01 ## 6 2012 rep_distance -1.23 Select helper functions also work for selecting which variables to gather.\n# gather(data, key, value, varlist) gather(d, key = party, value = distance, contains(\u0026quot;distance\u0026quot;)) ## # A tibble: 6 x 3 ## cycle party distance ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 dem_distance 1.41 ## 2 2008 dem_distance 1.09 ## 3 2012 dem_distance 1.54 ## 4 2004 rep_distance -1.05 ## 5 2008 rep_distance -1.01 ## 6 2012 rep_distance -1.23  Reshaping long to wide, with spread() Spreading is the opposite of gathering. It takes a column and unstacks it into several columns. We need a corresponding label variable also, which becomes the variable names. Observe:\nl ## # A tibble: 6 x 3 ## cycle party distance ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 dem_distance 1.41 ## 2 2008 dem_distance 1.09 ## 3 2012 dem_distance 1.54 ## 4 2004 rep_distance -1.05 ## 5 2008 rep_distance -1.01 ## 6 2012 rep_distance -1.23 # \u0026#39;key\u0026#39; variable becomes new variable names. # \u0026#39;value\u0026#39; variable becomes new variable values spread(l, key = party, value = distance)  ## # A tibble: 3 x 3 ## cycle dem_distance rep_distance ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 1.41 -1.05 ## 2 2008 1.09 -1.01 ## 3 2012 1.54 -1.23  Piping data This section is extremely important.\nNow that we have covered some essential tools for wrangling data, let’s tie it all together with the concept of piping.\nLet’s start by identifying the problem. Data processing requires a lot of steps (each represented by the functions we have learned so far). Many of these steps are related. Can we string these operations together in a way that is easy to understand and easy to write?\nOne (suboptimal) way to sew multiple operations together is with nested functions. Just as we can nest functions in math like \\(f(g(h(x)))\\), we can also do this with R. The problem with this is that the order of operations creates an unintuitive reading experience—we have to read from the inside out. The code becomes ugly and difficult to interpret.\n# this works, and it follows order of operations, but it\u0026#39;s annoying tidy_data \u0026lt;- gather(summarize(group_by(filter(select(dataset, ...), ...), ...), ...), ...) Another (suboptimal) way would be to break up the operation into multiple lines. The problem with this method is that it is verbose and creates a lot of redundancy with object assignment (which can slow down your code with big datasets).\n# each function takes the results from the previous function # this also works, but requires overwriting the data a lot # and lots of redundant `d \u0026lt;- ` instances d \u0026lt;- select(dataset, ...) d \u0026lt;- filter(d, ...) d \u0026lt;- group_by(d, ...) d \u0026lt;- summarize(d, ...) d \u0026lt;- gather(d, ...) We’ll use the pipe operator %\u0026gt;% to make this process easier. The pipe operator takes a left-hand side object and “pipes” it into a right-hand side function. It sounds trivial, but just wait.\nHere is how it works. We’ll use x to represent data and f to represent functions.\n By default, x %\u0026gt;% f() sets x as the first argument in f. So f(x) is equivalent to x %\u0026gt;% f(). If x is needed elsewhere inside of f besides the first argument, we can use . to stand-in for x. For example, f(arguments, data = x) is equivalent to x %\u0026gt;% f(arguments, data = .). If only one argument is needed, you can drop the parentheses on f(). So f(x) is equivalent to x %\u0026gt;% f(), which is equivalent to x %\u0026gt;% f. I would recommend you keep the parentheses, however, because it’s easier to see which names are associated with data and which names are associated with functions.  The pipe operator allows you to do multiple dataset operations in a linear fashion without creating a ton of intermediary objects. The above processing task could be written as the following “pipe chain.”\n# take `dataset` object and pass it to select # the result from select is passed to filter # and so on d \u0026lt;- dataset %\u0026gt;% select(...) %\u0026gt;% filter(...) %\u0026gt;% group_by(...) %\u0026gt;% summarize(...) %\u0026gt;% gather(...) %\u0026gt;% print()  Adding print() at the end of the chain will print the results even if you are assigning the results to d.\nWhat has the pipe chain done? It has made our code linear and readable, and it makes our workflow more straightforward because we can think linearly again. Reading a complex set of operations linearly isn’t normally something you can easily do with programming, so we should really appreciate this!\nHere’s an example using real data. The pipe chain makes it extremely easy to understand exactly what the code is doing.\n# start with anes # keep certain variables # keep certain observations # group and summarize # gather # print result of the chain l \u0026lt;- anes %\u0026gt;% select(cycle, contains(\u0026quot;distance\u0026quot;)) %\u0026gt;% filter(cycle %in% c(2004, 2008, 2012)) %\u0026gt;% group_by(cycle) %\u0026gt;% summarize(n = n(), Democratic = mean(dem_distance, na.rm = TRUE), Republican = mean(rep_distance, na.rm = TRUE)) %\u0026gt;% ungroup() %\u0026gt;% gather(key = party, value = distance, Democratic, Republican) %\u0026gt;% print()  ## # A tibble: 6 x 4 ## cycle n party distance ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004 1212 Democratic 1.41 ## 2 2008 2322 Democratic 1.09 ## 3 2012 5914 Democratic 1.54 ## 4 2004 1212 Republican -1.05 ## 5 2008 2322 Republican -1.01 ## 6 2012 5914 Republican -1.23 We can use pipes to simplify other tasks from earlier in this lesson, like processing a table using proportions and rounding but without all of the nested functions.\n# create a table # send table to prop.table to calculate proportions # send proportions to round() table(anes$pid7, anes$libcon_self, exclude = NULL) %\u0026gt;% prop.table(margin = 1) %\u0026gt;% round(3) ## ## 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 1 0.033 0.129 0.078 0.157 0.042 0.040 0.012 0.508 ## 2 0.010 0.057 0.090 0.187 0.071 0.038 0.006 0.540 ## 3 0.026 0.095 0.124 0.243 0.071 0.039 0.007 0.396 ## 4 0.010 0.029 0.045 0.242 0.078 0.053 0.013 0.530 ## 5 0.003 0.021 0.048 0.201 0.184 0.157 0.025 0.360 ## 6 0.002 0.012 0.038 0.172 0.184 0.136 0.014 0.442 ## 7 0.003 0.012 0.015 0.087 0.114 0.322 0.066 0.381 ## \u0026lt;NA\u0026gt; 0.003 0.013 0.013 0.034 0.011 0.013 0.004 0.909 Although we covered piping last, you should not view it as afterthought. The pipe operator will change the way you use R forever.\n Tips for piping Create a keyboard shortcut If your text editor has the capability, create a keyboard shortcut for the pipe operator! I create this kind of thing with Sublime Text all the time. Rstudio can do it as well. Here’s what I do:\n super + . creates a pipe super + shift + . creates a pipe and adds a new line relatedly, I use super + shift + , to create an assignment operator (\u0026lt;-).   Other helpful pipes There is one other helpful pipe-like operator that we will talk about: %$%. It tells a right-hand function that the variable names in the function come from the left-hand dataset. It doesn’t pipe the entire dataset per se; it only says “look here for variable names.” The two following commands do the same thing:\n# look in anes for pid7 variable table(anes$pid7) ## ## 1 2 3 4 5 6 7 ## 10805 11261 6396 6409 5382 7413 6354 # look in anes for pid7 variable anes %$% table(pid7) ## pid7 ## 1 2 3 4 5 6 7 ## 10805 11261 6396 6409 5382 7413 6354 This is useful when you need to reference multiple variables.\n# notice which pipe I use after \u0026#39;anes\u0026#39; # %$% for piping just variable names # %\u0026gt;% for piping the entire object anes %$% table(pid7, libcon_self, exclude = NULL) %\u0026gt;% prop.table(margin = 1) %\u0026gt;% round(3) ## libcon_self ## pid7 1 2 3 4 5 6 7 \u0026lt;NA\u0026gt; ## 1 0.033 0.129 0.078 0.157 0.042 0.040 0.012 0.508 ## 2 0.010 0.057 0.090 0.187 0.071 0.038 0.006 0.540 ## 3 0.026 0.095 0.124 0.243 0.071 0.039 0.007 0.396 ## 4 0.010 0.029 0.045 0.242 0.078 0.053 0.013 0.530 ## 5 0.003 0.021 0.048 0.201 0.184 0.157 0.025 0.360 ## 6 0.002 0.012 0.038 0.172 0.184 0.136 0.014 0.442 ## 7 0.003 0.012 0.015 0.087 0.114 0.322 0.066 0.381 ## \u0026lt;NA\u0026gt; 0.003 0.013 0.013 0.034 0.011 0.013 0.004 0.909   Saving data You can write or save data from R with many write.xyz or save.xyz functions. I usually prefer to save data in an R-specific format.\nYou should save the data from this lesson so we can come back to it next week.\nsaveRDS(anes, \u0026quot;data/anes-modified.RDS\u0026quot;) If it’s possible that someone using Stata (or some other software) might be using your data, you might save in a more accessible format such as .csv.\nwrite_csv(anes, \u0026quot;data/anes-modified.csv\u0026quot;) R can read and save to a multitude of file types, including .dta for Stata. R could put Stat/Transfer out of business if more people knew about it.\nSome packages provide “swiss-army-knife” data input/output services, such as the import() and export() functions in Thomas Leeper’s rio package.\n Looking forward Make sure you are comfortable with piping, dplyr, and tidyr before beginning the lesson on graphics, because we will use those concepts throughout.\nThere are some other common data-munging tasks that we will put off until the final lesson:\n Writing your own functions Loops (and why you should not use them) apply() functions (and why you should use them instead of loops) Nesting and mapping, a tidy (and parallel!) method for applying complex functions across many datasets at once.   Postscript on coding style I would talk about coding style, but others can probably do that better than I can (though you can check the R scripts on Canvas for do-as-I-do examples). You can find lots of style guides for R online. They will broadly agree on how to write R with good style, but they won’t agree on every fine point. Here are some that I endorse:\n a short style guide by Hadley Wickham that will put you on the right track a longer style guide (again by Wickham) that has general style guidance but also guidance specifically for working with the tidyverse, for those who want to be a little more obsessive about their programming style   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"65cb7e8408c85e189281ee387ad38627","permalink":"/courses/811/811-03-data/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/811/811-03-data/","section":"courses","summary":"Read this before our first R lecture.\nOverview How to follow along A script file walking through some of these commands is available here.\n Objectives The goal of this lesson is to simulate some data manipulation for a research project. This requires…\n setting up the project on your computer Get data onto the computer (the ANES should already be downloaded!) Load the data into R Cleaning and modifying the data Doing some simple calculations  Future lessons pick up where this one leaves off, so you must complete this lesson!","tags":null,"title":"Data Manipulation","type":"docs"},{"authors":null,"categories":null,"content":" Read this before our second R lecture, after the data lesson.\nHow to follow along A script file walking through some of these commands is available here.\n Objectives The goal of this lesson is to provide an introduction to graphics in R, by way of ggplot2 in particular. We will cover:\n The “grammar of graphics”—the “gg” in ggplot How ggplot2 works Common graphics in social science Customizing the appearance of ggplot graphic Saving graphics  Some notes to get us started:  This lesson makes liberal use of pipe chains, which bundle many operations together in one block of code. After last week’s lesson, you should be getting comfortable with these. Hopefully my heavy use of them in this (and the next) lesson will demonstrate how essential piping is to the cutting-edge of using R. Because piping is so essential that it appears nearly everywhere, I cannot painstakingly describe every step of every pipe chain—it will take too much of your attention away from what I’m trying to teach in the moment. If you want to dissect what a particular pipe chain is doing, you should run the chain up to a point and notice what the results look like when you stop the chain here as opposed to there. This is often how you write pipe chains as well—writing a line of code, checking the results, and planning what the next step should be. Plots created using ggplot generally look good by default. As we get started, however, they may look a little awkward. This is because I want to begin by demonstrating ggplot functionality (which we don’t want to rush though), not creating the prettiest plot imaginable. As we introduce more and more ggplot concepts, plots will begin to look better. If you want more help with ggplot, I’d recommend that you consult resources I list in the syllabus, in other lessons, and Hadley Wickham’s ggplot2 book (which you can build into a PDF on your own computer [after installing some other tools]). For advice about the principles of creating good graphics, you can also check books by folks such as Claus Wilke (author of the ggridges and cowplot packages for augmenting ggplot2) and Kieran Healy (author of the brilliant “Fuck Nuance” paper, which isn’t about graphics but is worth reading anyway).    Get started The ggplot2 package is loaded whenever you load the tidyverse package (but I sometimes load the ggplot2 package anyway because it sometimes gives your code editor better auto-complete behavior). We’ll also install and load labelled, which we’ll use to remove Stata labels from the data (we don’t need them).\nlibrary(\u0026quot;magrittr\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;ggplot2\u0026quot;) install.packages(\u0026quot;labelled\u0026quot;) Set your directory where it was last lesson and load the modified ANES dataset from the previous lesson:\n# set directory setwd(\u0026quot;~/path/to/wherever\u0026quot;) # read saved RDS data from last lesson # and remove Stata labels from the data anes \u0026lt;- readRDS(\u0026quot;data/anes-modified.RDS\u0026quot;) %\u0026gt;% mutate_all(labelled::remove_labels) # pkg::function lets us use functions without loading the full package # mutate_all applies a function to every variable in the dataset # similar to apply() functions, which we\u0026#39;ll cover next lesson Verify that you imported the correct dataset when you notice the modified variable names.\nnames(anes)  The grammar of graphics The grammar of graphics is essentially a theory/model that describing the components of a graphic. ggplot2 is an R package that implements the grammar in a layered fashion by iteratively adding grammatical components to a figure using functions.\nIt is not necessary to consciously memorize the theoretical components of the grammar of graphics—you get an unconscious feel for it—though we will briefly describe it. Suffice it to say that what matters most for you is that the underlying grammar provides a structure to ggplot2 that makes it easy to create complex graphics with an integrated syntax and carefully chosen defaults.\nHere are the core components of the grammar of graphics:\n Data: what gets visualized Aesthetic mappings: Attributes of the plot that come directly from the data (plot axes, color groupings) Geoms: geometric representations of the data (lines, points, etc.); the shapes used to present the values in your data in the plot Scales: modify how aesthetic mappings are presented. Every mapping from data to plot can be altered (e.g. changing colors, modifying axes, etc.) Coordinates: the plane on which you’re plotting. Most plots use a Cartesian plane (\\(x\\) and \\(y\\) coordinates), but less conventional planes are also possible (e.g. polar coordinates) Faceting system: how to plot subsets of data in different panels. Theme: other aesthetic minutia, such as fonts  Our examples today will start from the basics and build outward, touching on all of these components of the grammar.\nBecause ggplot is rooted in an underlying model, it’s easy to create many different types of plots that share an underlying philosophy and syntax structure. Even if you have never made a particular sort of plot before, you know how you would create it, because all plots come from the same building blocks. For instructional purposes, this is great because we will be making very interesting graphics in a very short amount of time.\n Using ggplot2 Graphics from ggplot2 begin with the titular ggplot() function. It works generically like so:\nggplot(dataset, aes(x = xvariable, y = yvariable)) where dataset is the name of your dataset, xvariable is the variable you want to plot on the horizontal axis, and yvariable is the variable you want to plot on the vertical axis. Some details:\n All ggplot graphics begin with a data frame. It must be tidy, because we must specify the columns that are mapped to plot aesthetics. As such, we can see why the other components of the Tidyverse work with ggplot—ggplot requires tidy data, and the rest of the Tidyverse is designed to make that easy. The aes() function maps variables in your data to an aesthetic feature of the plot. We will use aes() whenever we want to modify the plot’s according to features of the dataset. Every graphic will begin with a declaration of a dataset and a declaration of at least one aesthetic mapping.  ANES example Let’s see this with real data.\nWe will first recode two variables, “feeling thermometer” scores for the Democratic and Republican presidential candidates. These variables ask respondents to rate candidates on a 0-100 scale, where 100 is “most warm” and 0 is “most cold,” with 50 in the middle.\nIf we consult the codebook for information on the “feeling thermometer,” We will notice that all values 97-100 are represented with the number 97 in the codebook, while 98 and 99 are special codes for “don’t know” and missing responses. I have no idea why this decision was made, but we have to work with it. We’ll want to keep only values 0–97 as valid, and recode everything else to NA.\nOnce we do that, we will take the difference between the thermometer scores to create a relative candidate thermometer rating. Positive values will indicate that an individual feels more warmly to the Republican candidate than to the Democratic candidate, so we will calculate this as the Republican rating minus the Democratic rating.\n# learn some stuff about the feeling therm variables summary(anes$VCF0424) # democratic candidate ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## 0.00 40.00 60.00 58.74 85.00 99.00 27799 summary(anes$VCF0426) # republican candidate ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## 0.00 40.00 60.00 55.94 85.00 99.00 27799 # bonus: try the skim() function in the \u0026#39;skimr\u0026#39; package # skim() is like summarize(), but it creates data frames # now that we have a sense of the values of this variable, we\u0026#39;ll recode # keep 0-97 with case_when(), which implicitly converts all else to NA # then calculate the difference anes \u0026lt;- anes %\u0026gt;% mutate(therm_demcand = case_when(VCF0424 \u0026gt; 0 \u0026amp; VCF0424 \u0026lt;= 97 ~ VCF0424), therm_gopcand = case_when(VCF0426 \u0026gt; 0 \u0026amp; VCF0426 \u0026lt;= 97 ~ VCF0426), reltherm_cand = therm_gopcand - therm_demcand) We’ll plot the Republican candidate thermometer over the 7-point index of party ID, which we recoded last week.\n# data come from anes dataset # x = party ID, y = candidate thermometer difference ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) This creates a blank plot, but if you think about the grammar, you will understand what happened. We mapped these variables to the x and y aesthetics, but we haven’t chosen how to present the data yet.\n  Geoms It is important to note that we could visualize the data using a variety of geoms. It happens to be the cases that points make intuitive sense here.\nWe layer on grammatical components to a plot using the plus sign + after the ggplot() function. Each thing that we layer on is done with a function. For geoms, there are a bunch of functions that share the prefix geom_. Points are done with geom_point(), lines with geom_line(), and so on.\n# represent the data as points ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_point() ## Warning: Removed 33520 rows containing missing values (geom_point). What happened here?\n We got a warning about missing data not being plotted. This is helpful, but I will suppress these for the remainder of the lesson. We have a lot of data printing overtop one another because we have so many observations at relatively few combinations of values (only 7 \\(x\\) values).  Jittering points To prevent data from printing overtop one another, it is often helpful to jitter the points. Jittering is no more than adding a little bit of noise to the data. We also change the “shape” of the point. Learn more about the available point shapes here.\n# geom_jitter() plots jittered points # we jitter the points IN THE PLOT, not in the underlying data # we specify how much noise to add (width = ) # shape = 1 plots empty points # alpha controls the opacity of the points ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3)  Fit lines Let’s add a regression line with geom_smooth(). “Smooth” refers to any model-fit curve we might impose on the plot. We specify that we want a linear curve with method = lm. Adding a smooth also adds a confidence interval to the plot by default, but we can’t see this one because it is so narrow. I will also demonstrate how you can directly change colors (learn more about available colors here).\n# geom_smooth() adds a model fit line # method = \u0026quot;lm\u0026quot; specifies a Linear Model # By default, `geom_smooth()` fits a loess regression (method = \u0026quot;loess\u0026quot;) # for large datasets, it will fit a polynomial model (method = \u0026quot;gam\u0026quot;) ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3) + geom_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;red\u0026quot;) You could plot your own custom regression using the formula = argument.\nHere’s an important point. The regression line gets fit to the underlying data (the x and y aesthetics), so it does not get biased by the jittering. This is because jittering is only applied to the points as they are printed in the plot; it does not jitter the underlying data directly. This is an example of an important philosophical point about plotting: if you need to perform any weird tricks to make the plot look good, you should strive to modify the plot rather than the underlying data. Because ggplot is another “opinionated” package, it has many tools that make it easy to adhere to this philosophy.\n  More aesthetics (color, etc) As we said above, aesthetic mappings apply features of the data to some aesthetic property in the plot. So far, we have mapped two variables to \\(x\\) and \\(y\\) axes. We can also map colors, fills (interior colors), the sizes of points and lines sizes, line patterns (called “linetypes”), and so on.\nInstead of plotting one regression line, we’ll add separate regression lines for two different groups in the data. Let’s use gender, so first we need to recode it.\n# here is the raw gender variable in the data count(anes, VCF0104) ## # A tibble: 3 x 2 ## VCF0104 n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 0 103 ## 2 1 24862 ## 3 2 30709 # 0 means NA/missing # use case_when() to implicitly recode 0 as NA anes \u0026lt;- anes %\u0026gt;% mutate(gender = case_when(VCF0104 == 1 ~ \u0026quot;Men\u0026quot;, VCF0104 == 2 ~ \u0026quot;Women\u0026quot;)) Now that we have this variable cleaned up, we can map it to the color aesthetic.\n# create different colored lines by gender # that is, map color according to gender (from the data) # any time we want to map from the data, we must use aes(). ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender)) Simple! These colors are also automatically added to a legend. (If you’re ever created a legend using base graphics, you know how awesome it is that you no longer have to struggle with that).\nWhen you map a variable to an aesthetic, ggplot automatically picks some default values. This happens for colors, fills (e.g. the color of the confidence interval shading), point sizes, line widths, and so on. You can change defaults using scales, which we’ll describe below.\n Scales When you use aes(), ggplot picks some default mapping settings, such as axis scales, color defaults, and so on. Use scale_ functions to modify the defaults of any aesthetic mapping.\nFirst we’ll use it to control gender’s mapping by selecting our own colors.\n# aesthetic mapping makes default choices # modify those default mapping choices using scale_aes_something() functions # scale_color_brewer modifies color using color palettes # (can do your own googling about color palettes) ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender)) + scale_color_brewer(palette = \u0026quot;Set2\u0026quot;) Scale functions follow a format of scale_aesthetic_xyz(), where aesthetic is color, size, and so on. You can control color using color palettes with scale_color_brewer(). If you want to specify colors manually, use scale_color_manual() with the values = argument.\n# manual specification of color: scale_color_manual() # one color for each category from the aesthetic mapping # this will fail if you don\u0026#39;t supply the correct number of colors ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25,) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) Here we also map the confidence interval’s fill color and the linetype of the smooth using gender.\n# modifying more mappings # such as fill color, linetype # (Again, pardon the ugliness of the graphic # as we demonstrate general ggplot functionality) ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) Because x and y are also aesthetics, we can also modify them using scale functions.\n# axes are aesthetics, so you also modify axes with scale functions # breaks are where ticks are drawn # `labels =` can be used to specify the label for each break # minor_breaks can be specified separately from breaks ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) I can also add text labels to the tick positions.\n# Specify axis labels for each break # The `\\n` character inserts a line break. ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) Labeling aesthetics Related to aesthetics and scales is the labels we give these aesthetics. Notice that when we map variables to x and y, axis titles are created, and as we map variables to color, fill, and so on, a legend title was created. The variable names that get printed can be modified with the labs() function.\n# Specify aesthetic labels using labs() # good for axes, legend titles, also plot title (title = ), etc. ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = \u0026quot;Gender\u0026quot;) The labs() function only changes the names that you specify. To fix this dirty legend, I should specify the name for color, fill, and linetype all as \u0026quot;Gender\u0026quot;.\n# specify all labels ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = \u0026quot;Gender\u0026quot;, fill = \u0026quot;Gender\u0026quot;, linetype = \u0026quot;Gender\u0026quot;) I could also remove labels by specifying them as NULL. In this case, we’re pretty sure what men and women signify, so maybe including the “Gender” label is redundant. So we can suppress the legend title.\n# removing labels using NULL ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) NULL isn’t the same as NA. We use NA for missing data—there should be data here, but we don’t know what it is. We use NULL to mean “nothing.”\n  Coordinates Coordinates are not very complicated. Most of the time you will use a Cartesian grid, so you’ll probably do one of two things.\n Set the axis limits Flip the \\(x\\) and \\(y\\) dimensions.  First, set the axis limits. This is an unrealistic application just to demonstrate the functionality.\n# set axis limits using the coordinate system # coord_cartesian(xlim = , ylim =) ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + coord_cartesian(xlim = c(0, 8), ylim = c(-125, 125)) Remember, because the underlying values in the x aesthetic are 1 to 7, setting the xlim to 0 through 8 will pad each side by one unit. We use the c() function to pass a two-element vector of limits (minimum and maximum).\nSecond is flipping x and y, which is handy for dot plots and coefficient plots (which we’ll see next week). This is going to look strange in this example, of course, but again we’re just demonstrating the functionality.\n# invert the coordinate system with coord_flip() # can specify changes to the coordinate limits when you do this as well ggplot(data = anes, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + coord_flip(xlim = c(0.5, 7.5)) As above with jittering and smoothing, the x and y aesthetics themselves have not changed, only the way they are displayed in the graphic. The scale_x_continuous() function, therefore, still modifies the party ID variable scale even though it’s now the vertical axis. That’s because scale_x_continuous() modifies the x aesthetic, not the horizontal axis per se. This same intuition works when we specify xlim inside the coord_flip() function.\n Faceting Faceting is relatively simple. We use it to plot subsets of the data in different panels. There are two ways to do this.\n facet_wrap() creates horizontal panels that will “wrap” to the next line if we run out of space. facet_grid() creates a grid according to variables that you specify as the rows and columns.  Both of these functions require us to use tildes ~, so pay attention.\nFirst: facet_wrap using the election cycle variable (on a subset of presidential year data)\n# save only recent presidential years prez \u0026lt;- anes %\u0026gt;% filter(cycle %% 4 == 0) %\u0026gt;% filter(cycle \u0026gt;= 1992) # wrap panels using facet_wrap( ~ cycle) # the tilde is necessary ggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_wrap(~ cycle) Next: facet_grid(), which requires us to specify row and column variables. We’ll use election cycle and gender. This will make the legend redundant, but just ignore that for now.\nggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_grid(cycle ~ gender) Use a period in the facet_grid function to suppress rows or columns. Observe:\n# suppress rows ggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_grid(. ~ gender) # suppress columns ggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_grid(gender ~ .)  Themes Themes control ancillary aesthetic components of the plot. There are several preloaded themes that you can learn about here, and you can learn about other ggplot theme packages online.\nWe’ll use theme_bw(), in case you (like me) think this gray background is kinda nasty.\n# theme_bw() ggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_wrap(~ cycle) + theme_bw() There is also a theme() function, which allows you to change individual details of the theme without changing the entire theme. Here we will suppress the “minor” grid lines.\nggplot(prez, aes(x = pid7, y = reltherm_cand)) + geom_jitter(width = 0.2, shape = 1, alpha = 0.3, size = 0.25, color = \u0026quot;slategray\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, aes(color = gender, linetype = gender, fill = gender)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_linetype_manual(values = c(\u0026quot;dotted\u0026quot;, \u0026quot;solid\u0026quot;)) + scale_x_continuous(breaks = 1:7) + labs(x = \u0026quot;Party ID Index\u0026quot;, y = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, color = NULL, fill = NULL, linetype = NULL) + facet_wrap(~ cycle) + theme_bw() + theme(panel.grid.minor = element_blank())  Finishing touches and saving a graphic This looks like an okay graphic now, and it didn’t take much code at all!\nSaving this graphic is easy with the ggsave function. You generally want to save graphic as a “vector” image type in order to scale (that is, increase the size of) a graphic without any pixelation. PDF is usually good for papers and presentations.\n# generally want to save it into your paper\u0026#39;s dedicated folder # then inside the graphics subfolder # set height and width (usually requires some trial and error) ggsave(\u0026quot;paper/graphics/my-plot.PDF\u0026quot;, width = 7, height = 3)  Another example: party ID of men and women over time Let’s take these tools for a spin. Let’s re-create this graphic from an old paper from a former professor of mine.\nWe will need to set up a dataset that has all of the required components. Let’s think of what we need to have in place.\n x is election cycle, y is the percentage of men and women identifying as Democrat and Republican (aesthetics). We will need to calculate the y variable. We have separate point styles based on party (aesthetic) We have separate panels for men and women (faceting) the x axis scale has been modified, and y axis scale probably needs to be coerced to go from 0 to 100 (scales)  Let’s add some extra flair:\n calculate confidence intervals for each trend and plot as a “ribbon,” which is a shaded region. Unlike before, let’s make this as pretty as we can.  Data cleaning First, we have to create the data.\nTo get the confidence intervals, we’ll use a user-defined confidence interval function that can handle survey weights. We’ll discuss custom functions in the final lesson; for now, just know that this function calculates confidence intervals for proportions based on the number of “successes” and the number of observations. (Side note: you don’t need to calculate your own confidence intervals most of the time—this is a special case because survey weights create non-whole-numbers of successes and observations.)\n# Confidence intervals for proportions # based on the normal approximation to the binomial prop_ci \u0026lt;- function(successes, n, level = 0.05) { # get pt estimates and standard error p \u0026lt;- successes / n q \u0026lt;- 1 - p SE \u0026lt;- sqrt( (p * q) / n ) # upper and lower bound lower \u0026lt;- p - qnorm(1 - (level / 2) ) * SE upper \u0026lt;- p + qnorm(0.975) * SE # return data frame return(data.frame(estimate = p, lower, upper)) } To get the DV, we’ll need the weight variable and to collapse the party ID variable. (We want to keep Independents to calculate proportions, but we’ll drop them before plotting.)\n# copy weight variable and # collapse the party variable into Dems and Reps # code \u0026quot;leaners\u0026quot; as partisans anes \u0026lt;- anes %\u0026gt;% mutate(wt = VCF0009z, party = case_when(pid7 %in% 1:3 ~ \u0026quot;Dem\u0026quot;, pid7 == 4 ~ \u0026quot;Ind\u0026quot;, pid7 %in% 5:7 ~ \u0026quot;Rep\u0026quot;)) We will need to find the proportion of men and women in each party ID category, in each cycle. We’ll do this with a pipe chain:\n find the number of men and women in each party ID category in each cycle get proportions by dividing by the total number of men and women in each cycle use the number of successes and the “denominator” from above to find upper and lower bounds on the confidence interval trim extraneous stuff from the data  # You may want to run this chain line by line # to see what each step does # we want to plot Women left of Men, so we\u0026#39;ll reorder the gender variable pid_gendergap \u0026lt;- anes %\u0026gt;% select(cycle, gender, party, wt) %\u0026gt;% # trim unnecessary stuff group_by(gender, cycle, party) %\u0026gt;% # calc weighted n in each PID count(wt = wt) %\u0026gt;% rename(x = n) %\u0026gt;% # less ambiguous name group_by(gender, cycle) %\u0026gt;% # \u0026quot;denominator\u0026quot; for gender x cycle mutate(n = sum(x)) %\u0026gt;% ungroup() %\u0026gt;% # proportions and CIs mutate(prop = prop_ci(successes = x, n = n)$estimate, lower = prop_ci(successes = x, n = n)$lower, upper = prop_ci(successes = x, n = n)$upper) %\u0026gt;% # keep only Ds and Rs (drop NA and Ind) # keep only presidential years since 1952 reorder variables filter(party %in% c(\u0026quot;Dem\u0026quot;, \u0026quot;Rep\u0026quot;)) %\u0026gt;% filter( (cycle \u0026gt;= 1952) \u0026amp; ((cycle %% 4) == 0) ) %\u0026gt;% select(cycle, gender, party, prop:upper) %\u0026gt;% # reorder gender as a factor mutate(gender = fct_relevel(as.factor(gender), \u0026quot;Women\u0026quot;, \u0026quot;Men\u0026quot;)) %\u0026gt;% print() ## # A tibble: 64 x 6 ## cycle gender party prop lower upper ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1952 Men Dem 0.571 0.537 0.605 ## 2 1952 Men Rep 0.324 0.292 0.356 ## 3 1956 Men Dem 0.536 0.501 0.571 ## 4 1956 Men Rep 0.343 0.310 0.376 ## 5 1960 Men Dem 0.521 0.488 0.554 ## 6 1960 Men Rep 0.352 0.320 0.383 ## 7 1964 Men Dem 0.602 0.566 0.638 ## 8 1964 Men Rep 0.297 0.264 0.331 ## 9 1968 Men Dem 0.532 0.495 0.570 ## 10 1968 Men Rep 0.352 0.317 0.388 ## # … with 54 more rows Now we’ll use this dataset to build the plot.\nWe know we need separate panels for men and women, separate trends and linetypes for party.\n# use `group =` to plot separate line trends # try removing `group =` from the plot to see why we include it ggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_line(aes(group = party)) + geom_point(aes(shape = party)) Let’s modify some scales. We’ll want to specify the point styles and change the axis scales.\n by default, fill any empty point with while fill (in grom_point()) set the coordinates a little wider (coord_cartesian()) manually specify point shapes (scale_shape_manual()) modify the ticks on x and y (scale_x_continuous())  ggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2014), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1)) + scale_x_continuous(breaks = seq(1952, 2012, 12)) The X axis looks funky now. Let’s do some theme modifications to make the text diagonal and suppress busy grid lines.\nggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2016), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1)) + scale_x_continuous(breaks = seq(1952, 2012, 12)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8), panel.grid.minor = element_blank()) Let’s suppress the legend created by geom_point() and add text labels.\nggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;, show.legend = FALSE) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2016), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1)) + scale_x_continuous(breaks = seq(1952, 2012, 12)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8), panel.grid.minor = element_blank()) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.65, label = \u0026quot;Democrats\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.25, label = \u0026quot;Republicans\u0026quot;) If we want to add confidence intervals as ribbons, we’ll use geom_ribbon(). It inherits the x aesthetic from the plot, but we need to specify extra ymin and ymax aesthetics to draw the upper and lower bounds of the “polygon” that gets drawn. Again, suppress the legend, and for good measure, let’s modify the fill color.\n# I always prefer to add ribbons below points and lines # we want to reduce the ribbon opacity ggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = party), alpha = 0.4, show.legend = FALSE) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;, show.legend = FALSE) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2016), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1)) + scale_x_continuous(breaks = seq(1952, 2012, 12)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;orangered\u0026quot;)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8), panel.grid.minor = element_blank()) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.65, label = \u0026quot;Democrats\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.25, label = \u0026quot;Republicans\u0026quot;) We’ll want to convert the axis to percentages. We can do this in the plot using the scale_y_continuous() function and applying a function to the labels.\n# check the labels = argument in scale_y_continuous # it uses the percent() function from the \u0026#39;scales\u0026#39; package # scales is automatically installed with ggplot2 ggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = party), alpha = 0.4, show.legend = FALSE) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;, show.legend = FALSE) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2016), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent) + scale_x_continuous(breaks = seq(1952, 2012, 12)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;orangered\u0026quot;)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8), panel.grid.minor = element_blank()) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.65, label = \u0026quot;Democrats\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.25, label = \u0026quot;Republicans\u0026quot;) Now we’ll need to change the labels so they fit the scale transformation.\nggplot(pid_gendergap, aes(x = cycle, y = prop)) + facet_grid(. ~ gender) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = party), alpha = 0.4, show.legend = FALSE) + geom_line(aes(group = party)) + geom_point(aes(shape = party), fill = \u0026quot;white\u0026quot;, show.legend = FALSE) + coord_cartesian(ylim = c(0, 1), xlim = c(1948, 2016), expand = FALSE) + scale_shape_manual(values = c(17, 22)) + scale_y_continuous(breaks = seq(0, 1, .1), labels = scales::percent) + scale_x_continuous(breaks = seq(1952, 2012, 12)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;orangered\u0026quot;)) + theme_bw() + theme(axis.text.x = element_text(angle = 45, vjust = 0.8), panel.grid.minor = element_blank()) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.65, label = \u0026quot;Democrats\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 1984, y = 0.25, label = \u0026quot;Republicans\u0026quot;) + labs(x = NULL, y = \u0026quot;Percent (including leaners)\u0026quot;, title = \u0026quot;Party Identification of Men and Women\u0026quot;, caption = \u0026quot;Source: NES Surveys of the indicated years\u0026quot;)   Other graphics There are plenty of other geoms that use slightly different aesthetics. Let’s just breeze through some now.\nAs we do this, let’s use theme_bw() as the default. Here’s a trick to set the default ggplot theme.\ntheme_set(theme_bw()) Histograms and density plots Histograms and density plots, because they only show the distribution of one variable, only need one axis aesthetic (x).\n# subset the anes data # pipe into the ggplot function # (can do that! but not usually recommended) anes %\u0026gt;% filter(!is.na(party)) %\u0026gt;% ggplot(aes(x = reltherm_cand)) + geom_histogram(aes(color = party)) + labs(x = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, y = \u0026quot;Frequency\u0026quot;, color = \u0026quot;Partisanship\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 31866 rows containing non-finite values (stat_bin). Histogram bars are stacked by default, which is weird. Let’s make them dodge each other.\n# make histogram bars non-stacked (\u0026quot;dodge\u0026quot;) anes %\u0026gt;% filter(!is.na(party)) %\u0026gt;% ggplot(aes(x = reltherm_cand)) + geom_histogram(aes(color = party, fill = party), alpha = 0.5, position = \u0026quot;dodge\u0026quot;) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;gray20\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;gray20\u0026quot;, \u0026quot;maroon\u0026quot;)) + labs(x = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, y = \u0026quot;Frequency\u0026quot;, color = \u0026quot;Partisanship\u0026quot;, fill = \u0026quot;Partisanship\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 31866 rows containing non-finite values (stat_bin). You can also do kernel density estimates (which allow you set the kernel bandwidth):\n# density plot instead of histogram anes %\u0026gt;% filter(!is.na(party)) %\u0026gt;% ggplot(aes(x = reltherm_cand)) + geom_density(aes(color = party, fill = party), alpha = 0.5, bw = 7) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;gray20\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;gray20\u0026quot;, \u0026quot;maroon\u0026quot;)) + labs(x = \u0026quot;Relative Candidate Thermometer Score\u0026quot;, y = \u0026quot;Density\u0026quot;, color = \u0026quot;Partisanship\u0026quot;, fill = \u0026quot;Partisanship\u0026quot;) ## Warning: Removed 31866 rows containing non-finite values (stat_density).  Bar graphs It’s a personal preference of mine that I think just about anything that you can show with a bar graph would be better with a dot plot. I tend to think that bar graphs are awkward and misused (e.g. for experimental treatment effects). In my head, bars should be for counts or other things where zero is a hard lower bound (such as proportions).\nBah! Anyway, here’s how you do them.\nLet’s make some data of the Democratic share of the two-party vote choice for each partisanship category.\n# dummy variable for Dem vote (0 = Rep vote, else NA) anes \u0026lt;- anes %\u0026gt;% mutate(vote = VCF0706, vote = case_when(vote == 1 ~ \u0026quot;Democratic Candidate\u0026quot;, vote == 2 ~ \u0026quot;Republican Candidate\u0026quot;)) # calculate averages for each party ID # presidential years since 2000 mean_votes \u0026lt;- anes %\u0026gt;% filter(cycle \u0026gt;= 2000 \u0026amp; (cycle %% 4) == 0) %\u0026gt;% filter(!is.na(pid7)) %\u0026gt;% group_by(cycle, pid7, vote) %\u0026gt;% # Numerator count(wt = wt) %\u0026gt;% filter(!is.na(vote)) %\u0026gt;% rename(x = n) %\u0026gt;% group_by(pid7, cycle) %\u0026gt;% # Denominator mutate(n = sum(x, na.rm = TRUE), prop = x / n) %\u0026gt;% filter(vote == \u0026quot;Democratic Candidate\u0026quot;) %\u0026gt;% print() ## # A tibble: 28 x 6 ## # Groups: pid7, cycle [28] ## cycle pid7 vote x n prop ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2000 1 Democratic Candidate 226. 233. 0.970 ## 2 2000 2 Democratic Candidate 139. 163. 0.855 ## 3 2000 3 Democratic Candidate 105. 135. 0.777 ## 4 2000 4 Democratic Candidate 32.8 73.5 0.446 ## 5 2000 5 Democratic Candidate 19.5 136. 0.144 ## 6 2000 6 Democratic Candidate 20.8 128. 0.163 ## 7 2000 7 Democratic Candidate 3.20 181. 0.0177 ## 8 2004 1 Democratic Candidate 137. 140. 0.975 ## 9 2004 2 Democratic Candidate 94.1 111. 0.851 ## 10 2004 3 Democratic Candidate 104. 119. 0.878 ## # … with 18 more rows And then plot these averages.\n# geom_col to create a column as tall as the (x, y) point ggplot(mean_votes, aes(x = pid7, y = prop)) + geom_col(fill = \u0026quot;gray50\u0026quot;) + facet_wrap(~ cycle) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Ind\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + labs(x = \u0026quot;Party ID\u0026quot;, y = \u0026quot;Democratic Share of Two-Party Vote\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) Here is the same plot as a dot plot, also changing the scales.\n# dots instead of bars ggplot(mean_votes, aes(x = pid7, y = prop)) + geom_point() + facet_wrap(~ cycle) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Ind\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + scale_y_continuous(labels = scales::percent) + labs(x = \u0026quot;Party ID\u0026quot;, y = \u0026quot;Democratic Share of Two-Party Vote\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) And you can often rotate dot plots so that their labels have more space to print out.\n# rotate the dot plot for axis readability # this should be used carefully... # not all plots are easy to read like this ggplot(mean_votes, aes(x = pid7, y = prop)) + geom_point() + facet_wrap(~ cycle) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong Democrat\u0026quot;, \u0026quot;Weak Democrat\u0026quot;, \u0026quot;Lean Democrat\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean Republican\u0026quot;, \u0026quot;Weak Republican\u0026quot;, \u0026quot;Strong Republican\u0026quot;)) + scale_y_continuous(breaks = seq(0, 1, .2), labels = scales::percent) + labs(x = \u0026quot;Party Identification\u0026quot;, y = \u0026quot;Democratic Share of Two-Party Vote\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) + coord_flip() There are some other things you can do with geoms. One thing you can do for dot plots is use geom_pointrange, which adds a point and an error bar. Like geom_ribbon, it takes a ymin and ymax aesthetic. This makes it good for point estimates and confidence intervals. In this case though, we’ll use it in a slightly hacky way to help us trace which point goes with which category, making it a little easier to read the plot. Notice how I set the min and max aesthetics.\n# set ymin = 0, ymax = prop. # the error bar goes from 0 to the point ggplot(mean_votes, aes(x = pid7, y = prop)) + geom_pointrange(aes(ymin = 0, ymax = prop), linetype = \u0026quot;dotted\u0026quot;) + facet_wrap(~ cycle) + scale_x_continuous(breaks = 1:7, labels = c(\u0026quot;Strong Democrat\u0026quot;, \u0026quot;Weak Democrat\u0026quot;, \u0026quot;Lean Democrat\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean Republican\u0026quot;, \u0026quot;Weak Republican\u0026quot;, \u0026quot;Strong Republican\u0026quot;)) + scale_y_continuous(breaks = seq(0, 1, .2), labels = scales::percent) + labs(x = \u0026quot;Party Identification\u0026quot;, y = \u0026quot;Democratic Share of Two-Party Vote\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) + coord_flip()   Categorical variables (strings and factors) If you plot a categorical variable on one of the axes, ggplot will automatically interpret it and plot categories side-by-side. We’ll demonstrate this with a toy example.\nOne potential problem, though, is that ggplot will order categorical variables alphabetically or, if the variable is already an ordered factor, will interpret the factor order out of the metadata about the levels.\nHere we show what happens when we plot a string variable that can be ordered alphabetically:\n# x and y: continuous variables # s: string variable # f: factor variable with weird order d \u0026lt;- data_frame(x = rnorm(100, 0, 1), y = 4 + (2 * x) + rnorm(100, 0, 1), s = sample(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;), 100, replace = TRUE), f = factor(s, levels = c(\u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;))) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. # plot y as a function of s (jittering the points) ggplot(d, aes(x = s, y = y)) + geom_jitter(width = 0.15) + labs(x = \u0026quot;String\u0026quot;, y = \u0026quot;Continuous Y\u0026quot;)  …and if we plot from a factor that already has ordered levels…\n# plot y as f(f), jittered points # notice weird order. This comes from the underlying data ggplot(d, aes(x = f, y = y)) + geom_jitter(width = 0.15) + labs(x = \u0026quot;Ordered Factor\u0026quot;, y = \u0026quot;Continuous Y\u0026quot;)  …or we wanted to assign colors using an ordered factor.\n# same situation here ggplot(d, aes(x = x, y = y)) + geom_point(aes(color = f)) + labs(x = \u0026quot;Continuous X\u0026quot;, y = \u0026quot;Continuous Y\u0026quot;, color = \u0026quot;Ordered Factor\u0026quot;)  To reorder these categories, I recommend using tools in the forcats package, which, conveniently, is part of tidyverse.\n fct_relevel(): take a factor variable and reorder the levels. Any level not listed during reordering is given the same precedence as the original variable fct_rev(): reverse the order of a factor’s levels fct_recode(): change the levels (text labels) of a factor  Here is an example where we take the f variable, a factor with ordered levels, and rearrange the order of the levels so that they appear in the legend with the proper order. This is called “releveling” (changing the order of levels), as opposed to “recoding” (changing the labels themselves).\n# start with the d object # mutate # create fixed_f, which is f but with levels of specified order fixed_d \u0026lt;- d %\u0026gt;% mutate(fixed_f = fct_relevel(f, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;)) %\u0026gt;% print() ## # A tibble: 100 x 5 ## x y s f fixed_f ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 -0.217 4.59 d d d ## 2 0.645 5.43 c c c ## 3 1.27 5.32 a a a ## 4 0.831 5.71 b b b ## 5 1.35 3.75 c c c ## 6 -0.563 2.65 c c c ## 7 -0.815 1.74 c c c ## 8 0.906 6.17 c c c ## 9 -0.555 3.80 c c c ## 10 -1.07 0.600 b b b ## # … with 90 more rows You’ll note that the variables f and fixed_f now have the same values, but the ordering of fixed_f’s levels has been modified.\n# f as color ggplot(fixed_d, aes(x = x, y = y)) + geom_point(shape = 1, aes(color = fixed_f)) + labs(x = \u0026quot;Continuous X\u0026quot;, y = \u0026quot;Continuous Y\u0026quot;, color = \u0026quot;Ordered Factor\u0026quot;)  # f as x axis ggplot(fixed_d, aes(x = f, y = y)) + geom_point(shape = 1) + labs(x = \u0026quot;Ordered Factor\u0026quot;, y = \u0026quot;Continuous Y\u0026quot;)  An aside on stringr The stringr package contains a few useful functions for modifying character vectors.\nLet’s take the state.name vector that comes with R.\nstate.name ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ## [5] \u0026quot;California\u0026quot; \u0026quot;Colorado\u0026quot; \u0026quot;Connecticut\u0026quot; \u0026quot;Delaware\u0026quot; ## [9] \u0026quot;Florida\u0026quot; \u0026quot;Georgia\u0026quot; \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; ## [13] \u0026quot;Illinois\u0026quot; \u0026quot;Indiana\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;Kansas\u0026quot; ## [17] \u0026quot;Kentucky\u0026quot; \u0026quot;Louisiana\u0026quot; \u0026quot;Maine\u0026quot; \u0026quot;Maryland\u0026quot; ## [21] \u0026quot;Massachusetts\u0026quot; \u0026quot;Michigan\u0026quot; \u0026quot;Minnesota\u0026quot; \u0026quot;Mississippi\u0026quot; ## [25] \u0026quot;Missouri\u0026quot; \u0026quot;Montana\u0026quot; \u0026quot;Nebraska\u0026quot; \u0026quot;Nevada\u0026quot; ## [29] \u0026quot;New Hampshire\u0026quot; \u0026quot;New Jersey\u0026quot; \u0026quot;New Mexico\u0026quot; \u0026quot;New York\u0026quot; ## [33] \u0026quot;North Carolina\u0026quot; \u0026quot;North Dakota\u0026quot; \u0026quot;Ohio\u0026quot; \u0026quot;Oklahoma\u0026quot; ## [37] \u0026quot;Oregon\u0026quot; \u0026quot;Pennsylvania\u0026quot; \u0026quot;Rhode Island\u0026quot; \u0026quot;South Carolina\u0026quot; ## [41] \u0026quot;South Dakota\u0026quot; \u0026quot;Tennessee\u0026quot; \u0026quot;Texas\u0026quot; \u0026quot;Utah\u0026quot; ## [45] \u0026quot;Vermont\u0026quot; \u0026quot;Virginia\u0026quot; \u0026quot;Washington\u0026quot; \u0026quot;West Virginia\u0026quot; ## [49] \u0026quot;Wisconsin\u0026quot; \u0026quot;Wyoming\u0026quot; Here are some useful functions. Play with these on your own.\n# look for pattern, return a logical vector str_detect(state.name, \u0026quot; \u0026quot;) # spaces # example: return all two-word state names state.name[str_detect(state.name, \u0026quot; \u0026quot;)] # substring based on character position str_sub(state.name, 1L, 3L) str_sub(state.name, -3L, -1L) # cut a string based on a pattern match (returns a list) str_split(state.name, pattern = \u0026quot; \u0026quot;) # grab the nth element of a split string list # in this case, the second element (x[2]) sapply(str_split(state.name, pattern = \u0026quot; \u0026quot;) , function(x) x[2]) # replace a pattern with another pattern str_replace(state.name, pattern = \u0026quot; \u0026quot;, replacement = \u0026quot;-\u0026quot;) # for strings that might contain multiple instances of the pattern # (won\u0026#39;t notice a difference here) str_replace_all(state.name, pattern = \u0026quot; \u0026quot;, replacement = \u0026quot;-\u0026quot;)   Concluding notes Why plot You should be plotting your data often, even if you don’t expect the plot to make it into your final product.\n Plotting shows you the distribution of your data, which can help you decide on certain modeling assumptions or variable transformations. (You should not rely entirely on plots for this…let theory guide you) Plotting shows outliers, which may help you think about unmodeled covariates. Plotting gives you a sense of the underlying trends in your data so you can be a better thinker about the problem at hand.  You can see Hadley Wickham do some exploratory analysis here, which shows how graphics (and ggplot in particular) facilitates principled and fast exploration of your data.\n Data and ggplot If you haven’t already picked up on it, ggplot is designed to complement the philosophy of tidy data. It does this mainly by forcing you to declare a data frame from which you are plotting. This helps ensure that the data you are plotting are conceptually related and can be easily mapped to plot aesthetics. And in order to shape your data so that these mappings are done correctly, the tools in tidyr and dplyr are extremely helpful.\nAs we have seen, many tools in ggplot are designed to prevent you from modifying the underlying data too much. This is generally a tenet of the Tidyverse philosophy. Your original data frames are sacrosanct, and they serve as the raw material from which analyses and plots are extracted. This is why pipe chains begin with a tidy dataset and work out from there. Changes that you make to the data that only exist for the purpose of plotting (or modeling, etc.) should be temporary, as they are not essential to the core underlying data.\nSay that you have raw data that has an ugly text label, and every time you plot that variable, you want a prettier text label. Rather than change the underlying data, you might make another table that serves as a dictionary for converting the ugly label to a prettier label. You can then merge this table into your data just before plotting. Practices such as this keep your original data free of extraneous stuff while still allowing you to produce beautiful graphics.\n Graphics approaches, base vs ggplot Base graphics, on the other hand, do not rely on an underlying data model in order to work. You can plot points, lines, labels, and so on, with no conceptual connection between the data used for any of these geoms. Furthermore, legends and labels are entirely detached from the data. This makes base graphics inefficient and prone to human error in a number of ways, mainly because updates to the elements of your graphics need to made separately from updates to legends, labels, axes and so on. These elements are all conceptually independent in base graphics, so they have no knowledge about how you change the data or the appearance of the graphics. That said, it is definitely possible to create beautiful graphics using base tools; it will just take more work and more time.\nAll this being said, base graphics do have some advantage on ggplot graphics if you have to create a plot that contains many unrelated elements that don’t share a common data frame. (It is possible to plot from multiple data frames in ggplot, but it isn’t easy.)\nYou can find plenty of disagreements online about how and when base or ggplot tools are better. I know no human being who has learned ggplot and opted for base graphics, so I’m making the bet that this lesson will lead you to the same conclusion.\n Other graphical Dos and Don’ts Some general advice for plotting:\n Don’t use multiple Y axes. It’s easy to use multiple Y axis scales to make variables look more or less related (see here), so you shouldn’t do it. In fact, for the longest time, ggplot would not allow you to create a second y-axis (because it’s opinionated like that). If you can label points or trends in the plot rather than in a legend, do so. You can do this with geom_text or the annotate() function. Try to make your graphics as minimal as possible. Add elements where it improves the graphic. Otherwise, you don’t want to create clutter. If you can use aesthetic features besides color to distinguish groups, that would be ideal. Some forms of color-blindness are remarkably common, and at any rate, journals may not allow your article to print in color. It’s best to get mentally prepared to use more line types, point styles, and so on, to distinguish groups of data. If you must use colors, you should check out some colorblindness-friendly color palettes and packages. You could check out the dichromat package to investigate how your color selections look to individuals with various forms of colorblindness. The viridis package also provides a variety of scales for colorblind-friendly graphics.   Other gg tools There are many extensions for ggplot that you can install. Rather than tediously describe them, I’ll point you to a web page discussing many of the tools out there. These let you create heatmaps, choropleths, network graphs, mosaic plots, autocorrelation plots, and much more. Extensions I have used in the past include ggfortify, ggridges, ggthemes, ggsci, GGally, and ggalt.\nMany more extensions are in development and cannot be found on CRAN, but they can be downloaded and installed. One that I particularly like is patchwork, which combines multiple graphics into the same image using an extremely simple syntax.\n Graphics in future lessons We will further explore graphics using ggplot in the final lesson on statistical analysis. In particular, we’ll discuss how to use ggplot tools to summarize statistical models and create post-estimation graphics.\n  Save data from this session saveRDS(anes, \u0026quot;data/anes-modified-2.RDS\u0026quot;)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"0ac4e3b43aa88cc20b229574719b1696","permalink":"/courses/811/811-04-graphics/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/811/811-04-graphics/","section":"courses","summary":"Read this before our second R lecture, after the data lesson.\nHow to follow along A script file walking through some of these commands is available here.\n Objectives The goal of this lesson is to provide an introduction to graphics in R, by way of ggplot2 in particular. We will cover:\n The “grammar of graphics”—the “gg” in ggplot How ggplot2 works Common graphics in social science Customizing the appearance of ggplot graphic Saving graphics  Some notes to get us started:  This lesson makes liberal use of pipe chains, which bundle many operations together in one block of code.","tags":null,"title":"Graphics","type":"docs"},{"authors":null,"categories":null,"content":" Read this before our final lecture, after the graphics lesson.\nHow to follow along A script file walking through some of these commands is available here.\n Objectives In this lesson, we will introduce how to do statistical analysis using R. Topics that you should cover to prepare for the take-home exercise include…\n Means, confidence intervals, and simple significance tests Estimating regression models Generating model output Model diagnostics and fit statistics Post-estimation graphics (model predictions)  This page also contains some content on more advanced topics, but these won’t be necessary for the take-home exercise. These include…\n intermediate R tools and routines  lists apply() functions mapping functions to nested data frames custom functions type coercion  a reference list of tools for advanced analysis  time series panel models multilevel models Bayesian models etc.   Since we should be getting used to R, I will sprinkle some more interesting data manipulation tricks into the analysis. Pay careful attention, as some of these tricks may come in handy in the future! As always, I recommend you run pipe chains chunk by chunk so you can see how each function in the chain contributes to the final result.\n Data and packages Load packages.\nlibrary(\u0026quot;Rmisc\u0026quot;) library(\u0026quot;magrittr\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;ggplot2\u0026quot;) library(\u0026quot;broom\u0026quot;) Set the default ggplot theme.\ntheme_set(theme_bw()) Set your directory and read data from that we saved at the end of the previous lesson.\nsetwd(\u0026quot;~/path/to/wherever\u0026quot;) anes \u0026lt;- readRDS(\u0026quot;data/anes-modified-2.RDS\u0026quot;) %\u0026gt;% print()  Non-regression analysis Previous lessons covered two major types of non-statistical analysis. We saw how to create some simple tables of variables in your data (using either the table() function or the count() function). We also saw how to create graphics, which are a major arena of non-statistical analysis.\nEstimates and confidence intervals When we generate estimates from data, we are usually interested in the point estimate and the uncertainty in that estimate.\nThe Rmisc package has CI and group.CI functions for estimating means with confidence intervals. The group.CI() function is better than CI() for several reasons, so we’ll use for subgroup estimates and ungrouped estimates.\nFor ungrouped estimates, we use the following syntax.\n# CIs for no groups # place a `1` where you\u0026#39;d otherwise put a group variable group.CI(libcon_self ~ 1, data = anes) ## libcon_self.upper libcon_self.mean libcon_self.lower ## 1 4.254325 4.238207 4.222089 Remember that every confidence interval has associated assumptions. This interval assumes that the sampling distribution of the mean is normally distributed. This is often a fine assumption, but the fact that we have only seven valid values makes this variable slightly problematic. (You would probably not get pushback for doing this though, because most people don’t think about these assumptions).\nYou can add a grouping variable like so. We’ll estimate the mean ideological self-placement within each party ID on the 7-point partisanship index.\ngroup.CI(libcon_self ~ pid7, data = anes) ## pid7 libcon_self.upper libcon_self.mean libcon_self.lower ## 1 1 3.475202 3.435936 3.396670 ## 2 2 3.887528 3.854468 3.821408 ## 3 3 3.669637 3.630272 3.590906 ## 4 4 4.231836 4.190113 4.148389 ## 5 5 4.779278 4.740784 4.702289 ## 6 6 4.804295 4.771926 4.739557 ## 7 7 5.502972 5.468599 5.434226 The group.CI() function returns a data frame, so you could plot this pretty easily. We’ll add a dividing line at “moderate.”\n# calculate mean self-placement, by party ID # since 2000 only mean_ideo \u0026lt;- anes %\u0026gt;% filter(cycle \u0026gt;= 2000) %\u0026gt;% group.CI(libcon_self ~ pid7, data = .) %\u0026gt;% print() ## pid7 libcon_self.upper libcon_self.mean libcon_self.lower ## 1 1 3.289021 3.227399 3.165776 ## 2 2 3.703974 3.641940 3.579906 ## 3 3 3.625136 3.561119 3.497103 ## 4 4 4.247556 4.181102 4.114649 ## 5 5 4.963396 4.895777 4.828159 ## 6 6 5.002320 4.943096 4.883872 ## 7 7 5.759156 5.706492 5.653827 # plot self-placement over party ID # modifying axis scales # confidence intervals are there, just small ggplot(mean_ideo, aes(x = as.factor(pid7), y = libcon_self.mean)) + geom_hline(yintercept = 4, color = \u0026quot;gray50\u0026quot;) + geom_pointrange(aes(ymin = libcon_self.lower, ymax = libcon_self.upper)) + labs(y = \u0026quot;Ideological Self-Placement\u0026quot;, x = \u0026quot;Party ID\u0026quot;) + scale_x_discrete(labels = c(\u0026quot;Strong\\nDem\u0026quot;, \u0026quot;Weak\\nDem\u0026quot;, \u0026quot;Lean\\nDem\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Lean\\nRep\u0026quot;, \u0026quot;Weak\\nRep\u0026quot;, \u0026quot;Strong\\nRep\u0026quot;)) + scale_y_continuous(breaks = 1:7, labels = c(\u0026quot;Very Lib\u0026quot;, \u0026quot;Lib\u0026quot;, \u0026quot;Slight Lib\u0026quot;, \u0026quot;Moderate\u0026quot;, \u0026quot;Slight Con\u0026quot;, \u0026quot;Con\u0026quot;, \u0026quot;Very Con\u0026quot;)) + coord_cartesian(ylim = c(1, 7)) + theme(panel.grid.minor = element_blank()) Be warned. Because the group.CI() function returns a data frame and not just a single value, it will make dplyr::summarize() upset if you try to group a data frame and estimate that way. We can show an advanced way of dealing with this later (nesting and mapping).\nLet’s do one more example where we try to detect some evidence of ideological polarization/sorting over time. We’ll track how the mean ideology among Democrats and Republicans changes over time. We use the as_data_frame() function to convert the table to a tibble (for nicer printing).\n# collapse party into Ds and Rs, else NA # Find mean self-placement in each party, in each cycle # convert to data_frame for prettier printing sorting \u0026lt;- anes %\u0026gt;% mutate(party = case_when(pid7 %in% 1:3 ~ \u0026quot;Democrat\u0026quot;, pid7 %in% 5:7 ~ \u0026quot;Republicans\u0026quot;)) %\u0026gt;% group.CI(libcon_self ~ party + cycle, data = .) %\u0026gt;% as_data_frame() %\u0026gt;% print() ## Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. ## # A tibble: 38 x 5 ## party cycle libcon_self.upper libcon_self.mean libcon_self.lower ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Democrat 1972 3.86 3.77 3.68 ## 2 Republicans 1972 4.73 4.64 4.55 ## 3 Democrat 1974 3.84 3.73 3.63 ## 4 Republicans 1974 4.85 4.74 4.62 ## 5 Democrat 1976 3.89 3.79 3.70 ## 6 Republicans 1976 4.98 4.88 4.79 ## 7 Democrat 1978 3.85 3.76 3.67 ## 8 Republicans 1978 4.96 4.86 4.76 ## 9 Democrat 1980 3.95 3.83 3.70 ## 10 Republicans 1980 5.09 4.98 4.86 ## # … with 28 more rows # plot ideological self-placement # Democrats and Republicans across time ggplot(sorting, aes(x = cycle, y = libcon_self.mean, color = party)) + geom_line(show.legend = FALSE) + geom_point(show.legend = FALSE) + geom_ribbon(aes(ymin = libcon_self.lower, ymax = libcon_self.upper, fill = party), alpha = 0.3, color = NA, show.legend = FALSE) + coord_cartesian(ylim = c(1, 7)) + annotate(\u0026quot;text\u0026quot;, x = 2000, y = 2.5, label = \u0026quot;Democrats\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 2000, y = 6.25, label = \u0026quot;Republicans\u0026quot;) + scale_y_continuous(breaks = 1:7, labels = c(\u0026quot;Very\\nLiberal\u0026quot;, \u0026quot;Liberal\u0026quot;, \u0026quot;Slightly\\nLiberal\u0026quot;, \u0026quot;Moderate\u0026quot;, \u0026quot;Slightly\\nConservative\u0026quot;, \u0026quot;Conservative\u0026quot;, \u0026quot;Very\\nConservative\u0026quot;)) + scale_x_continuous(breaks = seq(1972, 2012, 8)) + scale_color_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + scale_fill_manual(values = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;maroon\u0026quot;)) + labs(x = \u0026quot;Election Cycle\u0026quot;, y = \u0026quot;Mean Ideological Self-Placement\u0026quot;, color = NULL, fill = NULL) + theme(panel.grid.minor = element_blank())  Proportions For proportions, one could use prop.test() for the normal approximation method (which most people learn in school), or binom.test() for “exact” Clopper-Pearson intervals, which have better boundary assumptions and small-sample properties (they are estimated using quantiles of the beta distribution), but they can be conservative (a little wide, more than 95% coverage in some cases). They both work in R basically the same way though.\n# find the number of democratic voters in 2012, say. Sum the TRUEs dem_voters \u0026lt;- anes %\u0026gt;% filter(cycle == 2012) %$% sum(vote == \u0026quot;Democratic Candidate\u0026quot;, na.rm = TRUE) %\u0026gt;% print() ## [1] 2496 # find the num. of major party voters in 2012 twoparty_voters \u0026lt;- anes %\u0026gt;% filter(cycle == 2012) %$% sum(vote %in% c(\u0026quot;Democratic Candidate\u0026quot;, \u0026quot;Republican Candidate\u0026quot;), na.rm = TRUE) %\u0026gt;% print() ## [1] 4188 # estimate demvoters / majorparty voters, with CI results \u0026lt;- anes %$% prop.test(dem_voters, twoparty_voters) # view results results ## ## 1-sample proportions test with continuity correction ## ## data: dem_voters out of twoparty_voters, null probability 0.5 ## X-squared = 153.97, df = 1, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.5809257 0.6108740 ## sample estimates: ## p ## 0.5959885 # this is a complex object. See what\u0026#39;s inside of it attributes(results) ## $names ## [1] \u0026quot;statistic\u0026quot; \u0026quot;parameter\u0026quot; \u0026quot;p.value\u0026quot; \u0026quot;estimate\u0026quot; \u0026quot;null.value\u0026quot; ## [6] \u0026quot;conf.int\u0026quot; \u0026quot;alternative\u0026quot; \u0026quot;method\u0026quot; \u0026quot;data.name\u0026quot; ## ## $class ## [1] \u0026quot;htest\u0026quot; # let\u0026#39;s grab the point estimate. # We can use $ to go \u0026quot;inside\u0026quot; this object results$estimate ## p ## 0.5959885 # grab the confidence interval in the same way # it is a two-element vector (with some metadata) results$conf.int ## [1] 0.5809257 0.6108740 ## attr(,\u0026quot;conf.level\u0026quot;) ## [1] 0.95 As you can see, these old hypothesis testing functions produce weird objects as output, making them feel ancient and complex. It gets tougher to organize mentally if you need to estimate proportions for multiple groups. I’ll show you how, but it’s a little complicated.\n# Objective # a numerator and a denominator in every row # group the data and apply the test function to each row # groups = party ID # find num of dem voters (numerator) and major party voters (denominator) # In this chain, we take the sum of a logical variable # in arithmetic, logical variables are like dummy variables # (TRUE is treated as a 1, FALSE as a 0) grp_raw \u0026lt;- anes %\u0026gt;% filter(cycle == 2012) %\u0026gt;% filter(!is.na(pid7)) %\u0026gt;% group_by(pid7) %\u0026gt;% summarize(dem_voters = sum(vote == \u0026quot;Democratic Candidate\u0026quot;, na.rm = TRUE), twoparty_voters = sum(vote %in% c(\u0026quot;Democratic Candidate\u0026quot;, \u0026quot;Republican Candidate\u0026quot;), na.rm = TRUE)) %\u0026gt;% print() ## # A tibble: 7 x 3 ## pid7 dem_voters twoparty_voters ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 1219 1234 ## 2 2 519 605 ## 3 3 451 486 ## 4 4 174 331 ## 5 5 47 428 ## 6 6 61 448 ## 7 7 18 645 # think of binom.test() as being run separately for each row (group) # since each row is a group in this case # separately save mean, lower and upper bounds grp_prop \u0026lt;- grp_raw %\u0026gt;% group_by(pid7) %\u0026gt;% mutate(prop = binom.test(dem_voters, twoparty_voters)$estimate, lower = binom.test(dem_voters, twoparty_voters)$conf.int[1], upper = binom.test(dem_voters, twoparty_voters)$conf.int[2]) %\u0026gt;% print() ## # A tibble: 7 x 6 ## # Groups: pid7 [7] ## pid7 dem_voters twoparty_voters prop lower upper ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1219 1234 0.988 0.980 0.993 ## 2 2 519 605 0.858 0.827 0.885 ## 3 3 451 486 0.928 0.901 0.949 ## 4 4 174 331 0.526 0.470 0.581 ## 5 5 47 428 0.110 0.0818 0.143 ## 6 6 61 448 0.136 0.106 0.171 ## 7 7 18 645 0.0279 0.0166 0.0437 # plot estimates with CIs as pointranges (points plus error bars) ggplot(grp_prop, aes(x = as.factor(pid7), y = prop)) + geom_pointrange(aes(ymin = lower, ymax = upper)) + labs(x = \u0026quot;Party ID\u0026quot;, y = \u0026quot;Democratic share of two-party vote\u0026quot;, caption = \u0026quot;ANES 2012 data\u0026quot;) Some people have developed tools to make it easier to work with these old functions. The broom package is amazing one. Let’s use the broom::tidy function to clean up the output from the prop.test() function from above.\n# results object from before results  ## ## 1-sample proportions test with continuity correction ## ## data: dem_voters out of twoparty_voters, null probability 0.5 ## X-squared = 153.97, df = 1, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.5809257 0.6108740 ## sample estimates: ## p ## 0.5959885 # tidy results tidy(results) ## # A tibble: 1 x 8 ## estimate statistic p.value parameter conf.low conf.high method ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.596 154. 2.36e-35 1 0.581 0.611 1-sam… ## # … with 1 more variable: alternative \u0026lt;chr\u0026gt; The tidy() function returns a tidy frame with columns for estimates, test statistics, \\(p\\)-values, confidence interval bounds, and so on. You could run tidy() on lots of different proportions tests, stack them into one data frame, and then plot the results in cool ways. We’ll do something like that later when we cover regression models.\nFor formal hypothesis testing of means, the t.test() function works a lot like prop.test() and binom.test(). I won’t beat this lesson to death though.\n  Regression And now, the good stuff.\nR has functions for linear and generalized linear models. They work pretty similarly, with some important exceptions. First, we’ll review regression in general.\nA linear model estimates a “predicted value of \\(y\\)” (that is, the mean of \\(y\\), conditional on \\(x\\)) assuming that the observed data are the conditional mean plus a (normally distributed) residual. We could write that a few ways, but let’s start with a familiar way from PS-813.\n\\(\\begin{align} y_{i} \u0026amp;= \\alpha + \\beta x_{i} + \\varepsilon_{i} \\\\[6pt] \\varepsilon_{i} \u0026amp;\\sim \\mathrm{Normal} \\left( 0, \\, \\sigma \\right) \\end{align}\\)\nEach predicted value of the dependent variable (\\(\\hat{y}_{i}\\)) is a regression on a set of \\(x\\) variables and coefficients \\(\\beta\\) and a constant \\(\\alpha\\), and residuals are normally distributed with mean of \\(0\\) and some estimated standard deviation \\(\\sigma\\).\nHere’s how we estimate regression equations in R, generically, using the lm() function.\n# this is an example with multiple x variables # to show that you use the `+` to specify the additive equation form model_results \u0026lt;- lm(y ~ x1 + x2 + x3, data = dataset_name) The syntax can read that y is a function of x1, x2, and so on. We also must specify the data set where these data come from with data =.\nAfter estimating, we usually look at detailed results using the summary() function.\nsummary(model_results) Here’s a real example, predicting relative thermometer ratings (\\(y\\)) using ideological self-placement (\\(x\\)), with data from the 2000 election only (note the use of filter() in the data = argument). It seems reasonable that individuals who are more conservative are more likely to feel warmer toward the Republican candidate than they are toward the Democratic candidate.\n# relative feeling thermometer as a function of ideology therm_mod \u0026lt;- lm(reltherm_cand ~ libcon_self, data = filter(anes, cycle == 2000)) # regression summary summary(therm_mod) ## ## Call: ## lm(formula = reltherm_cand ~ libcon_self, data = filter(anes, ## cycle == 2000)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -104.115 -22.115 0.922 20.922 75.922 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -56.1059 4.2143 -13.31 \u0026lt;2e-16 *** ## libcon_self 13.0368 0.9446 13.80 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 32.24 on 598 degrees of freedom ## (1207 observations deleted due to missingness) ## Multiple R-squared: 0.2416, Adjusted R-squared: 0.2403 ## F-statistic: 190.5 on 1 and 598 DF, p-value: \u0026lt; 2.2e-16 The summary function shows us the estimated coefficients (for the intercept and predictor), standard errors, test statistics, p-values, and significance levels. We also get some information about the F test and explained variance.\nRegression tricks Let’s do a multiple regression example. How about we just do this ideological scale as a series of dummy variables instead of as a continuous predictor. We can coerce any variable to series of dummy variables by inputting the variable as a factor in the regression equation.\n# \u0026quot;dummy mod\u0026quot;: ideology as a set of dummies using as.factor() dummy_mod \u0026lt;- lm(reltherm_cand ~ as.factor(libcon_self), data = filter(anes, cycle == 2000)) # each effect is an offset relative to the constant summary(dummy_mod) ## ## Call: ## lm(formula = reltherm_cand ~ as.factor(libcon_self), data = filter(anes, ## cycle == 2000)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.992 -22.320 0.462 20.462 77.462 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -33.583 9.292 -3.614 0.000327 *** ## as.factor(libcon_self)2 6.888 10.067 0.684 0.494140 ## as.factor(libcon_self)3 13.122 9.981 1.315 0.189122 ## as.factor(libcon_self)4 26.888 9.566 2.811 0.005107 ** ## as.factor(libcon_self)5 45.276 9.813 4.614 4.85e-06 *** ## as.factor(libcon_self)6 58.575 9.745 6.011 3.23e-09 *** ## as.factor(libcon_self)7 60.289 12.136 4.968 8.87e-07 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 32.19 on 593 degrees of freedom ## (1207 observations deleted due to missingness) ## Multiple R-squared: 0.2504, Adjusted R-squared: 0.2428 ## F-statistic: 33.01 on 6 and 593 DF, p-value: \u0026lt; 2.2e-16 The as.factor() coerced the libcon_self variable, which is originally numeric (integers), to be treated as a factor variable. Whenever lm() encounters a factor as an independent variable, R interprets the factor as a set of dummy variables automatically, treating the “first” level as the omitted category. It’s important to remember that when we have dummy variables, each coefficient should be interpret as an offset relative to the intercept. Ordinarily you want to take great care to specify which category should be omitted, because significance testing for the dummy variables will be relative to that baseline category.\nIf we want, we could rewrite this model by suppressing the intercept entirely with -1. In that case, we would have no omitted category, so each estimated coefficient would essentially represent the mean for each group.\n# all-intercepts model (no omitted category =\u0026gt; no constant) int_mod \u0026lt;- lm(reltherm_cand ~ -1 + as.factor(libcon_self), data = filter(anes, cycle == 2000)) # each estimate is a group mean summary(int_mod) ## ## Call: ## lm(formula = reltherm_cand ~ -1 + as.factor(libcon_self), data = filter(anes, ## cycle == 2000)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -106.992 -22.320 0.462 20.462 77.462 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## as.factor(libcon_self)1 -33.583 9.292 -3.614 0.000327 *** ## as.factor(libcon_self)2 -26.696 3.875 -6.889 1.43e-11 *** ## as.factor(libcon_self)3 -20.462 3.645 -5.614 3.03e-08 *** ## as.factor(libcon_self)4 -6.695 2.276 -2.942 0.003393 ** ## as.factor(libcon_self)5 11.692 3.156 3.705 0.000232 *** ## as.factor(libcon_self)6 24.992 2.938 8.505 \u0026lt; 2e-16 *** ## as.factor(libcon_self)7 26.706 7.807 3.421 0.000667 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 32.19 on 593 degrees of freedom ## (1207 observations deleted due to missingness) ## Multiple R-squared: 0.2508, Adjusted R-squared: 0.2419 ## F-statistic: 28.35 on 7 and 593 DF, p-value: \u0026lt; 2.2e-16 This model makes a great deal of sense just looking at it. Each intercept is the estimated mean response for each level of party ID. When respondents are more liberal, they like the Democratic candidate more than the Republican candidate, and vice versa for more conservative respondents. Watch out for significance testing though: right now, p-values are only testing against the null hypothesis that each coefficient is different from zero. You’d have to do some extra work to determine whether one intercept is statistically different from another.\nWe could, if we want, estimate an intercept-only model (no predictors) of the following form:\n\\(\\begin{align} y_{i} \u0026amp;= \\alpha + \\varepsilon_{i} \\end{align}\\)\nby writing the formula as y ~ 1. This is also how we used the group.CI() to estimate means without any groups.\n# intercept-only model alpha_mod \u0026lt;- lm(reltherm_cand ~ 1, data = filter(anes, cycle == 2000)) # this makes sense in the math, if you think about it # the right hand data is a \u0026quot;1\u0026quot; # y = alpha*1 is equivalent to y = alpha # (if you\u0026#39;ve seen regression in matrix before, # this is why the first column in the X matrix is a set of 1s) summary(alpha_mod) ## ## Call: ## lm(formula = reltherm_cand ~ 1, data = filter(anes, cycle == ## 2000)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -94.366 -28.366 1.634 26.634 97.634 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.6339 0.9131 -1.789 0.0738 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 36.47 on 1594 degrees of freedom ## (212 observations deleted due to missingness)   Model output: tables Like Stata, R can produce regression tables for you to insert into your writing. You should use these functions.\nIn fact, my advice is that you should never, never, NEVER write up a regression table by hand. Not in \\(\\mathrm{\\LaTeX}\\), not in Word, not ever.\n You are likely to make some kind of transcription error by hand-typing numbers into the table If your analysis ever changes, even slightly, you need to modify your table or create a brand-new one Spend your time researching and doing stats, not punching tables into the computer  R provides several packages for formatting the output of an analysis into tables. This output could be formatted as \\(\\mathrm{\\LaTeX}\\) code, as HTML code, as plain text, and so on. We’ll play with some table packages using the models we just estimated.\nThe stargazer package is solid and highly customizable. Its tables look good, but I find that it requires a lot of customization to make the tables look great.\nlibrary(\u0026quot;stargazer\u0026quot;) # latex by default stargazer(therm_mod, dummy_mod, int_mod) # as plain text stargazer(therm_mod, dummy_mod, int_mod, type = \u0026quot;text\u0026quot;) I think I like texreg a little better. I’m fairly sure it supports more model types than stargazer, and its defaults are a bit more sensible than stargazer’s are, at least for my uses (your uses may be different).\nlibrary(\u0026quot;texreg\u0026quot;) texreg(list(therm_mod, dummy_mod, int_mod)) I love xtable for non-regression tables such as marginals and summary statistics. The tables are clean, slick, and you can get them to do what you want. (See the tables at the end of this document, for example.) However, xtable is weirder for regression, and it’s strange in general because it often requires you to place more arguments into the print() function, which is counter-intuitive. But these tables look great when you can make it work.\nlibrary(\u0026quot;xtable\u0026quot;) print(xtable(therm_mod)) These aren’t the only tools for making tables in R. Check here for more info.\nMy general practice is to rely only on code to make tables. This ensures that you don’t make any transcription errors. It also ensures that when you export these tables to an external file (next section), any update to the model in R is automatically updated in your paper (if using \\(\\mathrm{\\LaTeX}\\)).\nHere’s how I advise you learn about tables:\n Experiment with the packages to figure out which package you like best Figure out which modifications to the default tables you tend to use the most Save these modifications somewhere (in a file, or as a keyboard shortcut), to save yourself some time whenever you write regression tables into a paper   Exporting regression tables This is not that difficult, but it requires some thinking about your project directory.\nWhenever you have some project, your directory should contain separate folders for R/, data/, and your writing (which I call tex/ or paper/). Let’s assume that we’re writing a \\(\\mathrm{\\LaTeX}\\) document inside a folder called tex/, and we want to save a regression table from R. We should also assume that our directory in R is set to the project root, i.e. the top of the project folder (for example, \u0026quot;users/michaeldecrescenzo/box sync/research/thesis\u0026quot;).\nWe can use R to create a subfolder inside of the tex/ folder dedicated to tables, if we don’t already have one.\n# makes a \u0026quot;tex\u0026quot; folder dir.create(\u0026quot;tex\u0026quot;) # make a tables/ folder inside of tex/ # requires a tex/ folder to exist already dir.create(\u0026quot;tex/tables\u0026quot;) We can then save model output as a .tex file in this folder location. Table-creating functions allow to you specify where you want the table to save. Specify the name of the .tex file you want to save. (You can also save tables as plain text and HTML files, depending on the package, but you get the most direct utility by learning to use \\(\\mathrm{\\LaTeX}\\) and saving tables as \\(\\mathrm{\\TeX}\\) files).\n# save texreg table (with caption) as a .tex file, in specified location texreg(list(therm_mod, dummy_mod, int_mod), caption = \u0026quot;Estimated regression results, various specifications\u0026quot;, file = \u0026quot;tex/tables/reg-table.tex\u0026quot;) And then in your research-paper.tex file, you can include a code to insert code from another .tex file somewhere else.\n... blah blah blah. See the regression table for results. \\input{tables/reg-table} The regression table shows that...  Model output: graphics New tools in R make it very easy to produce graphical model output.\nWe’ll talk about the broom package, which we’ve already introduced somewhat. We can turn model output into a tidy data frame using tidy().\n# Tidy the all-intercepts model # separate columns for variable names, coefficients, std err, p-val... tidy(int_mod) ## # A tibble: 7 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 as.factor(libcon_self)1 -33.6 9.29 -3.61 3.27e- 4 ## 2 as.factor(libcon_self)2 -26.7 3.87 -6.89 1.43e-11 ## 3 as.factor(libcon_self)3 -20.5 3.64 -5.61 3.03e- 8 ## 4 as.factor(libcon_self)4 -6.69 2.28 -2.94 3.39e- 3 ## 5 as.factor(libcon_self)5 11.7 3.16 3.70 2.32e- 4 ## 6 as.factor(libcon_self)6 25.0 2.94 8.51 1.48e-16 ## 7 as.factor(libcon_self)7 26.7 7.81 3.42 6.67e- 4 Combine tidy model data frames using bind_rows(), which binds data frame rows together. We do this while also adding a variable for the model name (using mutate()).\nmods \u0026lt;- bind_rows(mutate(tidy(therm_mod, conf.int = TRUE), model = \u0026quot;Continuous Predictor\u0026quot;), mutate(tidy(dummy_mod, conf.int = TRUE), model = \u0026quot;Constant and Dummies\u0026quot;), mutate(tidy(int_mod, conf.int = TRUE), model = \u0026quot;All Intercepts\u0026quot;)) %\u0026gt;% as_data_frame() %\u0026gt;% print()  ## # A tibble: 16 x 8 ## term estimate std.error statistic p.value conf.low conf.high model ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 (Interc… -56.1 4.21 -13.3 1.33e-35 -64.4 -47.8 Conti… ## 2 libcon_… 13.0 0.945 13.8 8.18e-38 11.2 14.9 Conti… ## 3 (Interc… -33.6 9.29 -3.61 3.27e- 4 -51.8 -15.3 Const… ## 4 as.fact… 6.89 10.1 0.684 4.94e- 1 -12.9 26.7 Const… ## 5 as.fact… 13.1 9.98 1.31 1.89e- 1 -6.48 32.7 Const… ## 6 as.fact… 26.9 9.57 2.81 5.11e- 3 8.10 45.7 Const… ## 7 as.fact… 45.3 9.81 4.61 4.85e- 6 26.0 64.5 Const… ## 8 as.fact… 58.6 9.75 6.01 3.23e- 9 39.4 77.7 Const… ## 9 as.fact… 60.3 12.1 4.97 8.87e- 7 36.5 84.1 Const… ## 10 as.fact… -33.6 9.29 -3.61 3.27e- 4 -51.8 -15.3 All I… ## 11 as.fact… -26.7 3.87 -6.89 1.43e-11 -34.3 -19.1 All I… ## 12 as.fact… -20.5 3.64 -5.61 3.03e- 8 -27.6 -13.3 All I… ## 13 as.fact… -6.69 2.28 -2.94 3.39e- 3 -11.2 -2.23 All I… ## 14 as.fact… 11.7 3.16 3.70 2.32e- 4 5.49 17.9 All I… ## 15 as.fact… 25.0 2.94 8.51 1.48e-16 19.2 30.8 All I… ## 16 as.fact… 26.7 7.81 3.42 6.67e- 4 11.4 42.0 All I… We can then plot coefficients straight away. Use the variable name as the x and the coefficient as the y\n# we flip the x and y coordinates # to imitate the typical look of a coefficient plot # Notice how not every model has the same variable names... ggplot(mods, aes(x = term, y = estimate)) + geom_hline(yintercept = 0, color = \u0026quot;gray50\u0026quot;) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = model), position = position_dodge(width = -1)) + scale_color_brewer(palette = \u0026quot;Set2\u0026quot;) + coord_flip() + labs(x = NULL, y = \u0026quot;Estimated Coefficient\u0026quot;, color = \u0026quot;Specification\u0026quot;) ## Warning: position_dodge requires non-overlapping x intervals So easy! You could then save this plot…\n# creates a graphics folder, if one doesn\u0026#39;t already exist dir.create(\u0026quot;tex/graphics\u0026quot;) # save the plot in the graphics folder, setting height and width # saves as PDF ggsave(\u0026quot;tex/graphics/coefplot.pdf\u0026quot;, height = 5, width = 5) Some other tools create these sorts of plots for you. Some folks enjoy using sjPlot, but I have never bothered to use it. (I ordinarily don’t like packages to make plots for me. I prefer to create data and decide for myself how they should be plotted. This is my personal preference, though—you do you.)\nRelated: saving other quantities (from R to \\(\\mathrm{\\LaTeX}\\)) Just like with tables, you can save many other quantities to tex files. This way, the quantities in your paper reflect quantities in the analysis perfectly.\nFor example, let’s calculate the mean GOP candidate thermometer in (say) 2012, and save it.\n# create a subdirectory for referenced values from R # I call this folder location \u0026quot;refs\u0026quot; but you can call it whatever you want dir.create(\u0026quot;tex/refs\u0026quot;) # in the 2012 cycle, # what is the mean thermometer rating of GOP candidate? # round it to the nearest whole number # print the value to see what it looks like # save the value in a .tex file for later use anes %\u0026gt;% filter(cycle == 2012) %$% # \u0026lt;--- note the pipe! mean(therm_gopcand, na.rm = TRUE) %\u0026gt;% round() %\u0026gt;% print() %\u0026gt;% write(\u0026quot;tex/refs/mean-gop-therm-2012.tex\u0026quot;)  Then, in your .tex file, import this quantity directly from file…\n...the mean rating for Mitt Romney was $\\input{refs/mean-gop-therm}$ …which would automatically grab the contents of that saved tex file and place it into your paper when you compile the tex document! This practice cuts down human error, saves time (you no longer have to update everything in your .tex file by hand every time you slightly change an analysis), and enhances the reproducibility of your work. I highly recommend it!\n  Predicted values You can generate predicted values from the data used to model using predict().\n# try it out predict(therm_mod) We can also use predict() to generate predictions for new datasets. This is good for visualizing the effect of one variable, holding others constant. You can do this by creating a “counterfactual dataset” and generating model predicts for the observations in the counterfactual data.\n# we only need one variable # be sure it has the same name as the regression variable id_frame \u0026lt;- data_frame(libcon_self = 1:7) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. # use the estimated model to predict for the new data predict(therm_mod, newdata = id_frame) ## 1 2 3 4 5 6 ## -43.069118 -30.032334 -16.995550 -3.958767 9.078017 22.114801 ## 7 ## 35.151584 We can also add intervals of various kinds (confidence intervals for means, prediction intervals, and so on). Let’s generate predictions for the linear model and the “all intercepts” model, and then plot them side by side for comparison.\n# predictions from linear model, using the new id_frame data lin_preds \u0026lt;- predict(therm_mod, newdata = id_frame, interval = \u0026quot;confidence\u0026quot;) %\u0026gt;% as_data_frame() %\u0026gt;% print()  ## # A tibble: 7 x 3 ## fit lwr upr ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -43.1 -49.6 -36.5 ## 2 -30.0 -34.9 -25.1 ## 3 -17.0 -20.5 -13.5 ## 4 -3.96 -6.58 -1.34 ## 5 9.08 6.13 12.0 ## 6 22.1 17.9 26.3 ## 7 35.2 29.4 40.9 # predictions from intercepts model # predict() is smart enough to factorize your predictor variable # if you factorized it in the model int_preds \u0026lt;- predict(int_mod, newdata = id_frame, interval = \u0026quot;confidence\u0026quot;) %\u0026gt;% as_data_frame() %\u0026gt;% print() ## # A tibble: 7 x 3 ## fit lwr upr ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -33.6 -51.8 -15.3 ## 2 -26.7 -34.3 -19.1 ## 3 -20.5 -27.6 -13.3 ## 4 -6.69 -11.2 -2.23 ## 5 11.7 5.49 17.9 ## 6 25.0 19.2 30.8 ## 7 26.7 11.4 42.0 # combine! compare_mods \u0026lt;- bind_rows(mutate(lin_preds, libcon_self = id_frame$libcon_self, mod = \u0026quot;linear\u0026quot;), mutate(int_preds, libcon_self = id_frame$libcon_self, mod = \u0026quot;intercepts\u0026quot;)) %\u0026gt;% print() ## # A tibble: 14 x 5 ## fit lwr upr libcon_self mod ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 -43.1 -49.6 -36.5 1 linear ## 2 -30.0 -34.9 -25.1 2 linear ## 3 -17.0 -20.5 -13.5 3 linear ## 4 -3.96 -6.58 -1.34 4 linear ## 5 9.08 6.13 12.0 5 linear ## 6 22.1 17.9 26.3 6 linear ## 7 35.2 29.4 40.9 7 linear ## 8 -33.6 -51.8 -15.3 1 intercepts ## 9 -26.7 -34.3 -19.1 2 intercepts ## 10 -20.5 -27.6 -13.3 3 intercepts ## 11 -6.69 -11.2 -2.23 4 intercepts ## 12 11.7 5.49 17.9 5 intercepts ## 13 25.0 19.2 30.8 6 intercepts ## 14 26.7 11.4 42.0 7 intercepts When you have these predictions as a data frame, you could plot!\n# plot points with dodging # suppress minor grid ggplot(data = compare_mods, aes(x = libcon_self, y = fit)) + geom_hline(yintercept = 0) + geom_pointrange(aes(ymin = lwr, ymax = upr, shape = mod), position = position_dodge(width = 0.25), fill = \u0026quot;white\u0026quot;) + scale_x_continuous(breaks = 1:7) + scale_shape_manual(values = c(16, 21)) + coord_cartesian(ylim = c(-50, 50)) + labs(x = \u0026quot;Ideological Self-Placement\u0026quot;, y = \u0026quot;Relative Candidate Thermometer\\n(Republican minus Democrat)\u0026quot;, shape = \u0026quot;Model\u0026quot;) + theme(panel.grid.minor = element_blank()) I’ll leave it to you to interpret these models and assess the pros and cons of each.\nPackages for model predictions For more complex post-estimation graphics (e.g. first differences, marginal effects), there are some packages that make your job a little easier. I won’t cover them all (because not everyone engages in these types of predictions), but I’ll lay some out:\n The broom package (where tidy() comes from) provides the augment function, for augmenting a data frame with predictions from an accompanying model. This works a bit like predict(), but it returns tidy data frames. augment gives you a “standard error of the fit” variable rather than separate variables for upper and lower boundaries, but you can use the standard error to calculate however wide a confidence interval you desire. The margins package by Thomas Leeper is meant to imitate the workflow of Stata’s margins and marginsplot workflow. sjPlot Zelig, a Gary King project, tries to standardize model evaluation and visualization across model types effects, which is a little older but still commonly advocated  I prefer using broom because it is meant to work in a “tidy” workflow, but you have to do the work to generate your own critical value for generating a confidence interval. I don’t find this to be an onerous task, since I believe you should be actively thinking about your critical values anyway. I don’t use this kind of stuff much, however, because I tend to do uncertainty by simulation or with a Bayesian model. Needless to say, however, there are plenty of documents and examples on the web for these packages.\n  Generalized linear models Generalized linear models (GLMs, such as logit, poisson regression, negative binomial regression, and so on) are similar to linear models, but they are designed for nonlinear relationships. GLMs and LMs are similar, but the following key differences distinguish the two.\nDistribution of the dependent variable In a linear model, residuals are assumed to be normally distributed. Another way to write this is that \\(y_{i}\\) itself is normally distributed around the expected value of \\(y\\), conditional on \\(x\\). Call this conditional mean \\(\\mu_{i}\\).\n\\(\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Normal}\\left( \\mu_{i}, \\sigma \\right) \\\\[6pt] \\mu_{i} \u0026amp;= \\alpha + \\beta x_{i} \\end{align}\\)\nThis is how you write a linear model as a normally distributed \\(y\\). For generalized linear models, it’s the same basic idea, except the outcome distribution is non-normal (we’ll use \\(\\tau\\) to indicate a dispersion parameter).\n\\(\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Some \\, Distribution}\\left( \\mu_{i}, \\tau \\right) \\end{align}\\)\n Linearity The other component that sets GLMS apart is the linearity of the relationship between \\(x\\) and \\(y.\\)\n In a linear model: \\(x\\) is assumed to have a linear effect on \\(\\mu_{i}\\). Coefficients describe how \\(x\\) impacts \\(\\mu_{i}\\). In a GLM: \\(x\\) has a linear impact on a transformation of \\(\\mu_{i}\\), which implies that \\(x\\) is non-linearly related to \\(y\\). Coefficients describe how \\(x\\) impacts the transformed \\(\\mu_{i}\\).  So the full GLM is like the Normal model, but we use a different distribution, and \\(\\mu_{i}\\) isn’t linearly related to \\(x\\).\n\\(\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Some \\, Distribution}\\left( \\mu_{i}, \\tau \\right) \\\\[6pt] \\mathcal{f}\\left( \\mu_{i} \\right) \u0026amp;= \\alpha + \\beta x_{i} \\\\[6pt] \\mu_{i} \u0026amp;= f^{-1}\\left( \\alpha + \\beta x_{i} \\right) \\end{align}\\)\nThe transformation of \\(\\mu_{i}\\) is called a link function. We call these “generalized linear models” because there is a linear component, but it’s a linear relationship between \\(x\\) and \\(\\mathrm{link}(\\mu_{i})\\). And it’s generalized because a linear model fits into this framework as well, but the link function is simply \\(1 \\times \\mu_{i}\\).\n An example using logit We’ll use logistic regression to predict a vote for the Republican presidential candidate using ideological self-placement and gender as covariates.\nFitting this into the GLM framework…\n The outcome distribution is Bernoulli. The dependent variable \\(y\\) is a 1 or a 0, “success” or “failure”. We don’t model \\(y\\) directly. Instead, we want to model \\(\\pi_{i}\\), which is the expected value of \\(y_{i}\\). The link function connecting \\(\\pi_{i}\\) to the regression equation is called the logit function, a.k.a. the “log odds” of \\(\\y_{i}\\).  Here’s the math. We’ll use \\(y_{i}\\) to indicate the observed vote for voter \\(i\\), \\(\\pi_{i}\\) is the probability that \\(i\\) votes for the Republican.\n\\(\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Bernoulli}(\\pi_{i}) \\\\[6pt] \\ln \\left( \\frac{\\pi_{i}}{1 - \\pi_{i}} \\right) \u0026amp;= \\alpha + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\ldots \\end{align}\\)\nAs we can see, neither our data \\(y_{i}\\) nor the expected value \\(\\pi_{i}\\) are linearly related to our predictors. Instead, the transformation of \\(\\pi_{i}\\) (the log odds) is linearly related to the predictors.\n Let’s do this in R. Here are the data from 1996 only.\n# vote, ideology, and gender data from 1996 logit_data \u0026lt;- anes %\u0026gt;% filter(cycle == 1996) %\u0026gt;% select(vote, libcon_self, gender) %\u0026gt;% print() ## # A tibble: 1,714 x 3 ## vote libcon_self gender ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;NA\u0026gt; 4 Women ## 2 Democratic Candidate 4 Women ## 3 \u0026lt;NA\u0026gt; NA Men ## 4 Democratic Candidate 4 Men ## 5 Republican Candidate 4 Women ## 6 \u0026lt;NA\u0026gt; NA Men ## 7 Republican Candidate 7 Men ## 8 Democratic Candidate NA Men ## 9 Democratic Candidate 4 Women ## 10 \u0026lt;NA\u0026gt; 7 Men ## # … with 1,704 more rows Let’s transform this data to make it play nicely with modeling math.\n# convert rvote to a dummy (treat a logical as a number, 0 or 1) # same with gender # center the ideology scale on 4, so the constant (x = 0) represents moderates logit_data \u0026lt;- logit_data %\u0026gt;% mutate(rvote = as.numeric(vote == \u0026quot;Republican Candidate\u0026quot;), woman = as.numeric(gender == \u0026quot;Women\u0026quot;), ideo = libcon_self - 4) %\u0026gt;% print() ## # A tibble: 1,714 x 6 ## vote libcon_self gender rvote woman ideo ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 \u0026lt;NA\u0026gt; 4 Women NA 1 0 ## 2 Democratic Candidate 4 Women 0 1 0 ## 3 \u0026lt;NA\u0026gt; NA Men NA 0 NA ## 4 Democratic Candidate 4 Men 0 0 0 ## 5 Republican Candidate 4 Women 1 1 0 ## 6 \u0026lt;NA\u0026gt; NA Men NA 0 NA ## 7 Republican Candidate 7 Men 1 0 3 ## 8 Democratic Candidate NA Men 0 0 NA ## 9 Democratic Candidate 4 Women 0 1 0 ## 10 \u0026lt;NA\u0026gt; 7 Men NA 0 3 ## # … with 1,704 more rows You’ll see NAs in the data. Cases with missing values are automatically dropped during estimation. You may cover missing data imputation in your maximum likelihood course.\nThe estimation formula in R looks like lm(), but we specify a family of probability distributions. We use “binomial,” which is how you do logit. (Binomial is a Bernoulli for multiple observations).\n# estimate the model with glm() and binomial distribution vote_logit \u0026lt;- glm(rvote ~ ideo + woman + ideo*woman, family = binomial(link = \u0026quot;logit\u0026quot;), data = logit_data) summary(vote_logit) ## ## Call: ## glm(formula = rvote ~ ideo + woman + ideo * woman, family = binomial(link = \u0026quot;logit\u0026quot;), ## data = logit_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3949 -0.8651 -0.2842 0.6189 2.5415 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.78991 0.15241 -5.183 2.18e-07 *** ## ideo 1.19970 0.12051 9.955 \u0026lt; 2e-16 *** ## woman 0.01939 0.20222 0.096 0.924 ## ideo:woman -0.03668 0.16285 -0.225 0.822 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1185.24 on 857 degrees of freedom ## Residual deviance: 833.29 on 854 degrees of freedom ## (856 observations deleted due to missingness) ## AIC: 841.29 ## ## Number of Fisher Scoring iterations: 5 We can create coefficient plots and tables as before, so I won’t demonstrate those. I will say that when you want to compare GLMs to one another, it is usually smarter to compare predictions or model fit statistics than it is to compare coefficients themselves. Because coefficients are on unintuitive scales (the “link scale”) and sometimes involve ancillary parameters that help adjust the fit (such as cutoff parameters in ordinal logit), small changes to the model may change coefficients in ways that look large, but the effects on the actual predicted value may be negligible.\nVisualizing the predictions from a GLM is similar to linear modeling, but we add one step.\n Create counterfactual data for simulated predictions Generate linear predictions using model coefficients Transform linear predictions with inverse link function Plot as desired  Here we only use coefficients to generate the linear predictions from the model. The predictions and bounds are on the link scale (log odds ratios).\n# new data frame of values -3 through 3, # which is the rescaled ideology scale # for GLMs, the critical value is always 1.96 # (assumes normal coefficients on the link scale) # here are predictions on the link scale (for logit: the log-odds scale) # using the augment() function from the broom package logit_preds \u0026lt;- data_frame(ideo = rep(-3:3, 2), woman = c(rep(1, 7), rep(0, 7)), `ideo:woman` = ideo * woman) %\u0026gt;% augment(vote_logit, newdata = .) %\u0026gt;% mutate(lower = .fitted - (1.96 * .se.fit), upper = .fitted + (1.96 * .se.fit)) %\u0026gt;% print() ## # A tibble: 14 x 7 ## ideo woman ideo.woman .fitted .se.fit lower upper ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -3 1 -3 -4.26 0.402 -5.05 -3.47 ## 2 -2 1 -2 -3.10 0.300 -3.68 -2.51 ## 3 -1 1 -1 -1.93 0.204 -2.33 -1.53 ## 4 0 1 0 -0.771 0.133 -1.03 -0.510 ## 5 1 1 1 0.392 0.133 0.133 0.652 ## 6 2 1 2 1.56 0.204 1.16 1.95 ## 7 3 1 3 2.72 0.299 2.13 3.30 ## 8 -3 0 0 -4.39 0.470 -5.31 -3.47 ## 9 -2 0 0 -3.19 0.355 -3.88 -2.49 ## 10 -1 0 0 -1.99 0.245 -2.47 -1.51 ## 11 0 0 0 -0.790 0.152 -1.09 -0.491 ## 12 1 0 0 0.410 0.124 0.166 0.654 ## 13 2 0 0 1.61 0.192 1.23 1.99 ## 14 3 0 0 2.81 0.295 2.23 3.39 If we don’t transform the linear predictions, then we get predictions on the link scale (the log odds scale in a logit model). That’s how we can have negative predicted values, for example. We could plot them, and they’d look like straight lines (just like OLS), but log odd ratios are hard to interpret. Instead, we will transform the log odds to predicted probabilities using the plogis() function, which is the inverse of the logit link function (a.k.a. the “logistic function,” which happens to be the cumulative distribution function of the logistic distribution).\n# transform log odds to probabilities logit_preds \u0026lt;- logit_preds %\u0026gt;% select(-.se.fit) %\u0026gt;% # mutate only the selected variables mutate_at(vars(.fitted, upper, lower), plogis) %\u0026gt;% print() ## # A tibble: 14 x 6 ## ideo woman ideo.woman .fitted lower upper ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -3 1 -3 0.0139 0.00638 0.0302 ## 2 -2 1 -2 0.0432 0.0245 0.0752 ## 3 -1 1 -1 0.126 0.0883 0.178 ## 4 0 1 0 0.316 0.263 0.375 ## 5 1 1 1 0.597 0.533 0.658 ## 6 2 1 2 0.826 0.761 0.876 ## 7 3 1 3 0.938 0.894 0.965 ## 8 -3 0 0 0.0123 0.00492 0.0302 ## 9 -2 0 0 0.0396 0.0201 0.0763 ## 10 -1 0 0 0.120 0.0780 0.181 ## 11 0 0 0 0.312 0.252 0.380 ## 12 1 0 0 0.601 0.541 0.658 ## 13 2 0 0 0.833 0.774 0.879 ## 14 3 0 0 0.943 0.903 0.967 # modify data and plot ggplot(logit_preds, aes(x = ideo, y = .fitted)) + geom_pointrange(aes(ymin = lower, ymax = upper, color = as.factor(woman)), position = position_dodge(width = 0.5), show.legend = FALSE) + annotate(\u0026quot;text\u0026quot;, x = 0.5, y = .59, label = \u0026quot;Men\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 1.7, y = .59, label = \u0026quot;Women\u0026quot;) + scale_x_continuous(breaks = -3:3, labels = c(\u0026quot;Very\\nLiberal\u0026quot;, \u0026quot;Liberal\u0026quot;, \u0026quot;Slightly\\nLiberal\u0026quot;, \u0026quot;Moderate\u0026quot;, \u0026quot;Slightly\\nConservative\u0026quot;, \u0026quot;Conservative\u0026quot;, \u0026quot;Very\\nConservative\u0026quot;)) + labs(color = NULL, x = \u0026quot;Ideological Self-Placement\u0026quot;, y = \u0026quot;Probability of Republican Vote\u0026quot;) + theme(panel.grid.minor = element_blank()) Note how the predictions are non-linear. The predictions approach 0% and 100% but never exceed them. GLMs are useful because they accurately capture these sorts of ceiling and floor effects.\nThere are loads of GLMs out there, because there are loads of ways your data aren’t perfectly normal and linearly related. But they all fit the same basic framework: \\(y\\) follows some distribution, and you need some link function to describe how \\(y\\) is related to \\(x\\).\n  Model diagnostics The model objects created by lm() and glm() do include some model diagnosis tools, such as F-statistics, \\(R^{2}\\) values, deviance, AIC, so on. We’ll walk through some here.\nSome diagnostics for linear models can be easily visualized using plot(), including quantile plots, and analyses of residuals.\nplot(therm_mod) There are certain functions and packages that can be used to generate model fit statistics. I think the easiest tool for comparing models is broom::glance(). Just like broom::tidy(), you can stack the data frames created by glance() to compare models easily or plot the statistics.\nbind_rows(mutate(glance(therm_mod), mod = \u0026quot;LibCon\u0026quot;), mutate(glance(dummy_mod), mod = \u0026quot;Dummies\u0026quot;), mutate(glance(int_mod), mod = \u0026quot;Intercepts\u0026quot;)) ## # A tibble: 3 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.242 0.240 32.2 190. 8.18e-38 2 -2934. 5875. 5888. ## 2 0.250 0.243 32.2 33.0 2.22e-34 7 -2931. 5878. 5913. ## 3 0.251 0.242 32.2 28.4 9.97e-34 7 -2931. 5878. 5913. ## # … with 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, mod \u0026lt;chr\u0026gt; This works for GLMs as well, but maximum likelihood models have some different fit statistics than least-squares models. If you want to compare linear and nonlinear models, you could estimate the linear model as a GLM model of Gaussian family with an “identity” link.\nglance(vote_logit) ## # A tibble: 1 x 7 ## null.deviance df.null logLik AIC BIC deviance df.residual ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1185. 857 -417. 841. 860. 833. 854 Some additional tips and tools.\n Some of these diagnostics will show an improvement in model fit even if the improvement comes from fitting noise (such as \\(R^{2}\\)). These diagnostics are statistics that take a distribution, so you want to compare models using a statistical comparison—i.e. is the fit improvement enough given that you’ve added an extra variable to the model. Examples include F-tests and likelihood-ratio tests. If you go down this route, you might check out tools such as epicalc::lrtest() or the lmtest package. Other packages for model assistance (such as arm or rms) may have similar tools as well. Other diagnostic measures will penalize you for adding variables on the front-end, such as BIC, so they don’t require formal statistical tests. My advice is that if you want to be doing this kind of intense model comparison, make sure you know what these statistics are checking and that the use is appropriate for your task at hand. There really are no hard and fast rules here, so you want to do what makes sense for your use case. Out-of-sample prediction is a good test for model over-fitting. This can be evaluated using cross-validation. The loo package provides tools for easier CV performance. (Also, the AIC is intended to estimate out-of-sample model accuracy). Simulating artificial data can be a useful face-validity check. If you are estimating a generative model of your data (and you are…), the model should generate data that look like your data.   Intermediate R tricks Now we will quickly introduce some more nitty-gritty R tricks. These may not be essential for the problem set, but over the long run, you will be a much more efficient R user if you take these concepts seriously.\nType coercion As we covered early in the course, there are a few different data types in R: logical, numeric, factor, and character. Data can be coerced from one type to another with as.type() functions, where type refers to the resulting data type.\nLet’s start as broad as possible with characters. As the broadest of these data types, anything can be coerced to a character.\n# logical to character as.character(TRUE) ## [1] \u0026quot;TRUE\u0026quot; # numeric to character as.character(c(1, 2, 3)) ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; # factor to character F \u0026lt;- factor(c(\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;)) as.character(F) ## [1] \u0026quot;hello\u0026quot; \u0026quot;world\u0026quot; Converting to factor is similar. Each unique value is given its own factor level, and the order of levels is assigned alphabetically unless specified with factor(..., levels = c(...)).\nas.factor(c(TRUE, FALSE)) ## [1] TRUE FALSE ## Levels: FALSE TRUE as.factor(c(1, 2, 3)) ## [1] 1 2 3 ## Levels: 1 2 3 # note the level order factor(c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;), levels = c(\u0026quot;3\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;1\u0026quot;)) ## [1] 1 2 3 ## Levels: 3 2 1 Converting to numeric is slightly more confusing. Logical variables are easy and can be converted to 1a and 0s.\nas.numeric(c(TRUE, FALSE)) ## [1] 1 0 Factors also work, but the coercion gives numeric meaning to the underlying factor labels.\n# levels assigned in reverse order (F \u0026lt;- factor(c(\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;), levels = c(\u0026quot;world\u0026quot;, \u0026quot;hello\u0026quot;))) ## [1] hello world ## Levels: world hello # note the mapping to numeric... # Making a data frame to visualize data_frame(factor = F, numeric = as.numeric(F)) ## # A tibble: 2 x 2 ## factor numeric ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 hello 2 ## 2 world 1 Character vectors cannot be directly mapped to numeric. They need to be converted to factor first.\nchar \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;z\u0026quot;) # direct coercion leads to NAs as.numeric(char) ## Warning: NAs introduced by coercion ## [1] NA NA NA # coercion through factor works as.numeric(as.factor(char)) ## [1] 1 2 3 Here’s where you typically see these forms of coercion.\n using logicals to convert categorical variables into dummy variables using as.factor() to convert a numeric index into a set of dummy variables in a regression function converting numeric caseID variables to character in order to fix any problems with leading zeroes. This is common with geocodes like FIPS codes. Converting character vectors to factors for plotting (placing categories in order for legends or panel titles).   User-defined functions In R, we can write our own functions to perform repetitive tasks. Let’s demonstrate the a task for finding a mean.\nmy_mean \u0026lt;- function(x) { the_sum \u0026lt;- sum(x) n \u0026lt;- length(x) the_mean \u0026lt;- the_sum / n return(the_mean) } z \u0026lt;- 1:5 my_mean(z) ## [1] 3 User-defined functions have three components.\n The function name, which is what we assign the function to. Arguments, passed to the function, manipulated within the function The definition, which details how arguments are manipulated and what the function returns  It is important to note that the variables inside the function definition are called local variables. This means they only exist in the world of that function. They are not accessible elsewhere in R. In the above example, x, the_sum, n, and the_mean are manipulated by the function but are not available to you to play with. Furthermore, if there are other objects currently in R memory that share those same names, they have no bearing on how the function works. Local variables help define a function and perform its intended purpose, but they do not affect and are not affected by the other objects in your current R workspace.\n Lists There is one data structure that we have not yet discussed: lists. Lists are like vectors, but unlike vectors, their elements can be of any data type. Let’s demonstrate.\n# create a list of named elements el \u0026lt;- list(num = 1, fact = factor(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;)), char = c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;z\u0026quot;)) # check it out el ## $num ## [1] 1 ## ## $fact ## [1] a b c ## Levels: a b c ## ## $char ## [1] \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; # numeric indexing gives you the named element (including the name) el[1] ## $num ## [1] 1 # to get all the way down to the data... el$num ## [1] 1 el[[2]] ## [1] a b c ## Levels: a b c el[[\u0026quot;char\u0026quot;]] ## [1] \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; This can be handy for stacks of data frames. For example, the anes dataset, but element is a data frame corresponding to each survey year.\n# This will print a big monstrosity, # but you should see what it looks like anes_list \u0026lt;- split(anes, anes$cycle)  Functional programming with apply() functions. You’ll see stuff about apply() functions online. They are scary at first, but they make sense if you give them a chance.\nLet’s see what we mean. Let’s create a two-D object.\ndf \u0026lt;- data_frame(a = 1:5, b = a, c = a) %\u0026gt;% print() ## # A tibble: 5 x 3 ## a b c ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 1 1 ## 2 2 2 2 ## 3 3 3 3 ## 4 4 4 4 ## 5 5 5 5 An apply() function takes an object, and applies a function across its dimension(s). This is easier to explain using an example: here, we will apply the mean function to the rows and columns of this df object.\n# 1 = row # get the mean of every row apply(df, 1, mean) ## [1] 1 2 3 4 5 It returns an object the same length as the number of rows in the object, containing the result of the mean() function for each row.\nHere it is for each column.\n# 2 = columns # get the mean of every column apply(df, 2, mean) ## a b c ## 3 3 3 You could pass a user-defined function to apply(), or you could define a function within apply() using “anonymous functions.” Example:\ndf  ## # A tibble: 5 x 3 ## a b c ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 1 1 ## 2 2 2 2 ## 3 3 3 3 ## 4 4 4 4 ## 5 5 5 5 # apply x * x to every column apply(df, 2, function(x) {x * x}) ## a b c ## [1,] 1 1 1 ## [2,] 4 4 4 ## [3,] 9 9 9 ## [4,] 16 16 16 ## [5,] 25 25 25 This anonymous function applies the function x^2 to each column in df.\nThere are a few types of apply functions, but you’re only likely to use a few of them.\n apply: apply a function over the margins of an object sapply: simplify the apply() results to a one-D vector, if possible lapply: apply for each element in a list  Here is an lapply example, using the anes_list object we created above. We’ll use an anonymous function to find the mean party_distance in each election cycle.\n# \u0026#39;x\u0026#39; refers to each element of the list # each element being a data frame # so x$var finds \u0026#39;var\u0026#39; in the x data frame lapply(anes_list, function(x) mean(x$party_distance, na.rm = TRUE)) From there, you can do cool things like “melt” the list into a data frame using reshape2::melt().\nlapply(anes_list, function(x) mean(x$party_distance, na.rm = TRUE)) %\u0026gt;% reshape2::melt()  Nesting: superpowered lists When you get really comfortable with function programming, you can do crazy stuff like nest a data frame. What is that?\nanes %\u0026gt;% group_by(cycle) %\u0026gt;% nest()  ## # A tibble: 30 x 2 ## cycle data ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 1948 \u0026lt;tibble [662 × 961]\u0026gt; ## 2 1952 \u0026lt;tibble [1,899 × 961]\u0026gt; ## 3 1954 \u0026lt;tibble [1,139 × 961]\u0026gt; ## 4 1956 \u0026lt;tibble [1,762 × 961]\u0026gt; ## 5 1958 \u0026lt;tibble [1,450 × 961]\u0026gt; ## 6 1960 \u0026lt;tibble [1,181 × 961]\u0026gt; ## 7 1962 \u0026lt;tibble [1,297 × 961]\u0026gt; ## 8 1964 \u0026lt;tibble [1,571 × 961]\u0026gt; ## 9 1966 \u0026lt;tibble [1,291 × 961]\u0026gt; ## 10 1968 \u0026lt;tibble [1,557 × 961]\u0026gt; ## # … with 20 more rows A nested data frame is a data frame where columns can themselves be a list of data frames (a.k.a. a “list column”). In this data frame, the data column isn’t really a variable; it contains a list of data frames, each corresponding to the grouping variable (cycle).\nUnnest a list column from a data frame like so:\nanes %\u0026gt;% group_by(cycle) %\u0026gt;% nest() %\u0026gt;% # unnest the `data` column unnest(data) Why is this useful? Well…\n Mapping a function over a list column This is another functional programming trick, like apply, but applied to a list column in a nested data frame.\nLet’s say we had the above nested frame (a data frame for each survey wave), but we wanted to estimate a regression for separate data frames.\nHere, we estimate the effect of gender on Republican voting using purrr::map(), which is like apply() but it works across a list column in nested data frame.\n# nesting the data # get indicator for R vote # removing NAs # group by cycle and nest # Apply function over the list column # use every data frame in the list column # run glm() using an intercepts model for comparisons (no constant) # tidy the model output # Unnest the data # unnest the results of map() # create a gender label gender_gaps \u0026lt;- anes %\u0026gt;% mutate(rvote = as.numeric(vote == \u0026quot;Republican Candidate\u0026quot;)) %\u0026gt;% filter(!is.na(rvote) \u0026amp; !is.na(gender)) %\u0026gt;% group_by(cycle) %\u0026gt;% nest() %\u0026gt;% mutate(model = map(data, ~ glm(rvote ~ -1 + as.factor(gender), data = ., family = \u0026quot;binomial\u0026quot;) %\u0026gt;% tidy(conf.int = TRUE))) %\u0026gt;% unnest(model) %\u0026gt;% mutate(term = case_when(str_detect(term, \u0026quot;Men\u0026quot;) ~ \u0026quot;Man\u0026quot;, TRUE ~ \u0026quot;Woman\u0026quot;)) %\u0026gt;% print() ## # A tibble: 34 x 8 ## cycle term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1948 Man -0.228 0.145 -1.58 1.15e- 1 -0.513 0.0544 ## 2 1948 Woman -0.113 0.143 -0.787 4.31e- 1 -0.395 0.168 ## 3 1952 Man 0.286 0.0840 3.40 6.76e- 4 0.121 0.451 ## 4 1952 Woman 0.372 0.0829 4.49 6.98e- 6 0.211 0.536 ## 5 1956 Man 0.259 0.0815 3.18 1.46e- 3 0.1000 0.419 ## 6 1956 Woman 0.517 0.0809 6.39 1.71e-10 0.359 0.676 ## 7 1960 Man -0.107 0.0967 -1.11 2.67e- 1 -0.297 0.0820 ## 8 1960 Woman 0.124 0.0925 1.34 1.81e- 1 -0.0573 0.306 ## 9 1964 Man -0.640 0.0934 -6.85 7.14e-12 -0.825 -0.459 ## 10 1964 Woman -0.810 0.0881 -9.19 4.04e-20 -0.985 -0.639 ## # … with 24 more rows # # plot coefficients over time # # map pt shape, solid and empty points, generic white fill ggplot(gender_gaps, aes(x = cycle, y = estimate, color = term)) + geom_hline(yintercept = 0) + geom_pointrange(aes(ymin = conf.low, ymax = conf.high, shape = term), fill = \u0026quot;white\u0026quot;, position = position_dodge(width = 0.5)) + scale_shape_manual(values = c(16, 21)) + scale_x_continuous(breaks = seq(1948, 2012, 8)) + labs(y = \u0026quot;Effect on Republican Voting (Log Odds Scale)\u0026quot;, x = \u0026quot;Election Cycle\u0026quot;, color = NULL, shape = NULL) Mapping is a tool that takes some getting used to at first. In particular, you have to get a feel for the formula syntax where a function follows a ~ symbol, and you use . to represent element names in the data column. Once you get this down, however, mapping is an extremely powerful tool for scaling up analysis because not only can you do a lot of repetitive work with very little code, the code is executed using parallel processing when possible. This makes it much faster because it distributes tasks across processing cores on your computer (nice!).\n  References for advanced topics As you develop your substantive scholarly interests, it is likely that you will develop a methodological expertise to fit your topic of study. Luckily, many others have come before you and have developed R tools for doing these analyses. Better yet, these computational tools are being increasingly folded into a tidyverse-style tools. We’ll quickly point out a few of these resources. You will NOT be required to use these tools for any take-home exercises.\nSome higher-level advice for navigating these packages:\n My philosophy is that I like to rely on external packages for computation and estimation, but not for graphics. If there is a tool that estimates a model for me, or performs a particular statistical test, then that’s great. But I tend not to like the graphics that these tools produce. As a result, I look for tools that make it easy for me to extract the data that I want to plot. Sometimes it is tedious to extract the data from these objects. In these situations, I tend to write my own functions to process the output from these packages into a tidy format for plotting or tabulating. If you want a quick and easy way to learn about packages, make a Twitter for your “academic self” and follow some researchers and R developers.  Survey analysis The statistics that we learn apply to data collected from simple random samples. In the real world, however, survey data often require some kind of clustered sample design and contain accompanying sample weights. Analyzing surveys requires (or, should require) accounting for weights and design as separate elements of the analysis.\nIf you have a non-clustered sample design but some degree of oversampling, you might handle weights analytically—calculating weighted means and weighted sample sizes. If you have a more advanced sample design, you should incorporate elements of the design into the estimation. To that end, I’d recommend Thomas Lumley’s survey package. You use your dataset to create a new object that contains metadata about the cluster structure of the sample. Functions in the survey package then use the metadata about the sample design to estimate things properly. This is similar to the way you can declare survey design information using svy-based commands in Stata.\n Time series For time series, you will want some special tools to deal with the accompanying statistical pitfalls: functional forms for autocorrelated errors, standard errors for autocorrelation, and the estimation of ancillary parameters for models designed for certain temporal interventions.\nFirst, for data manipulation, you will want some kind of data structure that contains metadata about which variable defines the time period. This structure will allow you to properly calculate differenced variables, lags, and leads. To create tidy, time-aware tibble datasets, you could use tibbletime or the more recent (and supposedly more capable) tsibble. You could also check out the lubridate, zoo, and hms (Tidyverse!) packages for manipulating data-time variables, since the baked in R tools for dealing with POSIXct and POSIXlt data are very difficult to figure out. If you read that sentence and were like “wtf are POSIXct and POSIXlt?”, that’s exactly what I mean.\nFor time series modeling, you will want tools that perform a variety of functions.\n ARIMA modeling Unit root and (fractional) integration testing modeling for interventions, autoregressive distributed lag (ADL), error-correction (ECOM) vector autoregression (VAR), granger causality tests, impulse-response functions, and so on  I don’t have expert-level advice here, but when I took our time series ITV course, I found the following packages useful for several of these needs: TSA, fUnitRoots, egcm, fracdiff, forecast.\n Panel data Panel data tends to be the realm of “fixed-effects” modeling, meaning that when you measure features over time, time-invariant predictors are absorbed into fixed unit-level averages, and time-varying features have coefficients that are constant across time. I don’t typically do this kind of analysis, but those who do often use the plm package for these types of models.\nAlternatives to plm include hierarchical modeling approaches, which we’ll cover in a separate subsection.\n Hierarchical/multilevel models For complex hierarchical data structures (individuals within time periods, individuals within geographic groups, observations within countries within regions within time periods…), hierarchical models may more be a more direct modeling approach to attributing variation in the data to covariates at different levels of analysis without as much scrutiny about clustered variance estimators and so on. This is because hierarchical modeling allows you to directly model parameter estimates as functions of covariates at other levels of the data. For example, the probability that an individual votes Republican may be a function of their demographic characteristics but also the context of state the state in which they live.\n\\(\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Bernoulli}\\left( \\pi_{i} \\right) \\\\[6pt] \\mathrm{logit} \\left( \\pi_{i} \\right) \u0026amp;= \\alpha + \\gamma^{\\mathtt{demographics}}_{j[i]} + \\delta^{\\mathtt{state}}_{s[i]} \\end{align}\\)\nThe state effect applies to every individual in that state and could itself be a regression on state-level features such as the presidential vote in the state, state-level economics, and so on.\n\\(\\begin{align} \\delta^{\\mathtt{state}}_{s} \u0026amp;\\sim \\mathrm{Normal}\\left( \\beta_{1} \\mathrm{pvote}_{s} + \\beta_{2} \\mathrm{GDP}_{s} + \\ldots , \\, \\sigma^{\\mathtt{state}} \\right)\\end{align}\\)\nThis kind of modeling is useful because it allows estimates for small groups to “borrow strength” from larger groups. If we don’t have a lot of data for Alabama, for example, we can say that Alabama is probably like other states that have similar state-level characteristics, and it shrinks Alabama’s estimate toward the state regression trend. In other words, for small-\\(n\\) groups, we assume that the group-level effect looks like the other group-level effects unless the data give us a strong signal to the contrary. This is a key example of the bias-variance trade-off you heard about in stats courses.\nAlthough hierarchical models are “essentially Bayesian” because of the partial pooling setup, there are packages for fitting approximate maximum-likelihood versions. The most common would be lme4, which provides syntax similar to nlme for varying (“random”) effects, but it is more updated than nlme. What I’m saying is, don’t use nlme. The arm package provides additional tools for interacting with lme4 hierarchical models, including the bayesglm function that just says “screw-it” and fits the fully Bayesian version of the model. On that subject…\n Bayesian analysis Bayesian analysis varies from “frequentist” statistics in a few fundamental ways. The main source of difference is philosophical, where uncertainty estimates are understood as your uncertainty about the actual value of the parameter, and not uncertainty about the data. Stated differently, frequentism measures the probability of the data given an assumed model of null parameter values and infinitely repeated sampling. Bayesian statistics rejects the idea of the null model entirely and instead measures the probability parameter values after having observed the data, which requires prior information over the parameter values. When it comes to the actual parameter estimates, you can think about maximum likelihood models as being special cases of Bayesian models where the researcher inserts no prior information about the parameter values.\nThere are a few ways to fit Bayesian models. For reduced-form regression models (like lm and glm functional forms), you can use packages such as arm, brms, and rethinking to write Bayesian models using a syntax similar to glm and lme4 models.\nFor complicated structural models that are not easily expressed in a single regression equation (e.g. when you have a complex multi-level structure), you can may want to set up a fully Bayesian model using external Bayesian modeling software that can be accessed by R. For simpler models, one could use JAGS, which samples a posterior distribution using a Gibbs sampling algorithm. You would use the rjags package to talk to JAGS using R. For more complex hierarhical models, randomly-walking algorithms for Gibbs sampling (like JAGS) do a poor job, so I recommend using Stan (and talking to it with R using the rstan package). Stan fits the model using a version of Hamiltonian Monte Carlo, both of which drastically increase the speed and quality of posterior sampling. The Stan syntax is more complicated than JAGS, but the payoff of using Stan is worth it.\nFor diagnosing and visualizing Bayesian model results, rstan has some tools baked in. The ggmcmc package turns posterior samples into a tidy data frame (good for ggplot!), and bayesplot provides other tools for easy Bayes graphics.\n R as front-end As the Bayes packages indicate, R can serve as a front-end interface to other programs and syntaxes. Some further examples include the following packages…\n rsql and RSQLite for SQL and SQLite Rcpp for C++ rPython for Python  …and so on\n More materials from past years Sarah Bouchat (former instructor for this course) has online materials for some additional topics, including text analysis, Regular Expressions (RegEx), base graphics, loops, and so on. (I purposefully don’t teach loops because apply() functions are better!)\nView Sarah’s site here.\n  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"c0a5fa58001844be0f1a476c1605ff7b","permalink":"/courses/811/811-05-analysis/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/811/811-05-analysis/","section":"courses","summary":"Read this before our final lecture, after the graphics lesson.\nHow to follow along A script file walking through some of these commands is available here.\n Objectives In this lesson, we will introduce how to do statistical analysis using R. Topics that you should cover to prepare for the take-home exercise include…\n Means, confidence intervals, and simple significance tests Estimating regression models Generating model output Model diagnostics and fit statistics Post-estimation graphics (model predictions)  This page also contains some content on more advanced topics, but these won’t be necessary for the take-home exercise.","tags":null,"title":"Analysis","type":"docs"},{"authors":null,"categories":null,"content":"This repository contains code and documentation for compiling and cleaning data for the State Election Landscapes project. In the project\u0026rsquo;s own words\u0026hellip;\n State Election Landscapes provides a comprehensive overview of how elections are conducted in the six Great Lake States: Illinois, Indiana, Michigan, Michigan, Minnesota, and Wisconsin. [The] project\u0026hellip;provides:\n narrative material about the legal setting of elections links to important official information about the conduct of elections, and a visualization tool that helps display important statistical information about elections.   The project was led by the MIT Election Data and Sciences Lab in collaboration with researchers at the University of Wisconsin, the University of Minnesota, the Ohio State University.\n","date":1566432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566765989,"objectID":"50c96f25972d86911e6c78fbab9f0f83","permalink":"/code/great-lakes/","publishdate":"2019-08-22T00:00:00Z","relpermalink":"/code/great-lakes/","section":"code","summary":"Code and documentation for cleaning and organizing election data from Midwest states","tags":null,"title":"State Election Landscapes: The Great Lakes States","type":"code"},{"authors":null,"categories":null,"content":"In 2019 I organized and led discussion for a summer reading group on causal inference methods. Much of the material focused on \u0026ldquo;the view from political science,\u0026rdquo; but we often brought in resources from statistics, economics, and epidemiology. We explored potential outcomes and graphical models for causal inference, with estimation approaches covering regression approaches, propensity score adjustments, and other structural models.\nThe reading list below contains both the assigned and the \u0026ldquo;bonus\u0026rdquo; readings.\nReadings Introduction  Rubin (1974), \u0026ldquo;Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies\u0026rdquo; LaLonde (1986), \u0026ldquo;Evaluating the Econometric Evaluations of Training Programs with Experimental Data\u0026rdquo; McElreath (2019), \u0026ldquo;Statistical Rethinking 2019\u0026rdquo; Lecture 6 (DAGs) Aronow and Samii (2016), \u0026ldquo;Does Regression Produce Representative Estimates of Causal Effects?\u0026quot;  Causal Identification  Holland (1986), \u0026ldquo;Statistics and Causal Inference\u0026rdquo; Kang and Schafer (2007), \u0026ldquo;Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data\u0026rdquo; Sekhon (2007), \u0026ldquo;The Neyman-Rubin Model of Causal Inference and Estimation via Matching Methods\u0026rdquo; Stuart (2010), \u0026ldquo;Matching methods for causal inference: A review and a look forward\u0026rdquo; Bowers, Fredrickson, and Panagopoulos (2013), \u0026ldquo;Reasoning about Interference Between Units: A General Framework\u0026rdquo; Hartman, Grieve, Ramsahai, and Sekhon (2015), \u0026ldquo;From sample average treatment effect to population average treatment effect on the treated: combining experimental with observational studies to estimate population treatment effects\u0026rdquo; Keele (2015), \u0026ldquo;The statistics of causal inference: A view from political methodology\u0026rdquo; Hartman and Hidalgo (2018), \u0026ldquo;An equivalence approach to balance and placebo tests\u0026rdquo;  Causal Mediation  Imai, Keele, Tingley, and Yamamoto (2010), \u0026ldquo;Unpacking the black box of causality: Learning about causal mechanisms from experimental and observational studies\u0026rdquo; VanderWeele and Vansteelandt (2015), \u0026ldquo;Mediation Analysis with Multiple Mediators\u0026rdquo; Myers and Tingley (2016), \u0026ldquo;The Influence of Emotion on Trust\u0026rdquo;  Causal Heterogeneity  Kam and Trussler (2017) \u0026ldquo;At the Nexus of Observational and Experimental Research: Theory, Specification, and Analysis of Experiments with Heterogeneous Treatment Effects\u0026rdquo; Coppock, Leeper, and Mullinix (2017), \u0026ldquo;Generalizability of heterogeneous treatment effect estimates across samples\u0026rdquo; Meager (2019), \u0026ldquo;Understanding the Average Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of Seven Randomized Experiments\u0026rdquo;  Null Effects and Statistical Power  Mayo (2004), \u0026ldquo;An error-statistical philosophy of evidence\u0026rdquo; Broockman (2014), \u0026ldquo;Do female politicians empower women to vote or run for office? A regression discontinuity approach\u0026rdquo; Gelman and Carlin (2014), \u0026ldquo;Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors\u0026rdquo; Bhavnani (2017), \u0026ldquo;Do the Effects of Temporary Ethnic Group Quotas Persist? Evidence from India\u0026rdquo; Coppenolle (2017), \u0026ldquo;Political Dynasties in the UK House of Commons: The Null Effect of Narrow Electoral Selection\u0026rdquo; Loken and Gelman (2017), \u0026ldquo;Measurement error and the replication crisis\u0026rdquo; Fowler (2019), \u0026ldquo;But Shouldn’t That Work Against Me?\u0026quot;  G-Methods  Naimi, Cole, and Kennedy (2017), \u0026ldquo;An introduction to g methods\u0026rdquo; Acharya, Blackwell, and Sen (2016), \u0026ldquo;Explaining Causal Findings Without Bias: Detecting and Assessing Direct Effects\u0026rdquo;  DAGs (1)  Greenland, Pearl, Robins (1999), \u0026ldquo;Causal Diagrams for Epidemiological Research\u0026rdquo; Keele, Stevenson, and Elwert (2019) \u0026ldquo;The causal interpretation of estimated associations in regression models\u0026rdquo;  DAGs (2)  VanderWeele and Robins (2014) \u0026ldquo;Signed directed acyclic graphs for causal inference\u0026rdquo; Mohan and Pearl (2014), \u0026ldquo;Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data\u0026rdquo; Imbens (2019), \u0026ldquo;Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics\u0026rdquo; Lattimore and Rohde (2019), \u0026ldquo;Replacing the do-calculus with Bayes rule\u0026rdquo;  Machine Learning  Hastie, Tibshirani, and Friedman (2009) \u0026ldquo;The Elements of Statistical Learning Data Mining, Inference, and Prediction (2 ed.)\u0026quot; Hill (2011), \u0026ldquo;Bayesian Nonparametric Modeling for Causal Inference\u0026rdquo; Samii, Paler, and Daly (2016), \u0026ldquo;Retrospective causal inference with machine learning ensembles: An application to anti-recidivism policies in Colombia\u0026rdquo; Wager and Athey (2018), \u0026ldquo;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests\u0026rdquo; Abraham and Sun (2019), \u0026ldquo;Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects\u0026rdquo; Cheng, Khomtchouk, Matloff, and Mohanty (2019), \u0026ldquo;Polynomial Regression as an Alternative to Neural Nets\u0026rdquo; Hahn, Murray, and Carvalho (2019), \u0026ldquo;Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects\u0026rdquo; McElreath (2019), \u0026ldquo;Statistical Rethinking 2019\u0026rdquo; Lecture 19 (Gaussian Processes)  Regression Discontinuity  Calonico, Cattaneo, and Titiunik (2014), \u0026ldquo;Robust Nonparametric Confidence Intervals for Regression-discontinuity Designs\u0026rdquo; Caughey and Sekhon (2014), \u0026ldquo;Elections and the regression discontinuity design: Lessons from close US house races, 1942–2008\u0026rdquo; Skovron and Titiunik (2015), \u0026ldquo;A Practical Guide to Regression Discontinuity Designs in Political Science\u0026rdquo; Imbens and Wager (2019), \u0026ldquo;Optimized Regression Discontinuity Designs\u0026rdquo;  Time-Series Cross-Sectional and Panel Data  Blackwell and Glynn (2018), \u0026ldquo;How to Make Causal Inferences with Time-Series Cross-Sectional Data under Selection on Observables\u0026rdquo; Goodman-Bacon (2018), \u0026ldquo;Difference-in-Differences with Variation in Treatment Timing\u0026rdquo; Torres (2019), \u0026ldquo;Estimating Controlled Direct Effects Through Marginal Structural Models\u0026rdquo;  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601785957,"objectID":"cbaca45d42e6ab4f84d09143c4dab1a9","permalink":"/teaching/causal-inf-2019/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/teaching/causal-inf-2019/","section":"teaching","summary":"Kinda like \"summer book club\" but for nerds\n","tags":["Teaching","Methods","Causal Inference","Bayesian statistics","Computational Methods"],"title":"Causal Inference Reading Group\n","type":"teaching"},{"authors":null,"categories":null,"content":"Data are increasingly prevalent in politics (and in journalism, industry, academia\u0026hellip;), but data are complex and challenging. What can we learn about politics, and the social world more broadly, when we take an empirical view of it? When should we trust data, and when will data lead us astray?\nPolitical science 270 is an undergraduate course on consuming, criticizing, and producing data-driven analysis of political and social phenomena. Students learn to think about political data as evidence for or against social theories. The course emphasis practical data analysis skills that apply beyond the political science curriculum, including elementary statistical analysis, R programming (Rstudio, tidyverse, ggplot2), and technical writing.\nView course materials on Github\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601785957,"objectID":"978b784969e4a10c8a5d879c7d21495d","permalink":"/teaching/numbers/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/teaching/numbers/","section":"teaching","summary":"Quantitative political analysis and R for undergraduates\n","tags":["Teaching","Methods","Computational Methods"],"title":"PS 270: Understanding Political Numbers\n","type":"teaching"},{"authors":null,"categories":null,"content":"A Github repo for a simple business card built with $\\mathrm{\\LaTeX}$.\nAn example:\n","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566766708,"objectID":"4a6a2fb7705bd5bb9b9d0e3d05b15617","permalink":"/code/business-card/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/code/business-card/","section":"code","summary":"Code for a simple business card","tags":null,"title":"$\\mathrm{\\LaTeX}$ Business Card\n","type":"code"},{"authors":["Michael DeCrescenzo","Kenneth R. Mayer"],"categories":null,"content":"","date":1576195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593467257,"objectID":"365466d1d86b3dfc540f15ed79d638af","permalink":"/publication/nonvoters/","publishdate":"2019-12-13T00:00:00Z","relpermalink":"/publication/nonvoters/","section":"publication","summary":"How much did Wisconsin's voter identification requirement matter in 2016? We conducted a survey of registered nonvoters in the counties surrounding the cities of Milwaukee and Madison to estimate the number of registrants who experienced ID-related voting difficulties in the 2016 presidential election. We estimate that 10 percent of nonvoters in these counties lack a qualifying voter ID or report that voter ID was at least a partial reason why they did not vote in 2016, and six percent of nonvoters lacked a voter ID or cited voter ID as their primary reason for not voting. Theoretically, we argue that voter ID requirements “directly” affect voters who lack qualifying IDs but also “indirectly” affect voters who are confused about their compliance with the law. We find evidence of such confusion, with many respondents mistakenly believing that they did not have the necessary ID to vote when they actually did. Our analysis permits us to calculate bounds on the possible turnout effect in 2016. Most of our credible estimates suggest that the voter ID requirement reduced turnout in these counties by up to one percentage point.","tags":["Elections","Election Administration"],"title":"Voter Identification and Nonvoting in Wisconsin—Evidence from the 2016 Election","type":"publication"},{"authors":null,"categories":null,"content":"Custom document templates for the following document types:\n $\\mathrm{\\LaTeX}$ article ($\\mathrm{pdf\\LaTeX}$) Rmarkdown template files for articles and syllabi ($\\mathrm{pdf\\LaTeX}$) Preamble for Rmarkdown document compiled to $\\mathrm{pdf\\LaTeX}$ (Coming soon) Bookdown template file for UW–Madison thesis (based on buckydown)  ","date":1562630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566766939,"objectID":"9511cde01405c19ea9111fe6f2424832","permalink":"/code/doc-templates/","publishdate":"2019-07-09T00:00:00Z","relpermalink":"/code/doc-templates/","section":"code","summary":"My go-to document styles","tags":null,"title":"Rmarkdown/$\\mathrm{\\LaTeX}$ Templates for Papers and Syllabi\n","type":"code"},{"authors":null,"categories":null,"content":"2020 Materials View syllabus here. Weekly materials below:\n Introduction. [Slides] Projects \u0026amp; Intro R Markdown. [Slides] [Example Project] [Rmd Demo] Basics of R. [Script] [Homework] Data Manipulation (tidyverse/dplyr). [Script] [Homework] Graphics (ggplot2). [Script] [Homework] Regression (broom). [Slides] [Script] [Homework] Data reading, cleaning, joining. [Lecture] [Script] Data shaping (tidyr). [Script] R Markdown. [Scripts/Rmd Setup] [Homework: Final Pt 1] Git: Introduction. [Slides] [Homework: Final Pt 2] Intermediate Git: Data, branches, rewinding. [Slides] Functional Programming: scoped verbs, apply functions, purrr. [Script] Causal Inference: regression discontinuity, bootstrapping. [Slides] [Script] Machine Learning for Prediction and Causal Inference. [Slides] [Script coming soon]  2018 materials In 2018, this course was split between R and Stata. I covered the R unit, and Hannah Chapman covered Stata.\nR notes for 2018 are here, and the source code (.rmd files) for these workbooks are here.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601785957,"objectID":"f729a68b6266340430fc34aa931f4fd6","permalink":"/teaching/computing-811/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/teaching/computing-811/","section":"teaching","summary":"PhD-level computing course on R, R Markdown, and Git. \n","tags":["Teaching","Methods","Computational Methods"],"title":"PS 811: Introductory Statistical Computing for Political Science\n","type":"teaching"},{"authors":null,"categories":null,"content":"I have two years of experience teaching the UW political science \u0026ldquo;math bootcamp\u0026rdquo; for incoming graduate students. My most recent materials (2018) are open-source on Github.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566486660,"objectID":"470d62996d8cd9d0df94466609230cda","permalink":"/teaching/math-camp/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/teaching/math-camp/","section":"teaching","summary":"Incoming grad student \"prefresher\" workshop on algebra, calculus, and probability\n","tags":["Computational Methods"],"title":"Graduate Math Camp (UW–Madison Political Science)\n","type":"teaching"},{"authors":["Barry C. Burden","Michael DeCrescenzo"],"categories":null,"content":"Groups in the electorate vote differently from one another. Racial minorities and young voters prefer Democratic candidates on average, while evangelicals and rural voters prefer Republican candidates. How do these group differences affect party vote shares, given that the underlying groups are differently sized? Most research characterizes voting preferences by conditioning vote choices on group membership, which obscures the numerical impact of group dynamics in voting across time.\nWe lay out a theoretical framework and a generative modeling approach to understand how group differences in voting evolve over time and aggregate into election outcomes. The approach decomposes group shares into useful theoretical components for understanding how groups affect elections and in what way: partisan predispositions, partisan mobilization, swing voting, and the choices of party-unaffiliated voters. The method traces how these theoretical mechanisms change over time, and it can be extended for multichotomous groups and more complex hierarchical modeling approaches.\nWe apply this new method to the gender gap in U.S. presidential voting. Many observers speculate that a larger gender gap in voting—larger Democratic vote shares among women than among men—numerically benefits the Democratic Party. We show, by contrast, that the size of the gender gap has no necessary bearing on the partisan vote outcome. Rather, the relationship is contingent on the changing numerical impact of partisanship, partisan mobilization, and persuasion over time. Although the gender gap and the Democratic vote in presidential elections have both increased over the years, this relationship is spurious. The primary cause of the gender gap (partisan change) was actually harmful to the Democrats. Meanwhile, forces that increased the Democratic vote (mobilization and persuasion) were minor influences on the gender gap.\nThe insights enabled by this approach can be extended to other salient groups in the electorate including race, urban/rural divisions, socioeconomic groups, and more.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593467257,"objectID":"5db08898d853d4ac7cebf6546d175ed4","permalink":"/research/gender-gap/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/research/gender-gap/","section":"research","summary":"A new approach for understanding parties' electoral coalitions (with Barry C. Burden)","tags":["Elections","Methods"],"title":"Partisanship, Mobilization, and Persuasion in Group Voting: A Study of the Gender Gap\n","type":"research"},{"authors":null,"categories":null,"content":"Causal inference and Bayesian analysis are two powerful and attractive methodological approaches for conducting empirical research, but rarely in political science does a single study employ both approaches. This stands in contrast to other fields\u0026mdash;such as psychology, epidemiology, and biostatistics\u0026mdash;where Bayesian and causal methods are more commonly applied together. In this paper I argue that the partition between these methodological schools in political science has no inherent basis in their fundamental goals, which are actually quite compatible. In fact, Bayesian analysis provides a number of distinct benefits for estimating statistical models for causal inference. The methodological partition instead owes itself to informal norms surrounding each school in empirical political science. I discuss these sources of normative tension, describe go-to practices doing skeptical Bayesian causal inference, and demonstrate these practices using real examples from recent political science publications. The purpose of the paper is not to convince all causal inference practitioners to adopt Bayesian estimation. The purpose is to show how Bayesian methods add value to causal effects by improving causal estimates and providing an appealing framework for evaluating causal evidence.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593467257,"objectID":"f39a2483db38f22a533e1063abbf6300","permalink":"/research/causal-bayes/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/research/causal-bayes/","section":"research","summary":"Posterior beliefs over unobserved potential outcomes","tags":["Elections","Methods"],"title":"Bayesian Causal Inference in Political Science\n","type":"research"},{"authors":null,"categories":null,"content":"Every year, the UW Political Science department holds a $\\mathrm{\\LaTeX}$ workshop\u0026mdash;a few introductory training sessions for graduate students on document preparation. I have been involved with organizing this workshop since 2015, and have been leading it since 2017.\nAll materials for the worksho, source code and all, are publicly available (in their 2018 vintage) on my Github.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566486660,"objectID":"fff795e04e874f0541a589b924bc0790","permalink":"/teaching/latex-workshop/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/teaching/latex-workshop/","section":"teaching","summary":"$\\mathrm{\\LaTeX}$ (and RMarkdown!) training for UW polisci graduate students\n","tags":["Computational Methods"],"title":"Graduate $\\mathrm{\\LaTeX}$ Workshop (UW–Madison Political Science)\n","type":"teaching"},{"authors":null,"categories":null,"content":"This paper uses a conjoint experiment to study how common journalistic practices for presenting polling data affect audience confidence in election outcomes. Using a survey experiment of Mechanical Turk (MTurk) participants, we study how people process polling data when it is expressed as a single poll, as an average of recent polls, or as a probabilistic forecast. The conjoint study varies the circumstances of the election—vote share for each candidate, the amount of undecided voters—and journalistic strategies believed to improve the audience\u0026rsquo;s interpretation of uncertain polling information—language about margins of sampling error and graphical depictions of uncertainty and possible election outcomes. Our results help to empirically ground discussions about ways to improve coverage of survey data and public opinion by accounting for how the public actually understands and makes sense polls and forecasts, aligning journalistic aims with strategies for effective communication.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593467257,"objectID":"46725d65d1c08ebfdc64362d665c3357","permalink":"/research/forecast-uncertainty/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/research/forecast-uncertainty/","section":"research","summary":"An experiment about uncertainty over polls and forecasts (with Benjamin Toff and Zach Warner)","tags":["Elections"],"title":"Polls, Forecasts, and Voters’ Perceptions of Uncertainty\n","type":"research"},{"authors":["Barry C. Burden","Evan Crawford","Michael DeCrescenzo"],"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593467257,"objectID":"0ff5b6054c07fe201ce1c8c0cc648dca","permalink":"/publication/gender-2016/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/gender-2016/","section":"publication","summary":"Because of the particular candidates who ran, the 2016 presidential campaign was defined by gender to a remarkable degree. This led many observers to expect a historically large gender gap in voting. In contrast to these expectations, the gender gap between men and women’s votes in 2016 was only slightly larger than in other recent elections. We argue that an immense gender divide did not emerge because it was constrained by high levels of partisanship in the electorate, especially “negative partisanship” toward the opposing party that leaves little room for gender to matter. In addition, we challenge two common assumptions: that the gender gap helps Democratic candidates and that women were more persuadable than men over the course of the campaign. Both men and women vacillated in their views of Clinton’s honesty during the campaign, with men shifting away from her and toward Trump just before election day.\n","tags":["Elections","Methods"],"title":"The Unexceptional Gender Gap of 2016\n","type":"publication"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Motivation Many researchers, when they’re introduced to Bayesian methods, are nervous about the possibility that their prior distributions will corrupt their posterior inferences. Since they know that the posterior distribution is a precision-weighted average of the prior and the data (or “likelihood”), it initially makes sense to err toward a flatter, more diffuse prior density for model parameters. These diffuse densities let the model put relatively more weight on data, which feels safer.\nThe purpose of this post is to highlight a few areas where this “default tendency” to use flat priors runs into unexpected consequences. We show how functions of model parameters have implied priors: density functions of their own that inherit the prior uncertainty about the parameters that compose the function. These implied priors can have strange shapes that you wouldn’t anticipate based on the raw parameters.1 We then show two cases where these strange shapes appear. The first case comes from my own published work on voter ID in Wisconsin. The second case is a hypothetical experiment where we “accidentally” create a non-flat prior for the treatment effect in a randomized experiment where we weren’t expecting it.\nTogether, these exercises give concrete examples for the way flat priors and “uninformative” don’t necessarily mean the same thing.\n Implied priors We’ll begin with a notion of the implied prior. With some random variable \\(\\theta\\), we can construct a function \\(f\\left(\\theta\\right)\\), which is necessarily a random variable as well. If \\(\\theta\\) has a probability distribution \\(p\\left(\\theta\\right)\\), \\(f\\left(\\theta\\right)\\) will have some probability distribution \\(p\\left(f\\left(\\theta\\right)\\right)\\).\nA simple example. Imagine some standard normal variable \\(\\nu\\) that is distributed \\(\\mathrm{Normal}\\left(0, 1\\right)\\) prior. If we have some function \\(f(\\nu) = \\mu + \\sigma\\nu\\), then \\(f(\\nu)\\) will have a probability distribution. In this case, it is straightforward to see that this prior would be \\(\\mathrm{Normal}(\\mu, \\sigma)\\), but in more complicated examples it won’t be so easy to glean the implied prior directly. We can create this example using code, setting \\(\\mu = 4\\) and \\(\\sigma = 2\\), to reassure you that I’m telling the truth.\nlibrary(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;hrbrthemes\u0026quot;) library(\u0026quot;latex2exp\u0026quot;) library(\u0026quot;viridisLite\u0026quot;) theme_ipsum(base_family = \u0026quot;Fira Sans\u0026quot;) %+replace% theme( panel.grid.minor = element_blank(), axis.title.x.bottom = element_text( margin = margin(t = 0.35, unit = \u0026quot;cm\u0026quot;), size = rel(1.5) ) ) %\u0026gt;% theme_set() accent \u0026lt;- viridis(n = 1, begin = 0.5, end = 0.5) # set mu and sigma values mu \u0026lt;- 4 sigma \u0026lt;- 2 # simulate nu and f(nu) normal_example \u0026lt;- tibble( nu = rnorm(100000), f_nu = mu + (nu * sigma) ) %\u0026gt;% print(n = 4) ## # A tibble: 100,000 x 2 ## nu f_nu ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.669 5.34 ## 2 -0.448 3.10 ## 3 0.0987 4.20 ## 4 0.591 5.18 ## # … with 99,996 more rows # implied prior is Normal(mu, sigma) ggplot(normal_example) + aes(x = f_nu) + geom_histogram(binwidth = .1, fill = accent) + geom_vline( xintercept = c(mu - sigma, mu + sigma), linetype = \u0026quot;dashed\u0026quot;, size = 0.25 ) + geom_vline(xintercept = mu) + scale_x_continuous( breaks = seq(mu - 5*sigma, mu + 5*sigma, sigma) ) + scale_y_continuous(labels = scales::comma) + labs( title = \u0026quot;Implied Prior for Transformed Normal\u0026quot;, subtitle = \u0026quot;Histogram of prior samples\u0026quot;, x = TeX(\u0026quot;$f(\\\\nu) = \\\\mu + \\\\sigma\\\\nu, \\\\; \\\\nu \\\\sim N(0, 1)$\u0026quot;), y = NULL ) Bayesians will recognize this as a “non-centered parameterization” of a Normal distribution, or a normal distribution that sneaks the mean and standard deviation values out of the random variable. Bayesian modelers invoke this trick all the time in hierarchical models, since sampling \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\) separately is easier for a computer to do than sampling a distribution for \\(f(\\nu)\\) that itself contains \\(\\mu\\) and \\(\\sigma\\). Parameterizations that de-correlate these parameters are generally easier to sample and, conveniently, more manageable to set priors for.\n Flat priors meet nonlinear transformations Suppose we have a parameter \\(\\pi\\) has a flat prior in the \\([0, 1]\\) interval, and we calculate some function \\(g(\\pi)\\). Will \\(g(\\pi)\\) have a flat distribution? It depends.\nI encountered an example of this in my work with Ken Mayer on voter ID in Wisconsin in Wisconsin,2 although I didn’t put this lesson about prior flatness in the paper. We wanted to estimate the number of eligible registered voters in two Wisconsin counties for whom Wisconsin’s voter ID requirement hindered their voting in 2016. This estimate contains three ingredients.\n \\(N\\): the number of individual records in the registered voter file for the target population. \\(\\epsilon\\): the proportion of the population in the voter file that was eligible to vote in 2016. This is relevant because voter files contain individuals who moved, died, or who were otherwise ineligible, that have to be cleaned out of voter files periodically. The population size must be penalized by \\(\\epsilon\\) to remove these ineligible records from our estimate. Estimated by coding a finite sample of the voter file. \\(\\pi\\): the proportion of eligible registrants who experienced ID-related difficulty voting. Estimated using a survey sample of registrants in the voter file.  The quantity we want to estimate is \\(N\\epsilon\\pi\\), an eligibility-penalized population estimate for the number of voters affected by the voter ID requirement. Suppose that we know \\(N = 229,625\\) from the voter file, but \\(\\pi\\) and \\(\\epsilon\\) are proportions that must be estimated. We give each proportion a flat \\(\\mathrm{Beta}(1, 1)\\) prior on the \\([0, 1]\\) interval. What is our implied prior for the population estimate?\ntibble( pi = rbeta(10000, 1, 1), epsilon = rbeta(10000, 1, 1), N = 229625, pop_estimate = N * epsilon * pi ) %\u0026gt;% ggplot() + aes(x = pop_estimate) + geom_histogram(fill = accent, bins = 100, boundary = 0) + labs( title = \u0026quot;Implied Prior for Population Estimate\u0026quot;, subtitle = TeX(\u0026quot;Histogram of prior simulations\u0026quot;), x = TeX(\u0026quot;Population estimate $= N \\\\epsilon \\\\pi$\u0026quot;), y = NULL ) + scale_x_continuous(labels = scales::comma) Seriously, what? If I had plopped this graphic into my paper and said that it was my prior for this population quantity, I would have been in trouble. Look at how swoopy that prior looks! How can that be an uninformative prior? Well, we know that the random components have vague priors, so this is the natural result of sending these parameters through a nonlinear function. If you don’t like it, you should think about how this quantity is parameterized and whether some other priors make more sense.\nBayesians may recognize this feature of prior distributions as well, where nonlinear functions of parameters have a density that does not simply reflect a shifting or scaling of the original density. This happens because nonlinear transformations of parameters can “squish” or “stretch” probability mass into different areas/volumes than they were previously, thereby changing probability density. If we wanted to write out the new density, we would need to start with the old density and use (spooky voice) the Jacobian.\n Honey, I shrunk my treatment effect When we think about causal inference, we are thinking about methods that want to be light on their assumptions. If we imagine a Bayesian interpretation of this experiment (even if we don’t specify the Bayesian model per se), it makes sense that we want vague priors on important quantities in order to “let the data speak” instead of deriving results from the prior. This turns out to be less straightforward than you would think.\nImagine an experiment where we treat individuals with an advertisement or we don’t, \\(z_{i} \\in \\{0, 1\\}\\), and then we measure whether they intend to vote for the Democratic candidate or not, \\(y_{i} \\in \\{0, 1\\}\\). My treatment effect \\(\\tau\\) is the comparison between the Democratic vote proportion in the control group, \\(\\mu_{z = 0}\\), and the Democratic vote proportion in the treatment group, \\(\\mu_{z = 1}\\). \\[\\begin{align} \\tau = \\mu_{1} - \\mu_{0} \\end{align}\\]\nIf we estimate this effect with a linear model, we have a choice about how to parameterize the regression function. We could use a constant and a treatment effect with error term \\(u_{i}\\), \\[\\begin{align} y_{i} = \\mu_{0} + \\tau z_{i} + u_{i} \\end{align}\\] or we have two intercepts for each condition. \\[\\begin{align} y_{i} = \\mu_{z[i]} + u_{i} \\end{align}\\] Bayesians think it makes more sense to use the second parameterization. Why? If we want to set the same prior on both groups, it’s easier to do that when we can directly set priors on each mean instead of on one mean and the difference-in-means. So let’s take that approach, giving each vote share a flat prior that says any vote share for both groups is a priori equally likely. I will write these as flat Beta densities, but you could imagine them as standard Uniform densities as well. \\[\\begin{align} \\mu_{1}, \\mu_{0} \\sim \\mathrm{Beta}(1, 1) \\end{align}\\] What is the prior for the treatment effect (the difference-in-means)? Not flat! We will again simulate to see the effect of combining parameters into a single function.\n# simulate means and calculate difference tibble( mu_0 = rbeta(100000, 1, 1), mu_1 = rbeta(100000, 1, 1), trt = mu_1 - mu_0 ) %\u0026gt;% ggplot() + aes(x = trt) + geom_histogram(fill = accent, binwidth = .05, boundary = 0) + scale_y_continuous(labels = scales::comma) + labs( title = \u0026quot;Implied Prior for Treatment Effect\u0026quot;, subtitle = \u0026quot;Histogram of prior samples\u0026quot;, x = \u0026quot;Difference in means\u0026quot;, y = NULL ) What happened to my vague prior beliefs? Why do I have this non-flat prior for something I thought I wanted to have vague information for?3\nIt still is a vague prior, but we’re wrong to expect it to be flat. Why? Well, averaging over my prior uncertainty in both groups, my expected difference in means ought to be mean zero (natch). But more than that, the reason why we get a mode at zero there are many more ways to produce differences near zero with my raw means than differences far from zero. The only way to get big differences (near \\(-1\\) or \\(1\\)) is for both means to be far apart, which isn’t as likely to happen randomly as two means that are closer together in the prior. When we think about the treatment effect prior in this way, we can understand why this actually feels less informed than a direct flat prior for the treatment effect. Putting a flat prior on the treatment effect is saying that we think big differences are just as likely as small differences. This is like a prior that says my group means should be negatively correlated, effectively upweighting bigger differences from what we’d otherwise expect. Weird! I’d rather set reasonable priors for my means and let my treatment prior do what it do.\n Flat priors often have non-flat implications These implications feel strange at first, but they are all around us whether or not we notice them. The flatness of a prior (or any shape, flat or not) is a relative feature of a model parameterization or a quantity of interest, not an absolute one. Inasmuch as we believe priors are at work even when we don’t want to think about them—i.e. we accept Bayesian models as generalizations of likelihood models—we should respect how transforming a likelihood affects which parameters are exposed to the researcher, and which spaces those parameters are defined in. We should know that flat doesn’t imply uninformative, and that non-flat doesn’t imply informative. What we’re seeing here is that flatness begets non-flatness in tons of circumstances, and that’s totally ordinary. And more examples of how prior predictive checks show us what our model thinks about key quantities of interest.\n   These strange shapes tend to be extremely useful in practice. For example, it is straightforward to create Bayesian versions of “L1” and “L2” by combining parameters with particular densities. Topic for a future post maybe.↩\n  Or see the published version.↩\n  You’ve probably seen this phenomenon previously in our stats education. If you keep adding and subtracting more uniform variables, we would approach a Normal distribution.↩\n   ","date":1593475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593475200,"objectID":"d7725ebd4648fa2db22ecb4c73ece185","permalink":"/post/nonflat-implications/","publishdate":"2020-06-30T00:00:00Z","relpermalink":"/post/nonflat-implications/","section":"post","summary":"Understanding the \"implied prior\" for functions of parameters","tags":["Computational Methods","Bayesian Statistics"],"title":"Non-Flat Implications of Flat Priors","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Motivation R 4.0 is here, and that’s great. Be warned that if you install it, you will lose all of your installed packages. How should you reinstall them?\nFirst, there’s something to be said for installing as you go rather than inheriting all of your packages from your past self. A major upgrade can be a good excuse to clean house, trim unnecessary stuff from your computer, and install packages from scratch. As much as I like to take this approach myself, I have to teach next week, so I want my R environment established quickly.1\nIf you don’t particularly want to install-as-you-go, one way to revitalize your R environment is to record which packages you have installed and automate their re-installation with some code. You can find several helpful online guides that walk through the main idea: save the names of installed packages as a vector, and then pass these names to install.packages() to do a batch installation from CRAN.\nBut what about the packages that don’t live on CRAN? My experiments with R have led me to install several packages from Github: maybe they were “development versions” that had yet to be published on CRAN, or they will never be on CRAN due to policy incompatibilities or the wishes of the package developer. (See Reason #5 in this tweet.) How can we automate the reinstall process when these packages have different online sources? This post walks through a process for doing that, adapted from this Gist that I shared on Twitter. I adapted the whole thing to go into a little Github repository if you’d like to fork/clone that instead.\n Roadmap The routine has the following main ideas.\nWhat packages have I installed, and where did they come from? Check my local package versions against the CRAN versions. Are the most recent versions on CRAN, or was I using a Github version that I should keep using? Render unto Caesar: install the packages from CRAN that makes sense to get from CRAN, but install the packages from Github that makes sense to get from Github.   What is already installed? First we collect information on the packages we already have installed. I am working out of a project directory that manages all of my R updating business, so I use the {here} package consistent with a project-oriented R workflow. We will also use tidyverse-style data manipulation.\nlibrary(\u0026quot;here\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 4.0.2 The installed.packages() function returns a table of package information for all packages in your library. I convert this to a tibble to make things easier.\n# data frame of all installed packages local_pkgs \u0026lt;- installed.packages() %\u0026gt;% as_tibble() %\u0026gt;% print() ## # A tibble: 392 x 16 ## Package LibPath Version Priority Depends Imports LinkingTo Suggests Enhances ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 abind /Libra… 1.4-5 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… method… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 2 adagio /Libra… 0.7.1 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… graphi… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 ade4 /Libra… 1.7-15 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… graphi… \u0026lt;NA\u0026gt; \u0026quot;ade4Tk… \u0026lt;NA\u0026gt; ## 4 AER /Libra… 1.2-9 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… stats,… \u0026lt;NA\u0026gt; \u0026quot;boot, … \u0026lt;NA\u0026gt; ## 5 Amelia /Libra… 1.7.6 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… foreig… Rcpp (\u0026gt;=… \u0026quot;tcltk,… \u0026lt;NA\u0026gt; ## 6 arrayh… /Libra… 1.1-0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; method… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 askpass /Libra… 1.1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; sys (\u0026gt;… \u0026lt;NA\u0026gt; \u0026quot;testth… \u0026lt;NA\u0026gt; ## 8 assert… /Libra… 0.2.1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; tools \u0026lt;NA\u0026gt; \u0026quot;testth… \u0026lt;NA\u0026gt; ## 9 audio /Libra… 0.1-7 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 backpo… /Libra… 1.1.7 \u0026lt;NA\u0026gt; \u0026quot;R (\u0026gt;=… utils \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # … with 382 more rows, and 7 more variables: License \u0026lt;chr\u0026gt;, ## # License_is_FOSS \u0026lt;chr\u0026gt;, License_restricts_use \u0026lt;chr\u0026gt;, OS_type \u0026lt;chr\u0026gt;, ## # MD5sum \u0026lt;chr\u0026gt;, NeedsCompilation \u0026lt;chr\u0026gt;, Built \u0026lt;chr\u0026gt; This is a start, but I also want to know if I got these packages from CRAN or from Github. I can do this with sessioninfo::package_info(), passing a vector of package names to the function.\n# get source details (cran, github...) from package_info() local_details \u0026lt;- sessioninfo::package_info(pkgs = local_pkgs$Package) %\u0026gt;% as_tibble() %\u0026gt;% select(package, local_version = ondiskversion, source) %\u0026gt;% print() ## # A tibble: 378 x 3 ## package local_version source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 abind 1.4-5 CRAN (R 4.0.0) ## 2 adagio 0.7.1 CRAN (R 4.0.0) ## 3 ade4 1.7-15 CRAN (R 4.0.0) ## 4 AER 1.2-9 CRAN (R 4.0.0) ## 5 Amelia 1.7.6 CRAN (R 4.0.0) ## 6 arrayhelpers 1.1-0 CRAN (R 4.0.0) ## 7 askpass 1.1 CRAN (R 4.0.0) ## 8 assertthat 0.2.1 CRAN (R 4.0.0) ## 9 audio 0.1-7 CRAN (R 4.0.0) ## 10 backports 1.1.7 CRAN (R 4.0.0) ## # … with 368 more rows Notice that this new table has 14 fewer rows. That’s because sessioninfo::package_info() isn’t returning the base packages that show up in installed.packages(). That’s fine, since those will come with R 4.0 anyway.\nThe source column in this new table shows us what we want to know. For instance, let’s look at packages that I have from Github.\nfilter(local_details, str_detect(source, \u0026quot;Github\u0026quot;)) ## # A tibble: 8 x 3 ## package local_version source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 colorout 1.2-2 Github (jalvesaq/colorout@726d681) ## 2 emo 0.0.0.9000 Github (hadley/emo@3f03b11) ## 3 ggkeyboard 0.0.0.9009 Github (sharlagelfand/ggkeyboard@b1a965d) ## 4 mRkov 0.0.0.9000 Github (serrat839/mRkov@0f520e8) ## 5 rethinking 2.00 Github (rmcelreath/rethinking@f393f30) ## 6 Statamarkdown 0.4.5 Github (hemken/Statamarkdown@506cfc9) ## 7 texreg 1.36.28 Github (leifeld/texreg@c1da5c8) ## 8 waffle 1.0.1 Github (hrbrmstr/waffle@3f61463)  Determining install source by comparing package versions Before you update R, you may be using packages installed from Github, even if those packages are also on CRAN. We want to compare our locally installed package versions against the versions on CRAN. If the CRAN versions are more recent, we can go ahead and get those packages from CRAN. If the Github versions are still the most recent (or the only) versions of some packages, we want to get them from Github.\nWe will want to get a table of data on CRAN package versions. The available.packages() function returns info for all packages on CRAN.\n# available.packages() returns pkg info for ALL pkgs on CRAN. cran_pkgs \u0026lt;- available.packages() %\u0026gt;% as_tibble(.name_repair = tolower) %\u0026gt;% print() ## # A tibble: 16,578 x 17 ## package version priority depends imports linkingto suggests enhances license ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 A3 1.0.0 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; randomF… \u0026lt;NA\u0026gt; GPL (\u0026gt;… ## 2 aaSEA 1.1.0 \u0026lt;NA\u0026gt; R(\u0026gt;= 3… \u0026quot;DT(\u0026gt;=… \u0026lt;NA\u0026gt; knitr, … \u0026lt;NA\u0026gt; GPL-3 ## 3 AATtoo… 0.0.1 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026quot;magri… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; GPL-3 ## 4 ABACUS 1.0.0 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026quot;ggplo… \u0026lt;NA\u0026gt; rmarkdo… \u0026lt;NA\u0026gt; GPL-3 ## 5 abbyyR 0.5.5 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026quot;httr,… \u0026lt;NA\u0026gt; testtha… \u0026lt;NA\u0026gt; MIT + … ## 6 abc 2.1 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; GPL (\u0026gt;… ## 7 abc.da… 1.0 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; GPL (\u0026gt;… ## 8 ABC.RAP 0.9.0 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026quot;graph… \u0026lt;NA\u0026gt; knitr, … \u0026lt;NA\u0026gt; GPL-3 ## 9 abcADM 1.0 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026quot;Rcpp … Rcpp, BH \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; GPL-3 ## 10 ABCana… 1.2.1 \u0026lt;NA\u0026gt; R (\u0026gt;= … \u0026quot;plotr… \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; GPL-3 ## # … with 16,568 more rows, and 8 more variables: license_is_foss \u0026lt;chr\u0026gt;, ## # license_restricts_use \u0026lt;chr\u0026gt;, os_type \u0026lt;chr\u0026gt;, archs \u0026lt;chr\u0026gt;, md5sum \u0026lt;chr\u0026gt;, ## # needscompilation \u0026lt;chr\u0026gt;, file \u0026lt;chr\u0026gt;, repository \u0026lt;chr\u0026gt; We only care about the packages in this table that we have already installed, so we will narrow the table down using a join.\nslimmer_frame \u0026lt;- left_join( x = select(local_details, package, local_version, source), y = select(cran_pkgs, package, cran_version = version) ) %\u0026gt;% print() ## # A tibble: 378 x 4 ## package local_version source cran_version ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 abind 1.4-5 CRAN (R 4.0.0) 1.4-5 ## 2 adagio 0.7.1 CRAN (R 4.0.0) 0.7.1 ## 3 ade4 1.7-15 CRAN (R 4.0.0) 1.7-16 ## 4 AER 1.2-9 CRAN (R 4.0.0) 1.2-9 ## 5 Amelia 1.7.6 CRAN (R 4.0.0) 1.7.6 ## 6 arrayhelpers 1.1-0 CRAN (R 4.0.0) 1.1-0 ## 7 askpass 1.1 CRAN (R 4.0.0) 1.1 ## 8 assertthat 0.2.1 CRAN (R 4.0.0) 0.2.1 ## 9 audio 0.1-7 CRAN (R 4.0.0) 0.1-7 ## 10 backports 1.1.7 CRAN (R 4.0.0) 1.2.0 ## # … with 368 more rows Using this slimmer table, we categorize the sources of these packages and where we want to reinstall them from. Here is the basic idea: We want to install from Github only if our local Github version is more recent than the CRAN version. This also applies when there is no version of a package on CRAN.\nStated another way, we install a package from CRAN in any case that the CRAN version is more recent than the local version. This is true even if the local version was installed from Github! Remember, we don’t install from Github simply because we did so in the past. We install from Github if there is no better choice.\nThere are edge cases to be aware of: we may find that the CRAN version of a package is behind our local version, even if the local version was installed from CRAN. This happens for (at least) two reasons: if a package version was reverted on CRAN (which appeared to happen in the case of StanHeaders), or if the package is currently unavailable for installation from CRAN (due to some incompatibility, perhaps).\nThe code below does this categorization using the utils::compareVersion() function, which interprets the version numbers so we don’t have to. We do an additional step to note the Github repostory for any package that we still want to obtain from Github.\ncompare_frame \u0026lt;- slimmer_frame %\u0026gt;% group_by(package) %\u0026gt;% mutate( source_locale = case_when( compareVersion(local_version, cran_version) == 1 \u0026amp; str_detect(source, \u0026quot;Github\u0026quot;) ~ \u0026quot;Github\u0026quot;, compareVersion(local_version, cran_version) == 1 \u0026amp; is.na(cran_version) \u0026amp; str_detect(source, \u0026quot;CRAN\u0026quot;) ~ \u0026quot;Unavailable on CRAN\u0026quot;, compareVersion(local_version, cran_version) == 1 \u0026amp; (is.na(cran_version) == FALSE) \u0026amp; str_detect(source, \u0026quot;CRAN\u0026quot;) ~ \u0026quot;Downgraded on CRAN\u0026quot;, compareVersion(local_version, cran_version) %in% c(-1, 0) ~ \u0026quot;CRAN\u0026quot; ), github_repo = case_when( source_locale == \u0026quot;Github\u0026quot; ~ str_split(string = source, pattern = \u0026quot;@\u0026quot;, simplify = TRUE)[,1] %\u0026gt;% str_replace(\u0026quot;Github \\\\(\u0026quot;, \u0026quot;\u0026quot;), TRUE ~ as.character(NA) ), ) %\u0026gt;% ungroup() %\u0026gt;% print() ## # A tibble: 378 x 6 ## package local_version source cran_version source_locale github_repo ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 abind 1.4-5 CRAN (R 4.0… 1.4-5 CRAN \u0026lt;NA\u0026gt; ## 2 adagio 0.7.1 CRAN (R 4.0… 0.7.1 CRAN \u0026lt;NA\u0026gt; ## 3 ade4 1.7-15 CRAN (R 4.0… 1.7-16 CRAN \u0026lt;NA\u0026gt; ## 4 AER 1.2-9 CRAN (R 4.0… 1.2-9 CRAN \u0026lt;NA\u0026gt; ## 5 Amelia 1.7.6 CRAN (R 4.0… 1.7.6 CRAN \u0026lt;NA\u0026gt; ## 6 arrayhelpe… 1.1-0 CRAN (R 4.0… 1.1-0 CRAN \u0026lt;NA\u0026gt; ## 7 askpass 1.1 CRAN (R 4.0… 1.1 CRAN \u0026lt;NA\u0026gt; ## 8 assertthat 0.2.1 CRAN (R 4.0… 0.2.1 CRAN \u0026lt;NA\u0026gt; ## 9 audio 0.1-7 CRAN (R 4.0… 0.1-7 CRAN \u0026lt;NA\u0026gt; ## 10 backports 1.1.7 CRAN (R 4.0… 1.2.0 CRAN \u0026lt;NA\u0026gt; ## # … with 368 more rows  Updating R and reinstalling packages When we are satisfied with our decisions about where to install a package from, save this comparison table to file.\n# output data location dir.create(here(\u0026quot;data\u0026quot;)) # output file out_file \u0026lt;- as.character(str_glue(\u0026quot;pkg-data_{Sys.Date()}.rds\u0026quot;)) write_rds(compare_frame, here(\u0026quot;data\u0026quot;, out_file)) After updating, we reopen R and install our packages according to our classifications. We want to begin by installing {remotes} to enable installation from Github.\n# to install from github install.packages(\u0026quot;remotes\u0026quot;) # should still be operating in your working directory # so downloading {here} makes sense also install.packages(\u0026quot;here\u0026quot;) # read package data pkgs \u0026lt;- readRDS(here::here(\u0026quot;data\u0026quot;, \u0026quot;pkg-data_2020-04-25.rds\u0026quot;)) Everything that we want to install from Github, we install by iterating remotes::install_github over the github repository slugs that we saved previously. This requires us to write the code in “hard mode” because we aren’t using tidyverse dialect.\n# install from github repos github_pkgs \u0026lt;- pkgs[pkgs$source_locale == \u0026quot;Github\u0026quot;, ][[\u0026quot;github_repo\u0026quot;]] remotes::install_github(github_pkgs) Finally, everything that we said we wouldn’t get from Github, we get by using plain ol’ install.packages(). Before doing this, you may find it beneficial to filter out some of the packages that you don’t use anymore or that maybe we only installed as dependencies for other packages.\n# install from cran with remaining package names cran_pkgs \u0026lt;- pkgs[pkgs$source_locale != \u0026quot;Github\u0026quot;, ][[\u0026quot;package\u0026quot;]] install.packages(cran_pkgs) This should get you (more or less) up and running with R 4.0 and all of your old packages.\nFair warning: I’m already getting burned by some C++ configuration problems for packages that want to compile from source. I think this is particular to my own computer and the klugey fixes I undertook to set up {rstan} with MacOS Catalina. I remember reading somewhere that R 4.0 fixed some of the Stan x Catalina problems, so maybe I will confront these choices again soon, but I will cross that bridge when I get to it.\n   And maybe I will do a fresh install of everything once the semester is over.↩\n   ","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587859200,"objectID":"4d1396bdcf8fe8cecb5da6dc9d10df02","permalink":"/post/package-reinstall/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/package-reinstall/","section":"post","summary":"A quick solution to an R upgrade headache","tags":["Methods","Computational Methods","R"],"title":"How do I reinstall my packages for R 4.0 when many of them came from Github?","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" This is a post about temptation Or, resisting temptations when presenting statistical results.\nWhen you build a model to answer a question, it is often tempting to make a graphic about the coolest thing about the model. Maybe you learned something new to build the model, or you noticed and corrected an important structure in the data, so naturally you want to show off your good work. The purpose of this post is to reflect on why this practice isn’t useful for communicating statistical results. Instead, we should be communicating the information that will help the audience grasp the important takeaways of the analysis. A different focus entirely.\nThis is almost too obvious, but it is easier said than done. Researchers wrestle with it in different ways based on our audiences, our professional goals, and (to be honest) our insecurities. Speaking for myself, I need to grapple with my biases and how they manifest in my work product. As a PhD student, I sometimes feel like academia cultivates incentives to convince our colleagues that we are Very SmartTM, which is a distinct goal from doing good work (however defined). Cool graphics can be a way to show how much thought and work we put into something—a way of signaling that we belong. Understandable, but not always useful. I also like to use Bayesian methods, but I feel constant pressure to justify the Bayesian approach to audiences that I (rightly or wrongly) assume will be hostile to that choice. As a result, I feel tempted to plot something that would be impossible without Bayes—a way of saying, “Get off my back!” as if it ultimately mattered for what I’m trying to communicate with my analysis overall. Sometimes it does matter, but the way that it matters won’t be so simple as “just plot the flashy thing.”\nThis post unpacks this using a recent example from a grad methods course that I am TA’ing. The assignment requires students to write a policy memo informed by a statistical analysis. The statistical model contains an important component that students are learning about in the course, but that component isn’t actually important to communicate in the policy memo. So the assignment challenges students both on model-building but also communication skills: exercising restraint and judgment about what is and is not important to communicate about the details of the analysis.\n The setting The assignment for students to complete lays out the following scenario.\n You are an advisor to a newly elected mayor of Smallville. During the campaign, the mayor-elect charged that the Sanitation Department was being grossly mismanaged. Last year it cost \\(\\$ 48.50\\) per household for once-a-week curbside waste pick-up. A private contractor has made an informal bid of \\(\\$ 40.60\\) per household for collection services, but this would require eliminating Sanitation Department jobs, which would be difficult and politically costly. Before switching to private contracting, the mayor would like to know how much costs might be reduced with the appointment of a more competent Sanitation Department supervisor.\nPrepare a memorandum to the mayor advising her about the potential gains from better management. The mayor has had little statistical training, so be sure to explain your empirical work clearly and carefully.\n Students are given a dataset of 30 other municipalities in the region, simulated from a model that they don’t directly see.\n## # A tibble: 30 x 5 ## hholds density wage snowdays cost_per_household ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3.35 565. 18.2 3 29.4 ## 2 11.2 740. 15.7 10 52.0 ## 3 9.48 540. 17.4 3 34.4 ## 4 9.43 629. 19.6 1 41.5 ## 5 11.3 685. 20.6 6 63.4 ## 6 6.18 605. 20.2 6 30.0 ## 7 2.82 510. 16.5 4 20.8 ## 8 2.95 459. 15.8 2 12.9 ## 9 6.98 507. 16.5 3 21.8 ## 10 7.89 524. 19.9 2 37.8 ## # … with 20 more rows The variables are described as follows.\n  cost_per_household: cost per household (\\(\\$\\) U.S.) of once weekly curbside refuse pickup for last year. hholds: number of households served in 10,000s. (Note: previous studies suggest refuse collection may involve non-constant returns to scale; that is, there may be some number of households at which the cost per household is minimized; communities with smaller or larger numbers of households have higher costs per household.) Value for your city: 6.28. density: density of households per square mile. Value for your city: 620. wage: average hourly wage in dollars for collection workers. Value for your city: 19.50. snowdays: number of snow emergency days last year; may raise costs by interfering with regular schedule. Value for your city: 5.   So what’s the objective? Students are supposed to use the data to build a predictive (OLS) model for cost_per_household, and then interpret the model to advise the mayor about the choice between (a) enlisting the private sanitation contractor or (b) replacing the supervisor of the Sanitation Department.\n Source of temptation Students typically begin with a simple model where every variable is linearly related to the outcome variable… \\[\\begin{align} \\mathtt{cost\\_per\\_household}_{i} \u0026amp;= \\alpha + \\beta_{1} \\mathtt{hholds} + \\beta_{2} \\mathtt{density} \\\\ \u0026amp; \\quad + \\beta_{3} \\mathtt{wage} + \\beta_{4} \\mathtt{snowdays} + \\varepsilon_{i} \\end{align}\\] …with a normally distributed error. But they should find that the simple model violates OLS assumptions by producing residuals with a curvilinear pattern. If students inspect the data more, they detect that the likely culprit is the hholds variable, the number of households in the municipality. This is consistent with the hint in the variable descriptions above, that there may be some number of households that minimize sanitation costs per household. So they build a model with a quadratic term for hholds. \\[\\begin{align} \\mathtt{cost\\_per\\_household}_{i} \u0026amp;= \\alpha + \\beta_{1} \\mathtt{hholds} + \\beta_{2} \\mathtt{hholds}^{2} + \\beta_{3} \\mathtt{density} \\\\ \u0026amp; \\quad + \\beta_{4} \\mathtt{wage} + \\beta_{5} \\mathtt{snowdays} + \\varepsilon_{i} \\end{align}\\]\nFor the purposes of the assignment, this is the “correct” model, and the residuals look better than they did before (see below).\nNow that we have the appropriate model, how do we visualize it? I’ll tell you what I did when I was a student in this course (many years ago): I plotted the nonlinear relationship between the number of households and the outcome variable. It the most interesting part of the model, and it was hidden in the data…how could it not be the important thing that I should focus on? I think I did something like this: generate model predictions with other variables fixed at their means, and then plot those model predictions alongside my city’s data (Smallville) and the private contractor’s proposal for reducing the curbside pickup costs per household.\nFrom this graphic, we could reason that even if we enlist the private contractor, their bid does not get us close to what we would expect from the model. This leads us the direction of saying that maybe we expect more savings from better mismanagement of the Sanitation Department than what the current supervisor is delivering.\n Disciplined plotting choices There are a few things that are misguided about the above approach.\nFirst, the comparison in this above graphic isn’t actually the relevant comparison. It is typical to teach students to visualize partial relationships in regression by varying one predictor and fixing other predictors to their means. But we don’t always want to compare against a typical observation. In this example, we would be more interested in holding covariates to the same values as we observe for our city. Applied situations like this remind us that many of the problems that we encounter aren’t questions about “typical” observations at all.\nSecond, and more importantly, the only reason why we make the previous mistake is because we think that the quadratic relationship is the important thing to visualize. It’s understandable that we’re distracted by the quadratic relationship because it was initially a challenge to discover, but if we discipline ourselves about what is important to communicate to our audience (in this case, the mayor of “Smallville”), we would see that the nonlinearity is irrelevant to visualize. All that matters is comparing our city’s data and the contractor’s proposal to a specific model-based prediction for our city. Exploring how the prediction changes as we arbitrarily assign other values on key covariates doesn’t help us make a policy recommendation. We aren’t in a position to intervene on those variables (and we aren’t confident that our model identifies causal effects anyway), so we shouldn’t distract the mayor by presenting irrelevant information that draws focus away from the key insights.1\nWhat is a simpler way to show the model’s key takeaway? Here.\nThe takeaway from this figure is similar as it was above. We show a model-based prediction for a city with the same observable characteristics as Smallville (a point estimate, 95% confidence interval, and 95% prediction interval) alongside Smallville’s current costs and the contractor’s proposal.2 We see that the contractor’s proposal reduces sanitation costs, but they still get us nowhere near a level that we should expect given the characteristics of our city. If we are confident that replacing the Sanitation Department supervisor would make our costs “representative” of other similar towns, we would save a lot more money by replacing the supervisor than we would by hiring the contractor.\n We can do better: visualize cost-savings directly Even though the above graphic does a little better than what we were working with before, it still doesn’t directly communicate the aggregate financial impact of the policy. The audience has to do that work on their own still. If we really wanted to communicate the insights of the model, we could translate these predictions directly into something that mayors, comptrollers, and so on really understand: dollars.\nHere’s the idea. In terms of annual cost cost-per-household, we know our city’s current value, the contractor’s bid, and a distribution of model-based estimates for a city with our data. We also know the total number of households in our city, so it is straightforward to calculate the total costs from each of these per-household figures: \\[\\begin{align} \\text{Total annual cost} \u0026amp;= \\text{Cost per household} \\times \\text{Number of households} \\end{align}\\]\nSo we calculate annual costs for the current per-household rate, the contractor’s bid, and three model-based scenarios3 that result from replacing the Sanitation Dept. supervisor:\n An “average” scenario: plugging in the point estimate from the model as the annual cost per household under a new supervisor. An “optimistic” scenario: plugging in the 10th percentile of the predictive distribution as the estimated cost per household. A “pessimistic” scenario: plugging in the 90th percentile of the predictive distribution as the annual cost per household.  We plot two quantities for each scenario. First, how much money is saved annually by replacing the supervisor brings pickup costs to each of these benchmarks? And second, how much more does each benchmark save us when we compare to the private contractor’s bid?\nThe comparison against the contractor bid isn’t just for show! It gives decision-makers valuable information because it represents a “budget” for replacing the supervisor. How so? Replacing the supervisor wouldn’t be costless: the replacement may require a higher salary than the current supervisor, and the job search will involve some fixed costs. Comparing each scenario’s savings to the amount saved from the enlisting the contractor conveys how much money we can invest in a new supervisor while still saving more money than hiring the contractor. Visualizing the results this way shows exactly how much money we’re working with and how much we stand to save by hiring supervisors with different salary expectations.\nBy doing some further processing of the model’s insights, and in-turn moving farther from the technical details of the model, we actually learn more about the consequences of our choices. We see that even under the pessimistic cost-savings scenario, we still have a roughly half-million dollar cushion before the decision to replace the supervisor starts to look like the wrong choice.\n Reframing what to be “proud” of We feel proud when we build a technologically complex model. It makes us feel valuable when we work through something challenging, so we want to show it off. When we need to hide these technical details in order to communicate the results to non-experts, the choice is painful at first, especially for someone like me who takes a lot of pride in the time and effort that I invest in improving my statistical skills. This is a psychological game that we are playing with ourselves.\nBut the “difficulty-level” of our work isn’t the only skill to take pride in. Distilling essential information out of a complicated piece of machinery is valuable, and we can see it as a distinct source of pride when we do it well. Better still, doing an effective job summarizing the important takeaways of an analysis makes it all the more rewarding to describe the technical backend, since the technical backend has a much stronger clarity-of-purpose after if is introduced effectively.\nJust to drive the point home, that last graph was a bar graph. I kinda hate bar graphs. But I don’t hate the fact that I recognized a context where it made something simpler to communicate. That’s valuable.\n   At this point you may wonder if we can intervene on the system by reducing the wages of sanitation workers to make up the cost difference. First of all, how dare you. Secondly, even though we know that lowering wages should have a causal effect on pickup costs, we don’t know that the coefficient for wage in the model represents the causal effect of wages—certainly it doesn’t. Third, even if we could make that assumption, the numerical impact of decreasing sanitation worker wages to (say) the median wage for other cities would amount to only $5.54 saved per household, so we save even less than we would save from hiring the contractor.↩\n  The language used in the graphic (“95% range of predictions”) is doing a little violence against the meaning of a frequentist confidence interval. If I were implementing this analysis in real life, I would build a Bayesian model that lets me say, “This is the likely range of scenarios, as the data suggest,” because that’s conveniently what a posterior distribution actually means!↩\n  Once again, interpreting the prediction interval as “possible scenarios” is more closely tied to Bayes than to a frequentist model. The more I think about the value of communicating model predictions as “possible scenarios,” the more I think this warrants its own blog post. ↩\n   ","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"70482ae312cc37fcef2cb91b4e603abb","permalink":"/post/visualizing-what-matters/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/post/visualizing-what-matters/","section":"post","summary":"An exercise in communicating statistical results","tags":["Visualization","R","Computational Methods"],"title":"Plotting What Matters","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Overview Permutation testing (or “randomization inference,” RI) is an approach to statistical inference based on hypothetical allocations of treatment rather than sampling error. It has an appealing intuition, but researchers may feel intimidated by the prospect of implementing it computationally. Although helpful packages exist for conducting permutation tests in R,1 my goal for this post is to show that tidyverse tools (in particular, functional programming with purrr) make it easy to implement randomization inference in R flexibly and efficiently.\nBelow I provide a brief overview of RI’s logic, implement a simple permutation test for a single treatment, and then demonstrate how to incorporate additional design features such as blocking or clustered randomization into the workflow. We will see that many of these design features are easily translated into tidyverse dialect without warping or overextending the workflow into something unrecognizable or impractical.\n What is Randomization Inference Randomization inference is a method for conducting statistical hypothesis tests without making distributional assumptions about test statistics. Like all null hypothesis tests, it is a comparison between (1) an estimated effect or relationship from observed data and (2) a “null distribution” of estimates that we might have observed if the null hypothesis were true. Unlike traditional null hypothesis tests, however, the null distribution is not based on repeated sampling of a hypothetical population. Instead, it is constructed by imagining other ways that treatment may have been randomly allocated among units.\nFor instance, suppose we had some small sample of data for \\(n = 6\\) units. We randomly assign half of the units to treatment and half to control:\n    ID Treatment Outcome    1 0 \\(Y_1\\)  2 0 \\(Y_2\\)  3 1 \\(Y_3\\)  4 1 \\(Y_4\\)  5 1 \\(Y_5\\)  6 0 \\(Y_6\\)     We could use this data to calculate some difference in means \\(\\hat{\\delta} = \\bar{Y}_{1} - \\bar{Y}_{0}\\). How do we form an inference about \\(\\delta\\) from \\(\\hat{\\delta}\\)? Typically we would standardize \\(\\hat{\\delta}\\) and compare it against a \\(t\\)-distribution, finding a \\(p\\)-value that represents “the probability of obtaining some other \\(\\hat{\\delta}\u0026#39;\\) at least as extreme as \\(\\hat{\\delta}\\), under the null hypothesis that the true \\(\\delta\\) is zero.” Under this approach, this \\(t\\)-distribution serves as our “null distribution,” the sampling distribution of estimates that we would obtain assuming that the null is true.\nRandomization inference constructs this null distribution using a different method. Instead of making the parametric assumption that the standardized \\(\\hat{\\delta}\\) follows a \\(t\\) distribution, we take advantage of random assignment in the research design, constructing a null distribution of treatment effect estimates by repeatedly permuting the treatment assignments for our units. For example, the treatment vector in the small sample of data above was \\(\\mathbf{T} = \\left[0, 0, 1, 1, 1, 0\\right]\\), but we could have obtained a different treatment vector by random chance alone. We could have randomly realized treatment assignments as \\(\\mathbf{T}\u0026#39; = \\left[0, 0, 1, 0, 1, 1\\right]\\), or \\(\\mathbf{T}\u0026#39; = \\left[1, 1, 1, 0, 0, 0\\right]\\). Imagine that in each of these alternate treatment assignments, we estimate the treatment effect \\(\\hat{\\delta}\\). Under the null hypothesis, outcomes are not affected (on average) by treatment assignment, so each \\(\\hat{\\delta}\\) that we estimate under the permuted treatment assignments a draw from the “null distribution” of \\(\\hat{\\delta}\\). If we could specify every possible treatment assignment, we would have a full accounting of every equally-likely null \\(\\hat{\\delta}\\) estimate under the research design. This would let us calculate an “exact” \\(p\\)-value by comparing our in-sample estimate to the null distribution: the proportion of null estimates that are more extreme than the estimate obtained in our real data. These \\(p\\)-values are valid even in nonrandom samples because they operate by design uncertainty instead of sampling uncertainty.\nMore formally, suppose that random variable \\(S\\) is a test statistic that is a function of \\(\\mathbf{T}\\) and \\(\\mathbf{Y}\\), vectors of treatment assignments and outcome data respectively (Keele, McConnaughy, and White). A \\(p\\)-value is the probability that \\(S\\) exceeds the test statistic calculated from your data, \\(s^{*}\\), given the null distribution of \\(S\\). \\[\\begin{align} p \u0026amp;= \\mathrm{Pr}\\left(S ≥ s^{*} | H_{0}\\right) %* \\tag{1} \\end{align}\\] A permutation test finds this \\(p\\)-value by calculating the test statistic for your sample of data, \\(s^{*}\\), as well as test statistics \\(s_{m}\\) for \\(m \\in \\mathcal{M}\\), treatment permutations from the set of possible treatment permutations. The \\(p\\)-value in the permutation test is the proportion of permuted test statistics that exceed the in-sample test statistic. \\[\\begin{align} p \u0026amp;= \\frac{\\sum\\limits_{m = 1}^{M} I(s_{m} \u0026gt; s^{*})}{M} \\tag{2} \\end{align}\\] where \\(I(\\cdot)\\) is the indicator function and \\(M\\) is the number of permutations being considered. If \\(M\\) is equal to the total number of possible permutations, this \\(p\\)-value is exact. If the set of permutations \\(\\mathcal{M}\\) is too large to calculate every \\(s_{m}\\) (which is likely for even moderate samples sizes), sampling a subset of \\(M \u0026lt; |\\mathcal{M}|\\) permutations lets you treat Equation (2) as an approximation of the \\(p\\)-value.\nTLDR, how do we perform randomization inference?\nEstimate the treatment effect in our sample Compute or approximate the null distribution of the treatment effect estimate under repeated reallocations of treatment to units (conditional on the sample) Compare the in-sample estimate to its null distribution   Randomization Inference for a Simple Experiment This section shows how to implement permutation tests for a simple experiment with a binary treatment and binary outcome. First I describe the model used to generate data, and then we perform a permutation test using the generated data.\nData We will use various tools in the tidyverse, so for now that is the only package we will load.\nlibrary(\u0026quot;tidyverse\u0026quot;) I simulate a dataset of \\(n = 500\\) units with outcome variable \\(y_{i} \\in \\left\\{0, 1\\right\\}\\), treatment status \\(z_{i} \\in \\left\\{0, 1\\right\\}\\), treatment probability \\(0.5\\). The generative model can be represented as follows: \\[\\begin{align} y_{i} \u0026amp;\\sim \\mathrm{Bernoulli}\\left(0.5 + \\delta z_{i}\\right) \\\\ z_{i} \u0026amp;\\sim \\mathrm{Bernoulli}\\left(0.5 \\right) \\end{align}\\] where the treatment effect \\(\\delta\\) has a true value of \\(0.06\\). In R, I generate the data like so:\nset.seed(90181) n_obs \u0026lt;- 500 control_mean \u0026lt;- 0.5 treat_effect \u0026lt;- .06 d \u0026lt;- tibble(id = 1:n_obs) %\u0026gt;% mutate( treatment = rbernoulli(n = n(), p = 0.5) %\u0026gt;% as.numeric(), y = rbernoulli(n = n(), p = control_mean + (treatment * treat_effect)) %\u0026gt;% as.numeric() ) %\u0026gt;% print() ## # A tibble: 500 x 3 ## id treatment y ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 1 ## 2 2 0 0 ## 3 3 0 0 ## 4 4 1 1 ## 5 5 1 1 ## 6 6 1 1 ## 7 7 0 0 ## 8 8 0 1 ## 9 9 0 1 ## 10 10 0 1 ## # … with 490 more rows The data are a random draw from the generative model, so the treatment effect in the data won’t be exactly equal to the true effect. In these data, we estimate an in-sample treatment effect of about 0.05. We will want to save a data frame of in-sample estimates.\nobserved_estimates \u0026lt;- d %\u0026gt;% summarize( mean_control = mean(y[treatment == 0]), mean_treatment = mean(y[treatment == 1]), diff_means = mean_treatment - mean_control ) %\u0026gt;% print() ## # A tibble: 1 x 3 ## mean_control mean_treatment diff_means ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.514 0.563 0.0495  Permuting treatment assignments, tidily The tidyverse advantage comes in when we permute the treatment assignments. Ordinarily we might think about permuting treatment assignments using a for loop—for each iteration m in 1:M, reshuffle the treatment assignments and calculate \\(\\hat{\\delta}_{\\mathtt{m}}\\). Except we want to avoid a for loop because…\nFor loops in R are famously slow. For loops in R are rarely necessary. For many common data wrangling problems, iteration \\(m\\) does not depend on iteration \\(m-1\\), so a vectorized approach is usually more efficient. A technical but important workflow point: the data structure that results from a loop (usually a vector or a list) often throws away useful information, and it can be organizationally idiosyncratic if the rest of our work is data frame-driven. If we can do this routine using a more capable data structure and extensible workflow, we have the potential to do more cool things while staying organized along the way.  We deal with points (1) and (2) by using functional programming: applying a function over groups of data (see here for instance). In our case, a group will be a data frame, and the function is permuting the treatment assignments and estimating the treatment effect. We address point (3) by doing everything with a nested data frame and applying functions to groups using purrr::map(). I will describe the approach below one step at a time. After each step is explained, I will show how simple it is to combine them into a small, efficient block of readable code.\nThe first step is to make a nested data frame: a data frame that itself contains data frames (see e.g. [1], [2], [3]). Here I create a data frame with two columns: an identifier m that indexes our treatment permutations, and a data column that contains copies of our original dataset for each m.\n# total number of iterations n_sims \u0026lt;- 2000 # nested data frame of all M iterations sim_table \u0026lt;- d %\u0026gt;% crossing(m = 1:n_sims) %\u0026gt;% group_by(m) %\u0026gt;% nest() %\u0026gt;% print() ## # A tibble: 2,000 x 2 ## # Groups: m [2,000] ## m data ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 3]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 3]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 3]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 3]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 3]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 3]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 3]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 3]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 3]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 3]\u0026gt; ## # … with 1,990 more rows The data column is a list column, in this case a list of data frames for each row m (but it could contain other object types if we wanted). List columns are powerful for large-scale data manipulation because we can apply functions over each element in the list, similar to the way apply() functions work in base R. In our case, we want to apply a function that permutes the values in the treatment variable. We map this function over the nested data using purrr::map(), creating a new list column of data frames.\nperm_table \u0026lt;- sim_table %\u0026gt;% mutate( permuted_data = map( .x = data, .f = mutate, treatment = sample(treatment) ) ) %\u0026gt;% print() ## # A tibble: 2,000 x 3 ## # Groups: m [2,000] ## m data permuted_data ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; ## # … with 1,990 more rows If you are new to purrr::map(), it needs to know two things: a vector or list column .x, and a function .f that is applied to each element in .x. In this case, .x is a list of data frames, and the function we apply is mutate(), which creates/modifies variables. We supply the additional function argument saying that we are overwriting the treatment variable by sampling its current values without replacement. This adds a list-column of 2,000 new data frames, each with a different permutation of treatment assignments. In case you need convincing, here is a table that glimpses the treatment assignments for a subset of m values.\nperm_table %\u0026gt;% ungroup() %\u0026gt;% sample_n(5) %\u0026gt;% unnest(permuted_data) %\u0026gt;% select(m, id, treatment) %\u0026gt;% pivot_wider( names_from = m, values_from = treatment, names_prefix = \u0026quot;trt, m = \u0026quot; ) ## # A tibble: 500 x 6 ## id `trt, m = 561` `trt, m = 1471` `trt, m = 1325` `trt, m = 844` ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0 0 0 1 ## 2 2 0 1 1 0 ## 3 3 1 0 1 1 ## 4 4 0 1 1 1 ## 5 5 1 0 1 1 ## 6 6 0 0 1 0 ## 7 7 0 0 1 1 ## 8 8 1 0 1 1 ## 9 9 1 1 0 1 ## 10 10 0 0 1 1 ## # … with 490 more rows, and 1 more variable: `trt, m = 943` \u0026lt;dbl\u0026gt; Now that we have datasets with permuted treatments, we calculate some test statistic in each iteration. I will calculate the difference between the treatment and control means, but other statistics are possible as well.2 We again do this with map(), this time applying the summarize() function to calculate the treatment mean, the control mean, and the difference.\nest_table \u0026lt;- perm_table %\u0026gt;% mutate( estimates = map( .x = permuted_data, .f = summarize, mean_treatment_m = mean(y[treatment == 1]), mean_control_m = mean(y[treatment == 0]), diff_means_m = mean_treatment_m - mean_control_m ) ) %\u0026gt;% print() ## # A tibble: 2,000 x 4 ## # Groups: m [2,000] ## m data permuted_data estimates ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## # … with 1,990 more rows We can view the estimates in each iteration by unnesting the estimates column…\nest_summary \u0026lt;- est_table %\u0026gt;% select(m, estimates) %\u0026gt;% unnest(cols = estimates) %\u0026gt;% ungroup() %\u0026gt;% print() ## # A tibble: 2,000 x 4 ## m mean_treatment_m mean_control_m diff_means_m ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.563 0.514 0.0495 ## 2 2 0.539 0.537 0.00152 ## 3 3 0.563 0.514 0.0495 ## 4 4 0.535 0.541 -0.00648 ## 5 5 0.551 0.525 0.0255 ## 6 6 0.567 0.510 0.0575 ## 7 7 0.490 0.584 -0.0945 ## 8 8 0.535 0.541 -0.00648 ## 9 9 0.518 0.557 -0.0385 ## 10 10 0.588 0.490 0.0976 ## # … with 1,990 more rows We can calculate our \\(p\\)-value by comparing our sample estimate to the null distribution…\npval \u0026lt;- est_summary %\u0026gt;% summarize( p.value = mean(abs(diff_means_m) \u0026gt; abs(observed_estimates$diff_means)) ) %\u0026gt;% pull(p.value) %\u0026gt;% print() ## [1] 0.2515 and we can plot our estimate against the null distribution. In our case, 25% of estimates from the null distribution exceed our in-sample estimate in magnitude, so we would not reject a null hypothesis in this setting.3\n Putting it all together In practice, we would not tediously create lots of objects to trace our method every step of the way. Instead, we could build our nested data frame, permute the treatment assignments, and estimate null treatment effects in one continuous operation.\n# nested data, permute, estimate. # bonus: permuted_data demos the \u0026quot;quosure-style lambda function\u0026quot; syntax ri_data \u0026lt;- d %\u0026gt;% crossing(m = 1:n_sims) %\u0026gt;% group_by(m) %\u0026gt;% nest() %\u0026gt;% mutate( permuted_data = map( .x = data, .f = ~ mutate(.x, treatment = sample(treatment)) ), estimates = map( .x = permuted_data, .f = summarize, mean_treatment_m = mean(y[treatment == 1]), mean_control_m = mean(y[treatment == 0]), diff_means_m = mean_treatment_m - mean_control_m ) ) %\u0026gt;% print() ## # A tibble: 2,000 x 4 ## # Groups: m [2,000] ## m data permuted_data estimates ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [500 × 3]\u0026gt; \u0026lt;tibble [1 × 3]\u0026gt; ## # … with 1,990 more rows   Extensibility of the Tidy Approach We’ve seen that using purrr::map() lets us push around a lot of data with little code. Yet the purrr approach is still more verbose than other approaches that we might have used (see Thomas Leeper’s use of replicate() and by() here). Why would we use the tidy approach if slimmer methods exist? For one, it is eminently extensible. By keeping the fruits of your work together in one data frame, it is easy to adapt the workflow to incorporate other quantities of interest and research design features while keeping your work organized. This section will take a quick tour of how easy it is to (1) calculate confidence intervals and permute data from (2) multiple treatments, (3) cluster randomization, and (4) block-randomization.\nConfidence Intervals So far we have entertained a generative model where (1) treatment has a constant, additive effect \\(\\delta\\) on unit-level response and (2) a null hypothesis that \\(\\delta = 0\\).4 If we entertain null hypothesis values for \\(\\delta\\) other than strictly zero, we can construct a \\(100 - \\alpha\\) confidence region as the values of \\(\\delta\\) for which we cannot reject the null hypothesis.\nWe do this as follows. For a null value of \\(\\delta_{0}\\):\n Adjust the in-sample test statistic by subtracting \\(\\delta_{0}\\) from the difference in means. This is similar to the way we would subtract the null value from a standard \\(z\\)-test \\(\\left(\\frac{\\hat{\\mu} - \\mu_{0}}{\\sigma}\\right)\\). Calculate the \\(p\\) value for this adjusted \\(\\hat{\\delta}\\). If \\(p \u0026lt; \\frac{\\alpha}{2}\\), we reject the null hypothesis that \\(\\delta = \\delta_0\\), meaning the value \\(\\delta_{0}\\) is not contained in the confidence interval.  We can implement this routine by augmenting our data structure with a sequence of null values and performing the appropriate adjustments.\n# for each m x delta_0, adjust the test statistic # for each delta_0, calculate p-value conf_table \u0026lt;- ri_data %\u0026gt;% select(m, estimates) %\u0026gt;% unnest(estimates) %\u0026gt;% crossing(null_value = seq(-0.2, 0.2, .01)) %\u0026gt;% mutate(test_value = observed_estimates$diff_means - null_value) %\u0026gt;% group_by(null_value) %\u0026gt;% summarize( p.value = mean(diff_means_m \u0026gt; abs(test_value)) ) %\u0026gt;% print() ## # A tibble: 41 x 2 ## null_value p.value ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -0.2 0 ## 2 -0.19 0 ## 3 -0.18 0 ## 4 -0.17 0 ## 5 -0.16 0 ## 6 -0.15 0 ## 7 -0.14 0 ## 8 -0.13 0 ## 9 -0.12 0 ## 10 -0.11 0 ## # … with 31 more rows We saw above that our point estimate for the difference in means is roughly 0.05, but now that we found the \\(p\\)-value under this sequence of hypothesis tests, we know the null values of \\(\\delta_0\\) that cannot be rejected, giving us a 93% interval [-0.04, 0.13] inclusive.5 Below I plot the \\(p\\)-value at each null \\(\\delta_{0}\\) as bars. Bars are colored red if they represent values of \\(\\delta_{0}\\) that can be rejected at \\(\\alpha = .05\\). What remains is the values that cannot be rejected, which I use to draw the confidence interval below the bars. It’s important to remember that this is not a plot of a posterior distribution, so the height of the bar does not indicate the plausibility or credibility of a given \\(\\delta\\) value.\n Multiple treatments Suppose that we had outcome data that were a function of multiple treatment conditions. Here is some fake data from an experiment where subjects rate potential candidates on a 0-10 scale, where the researcher manipulates the candidates party affiliation, occupation, and issue stances.\n## # A tibble: 500 x 7 ## id cand_rating party occupation issue_tax issue_abortion issue_environment ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 1 9 1 4 3 2 3 ## 2 2 2 2 2 1 3 3 ## 3 3 2 2 4 2 4 1 ## 4 4 7 2 3 2 3 3 ## 5 5 2 1 4 2 3 1 ## 6 6 5 2 2 1 2 2 ## 7 7 2 2 1 3 4 3 ## 8 8 8 2 3 3 2 3 ## 9 9 10 1 4 3 3 2 ## 10 10 6 1 2 2 1 2 ## # … with 490 more rows It is easy to permute the values of multiple variables at once. The only thing that changes from the above workflow is that we map mutate_at() to each iteration, specifying that we want to sample the values of an arbitrary set of treatment variables. Supposing that this data were called multi_treat…\n # helpful but not necessary: # save a vector of treatment names for easy access treatments \u0026lt;- c(\u0026quot;party\u0026quot;, \u0026quot;occupation\u0026quot;, \u0026quot;issue_tax\u0026quot;, \u0026quot;issue_abortion\u0026quot;, \u0026quot;issue_environment\u0026quot;) # resample values for each var in `treatments` multi_permuted \u0026lt;- multi_treat %\u0026gt;% crossing(m = 1:1000) %\u0026gt;% group_by(m) %\u0026gt;% nest() %\u0026gt;% mutate( permuted_data = map( .x = data, .f = mutate_at, .vars = vars(one_of(treatments)), .funs = sample ) ) %\u0026gt;% print() ## # A tibble: 1,000 x 3 ## # Groups: m [1,000] ## m data permuted_data ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 7]\u0026gt; \u0026lt;tibble [500 × 7]\u0026gt; ## # … with 990 more rows You would then be able to map() whatever estimation model you wanted over the permuted_data column, thereby estimating the model in each group m.\n Block Randomization Suppose we had data that were randomized within blocks. The data below contain \\(n = 500\\) observations and 5 blocks, with each block containing 50 treated units and 50 control units.\n## # A tibble: 500 x 4 ## id block treatment y ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2 1 0 ## 2 2 3 0 1 ## 3 3 4 0 0 ## 4 4 5 1 1 ## 5 5 1 0 0 ## 6 6 2 0 1 ## 7 7 3 1 1 ## 8 8 4 1 0 ## 9 9 5 1 0 ## 10 10 1 1 1 ## # … with 490 more rows We can’t naively permute the treatments over the whole data, since we want to respect the blocking structure. To permute treatments within blocks, we only require one additional step, which is to group the data by block during the permutation step.\nblock_data %\u0026gt;% crossing(m = 1:1000) %\u0026gt;% group_by(m) %\u0026gt;% nest() %\u0026gt;% mutate( permuted_data = map( .x = data, .f = ~ .x %\u0026gt;% group_by(block) %\u0026gt;% mutate(treatment = sample(treatment)) %\u0026gt;% ungroup() ) ) %\u0026gt;% print() ## # A tibble: 1,000 x 3 ## # Groups: m [1,000] ## m data permuted_data ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## # … with 990 more rows  Cluster randomization With cluster-randomized data, units are grouped within a cluster, and then every unit within a cluster gets the same treatment. Cluster-randomized data might look like so:\n## # A tibble: 500 x 4 ## id cluster treatment y ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 0 1 ## 2 2 1 0 0 ## 3 3 1 0 0 ## 4 4 1 0 1 ## 5 5 1 0 0 ## 6 6 2 1 1 ## 7 7 2 1 0 ## 8 8 2 1 1 ## 9 9 2 1 1 ## 10 10 2 1 0 ## # … with 490 more rows In order to permute the treatment assignments, we have to reshuffle the treatment status for each cluster such that all units in the same cluster get the same treatment. This is possible, but it’s a bit abstract, since it requires a nest-within-a-nest (whoa…). When we map() the function to permute the data, we do the following within each iteration m.\n Nest the unit-level variables (id and y) into a list-column. What remains is a data frame that is one row per cluster. Permute the treatment assignments at the cluster level Unnest the unit-level data, at which point every unit inherits its cluster-level treatment assignment.  cluster_permuted \u0026lt;- cluster_data %\u0026gt;% crossing(m = 1:1000) %\u0026gt;% group_by(m) %\u0026gt;% nest() %\u0026gt;% mutate( permuted_data = map( .x = data, .f = ~ .x %\u0026gt;% nest(within_cluster = c(id, y)) %\u0026gt;% mutate(treatment = sample(treatment)) %\u0026gt;% unnest(within_cluster) ) ) %\u0026gt;% print() ## # A tibble: 1,000 x 3 ## # Groups: m [1,000] ## m data permuted_data ## \u0026lt;int\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 2 2 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 3 3 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 4 4 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 5 5 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 6 6 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 7 7 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 8 8 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 9 9 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## 10 10 \u0026lt;tibble [500 × 4]\u0026gt; \u0026lt;tibble [500 × 4]\u0026gt; ## # … with 990 more rows To assure you that this works, I will plot the treatment status of all units within 3 clusters across the first 10 permutations. We should see that all units in the same cluster share the same treatment status within the same treatment permutation m.\n    I’ll shout out ri2 in particular.↩\n  The paper by Keele, McConnaughy, and White discusses and includes examples of other statistics such as rank-based tests, encouraging researchers to consider theory and the assumptions they are willing to make about the data, such as the choice of a “sharp” null hypothesis.↩\n  The estimated treatment effect is actually very close to the true effect, so what we’re seeing is a low-powered study.↩\n  This arguably requires more (or at least “different”) assumptions compared to a pure “Fisherian” approach to randomization inference, which is possible under a sharp null hypothesis that treatment has zero effect for all units.↩\n  Due to the discrete nature of the \\(\\delta_{0}\\) values, we do not obtain a perfect 95% interval.↩\n   ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"1d140b924e7c48f5a82a8660e9adcb7b","permalink":"/post/randomization-inference-purrr/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/randomization-inference-purrr/","section":"post","summary":"Randomization Inference within the Tidyverse","tags":["Causal Inference","Computational Methods"],"title":"Purrrmutation Testing","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Motivation One benefit of open-source research is that it is possible to trace the history of a research product through its (potentially many) iterations using a versioning system such as Git. This is great for readers who encounter the project’s remote repository, but it’s more likely the case that readers will encounter only a PDF of your paper in an email or through a preprint archive. While services like ArXiv will watermark your paper, it (or so it seems) only includes information about the paper’s history in ArXiv specifically, rather its history in your Git repository. This post describes how you can use Rmarkdown to include Git information into a working draft of your research paper.\nWhat exactly do I mean? Your paper typically includes the date of compilation, but you could also include the current commit hash, the branch of the current commit, and so on. Why would you want to do this?\n A compilation system like \\(\\mathrm{\\LaTeX}\\) can print the date of compilation, but it is often the case that documents are re-compiled without any real changes. This means the compilation date can be a deceiving signal about when the paper was most recently modified. You may want to “timestamp” a version of your paper in a way that is robust to re-compilation at an arbitrary future time. As you develop your paper locally, you may commit several small changes between major versions of your paper. To prevent your “in-development” copy from being confused for a major version of the paper, you may want to note which commit generated the current PDF and perhaps link to a more stable “for public eyes” version of the paper elsewhere. A more general case of the previous point: suppose you develop your project across multiple branches (e.g. as with “Git flow”). You may reserve your “master” branch for major versions of the project while iteratively developing the project (and compiling the document) on a non-master branch. In this case, you might want to know if a PDF was compiled from source code on the master branch (i.e. “Am I looking at a major version of the paper”) or on an in-development branch.  Here is an example from one of my in-progress papers.\nThis setup prioritizes the commit hash over the compilation date as a method for “dating” your paper. The branch name is included in cases where the PDF is generated on a development branch instead of on the master/public branch. The footnote corresponding to the commit information contains the commit message (not shown). And lastly, the link to the public version takes you to the master branch PDF on Github—the most recent major version.\n How to do it Setting this up consists of essentially two steps:\nLearn to print Git commands to the console using R. Place that R code in your .Rmd document’s YAML header.  Console commands with R We can run console commands within R using the system() function. Ordinarily the results of the commands merely print to the console instead of being treated as objects, but we want to make these objects be accessible in the R environment using the intern = TRUE argument.\nHere are some examples that will display Git information for my website repo (where this code is currently being evaluated).\nFor instance, how can we print the branch name?\nsystem(\u0026quot;git symbolic-ref --short HEAD\u0026quot;, intern = TRUE) ## [1] \u0026quot;master\u0026quot; To print only the hashes in your Git log, you can supply %t to the the --pretty argument of git log.\nsystem(\u0026quot;git log --pretty=%t\u0026quot;, intern = TRUE) ## [1] \u0026quot;7ee9385\u0026quot; \u0026quot;2441a5b\u0026quot; \u0026quot;d17d13e\u0026quot; \u0026quot;04293bd\u0026quot; \u0026quot;d9c823e\u0026quot; \u0026quot;c5fcaa0\u0026quot; \u0026quot;d3b381a\u0026quot; ## [8] \u0026quot;ad7ee18\u0026quot; \u0026quot;c5fcaa0\u0026quot; \u0026quot;533e975\u0026quot; \u0026quot;2e7c7a8\u0026quot; \u0026quot;eaee307\u0026quot; \u0026quot;43bfc74\u0026quot; \u0026quot;1856a0b\u0026quot; ## [15] \u0026quot;5823e01\u0026quot; \u0026quot;66e52d7\u0026quot; \u0026quot;0f043a4\u0026quot; \u0026quot;af6087c\u0026quot; \u0026quot;56bc096\u0026quot; \u0026quot;ebd98df\u0026quot; \u0026quot;3074778\u0026quot; ## [22] \u0026quot;94bcada\u0026quot; \u0026quot;fa8f4fa\u0026quot; \u0026quot;1079fd7\u0026quot; \u0026quot;1a3553b\u0026quot; \u0026quot;f352285\u0026quot; \u0026quot;0e7c4b4\u0026quot; \u0026quot;3be8c27\u0026quot; ## [29] \u0026quot;2a80372\u0026quot; \u0026quot;950065a\u0026quot; \u0026quot;b8d24f2\u0026quot; \u0026quot;9da4ece\u0026quot; \u0026quot;8cccbf9\u0026quot; \u0026quot;45d0315\u0026quot; \u0026quot;9d26ba5\u0026quot; ## [36] \u0026quot;6ebb3b0\u0026quot; \u0026quot;af8a617\u0026quot; \u0026quot;2c6ec85\u0026quot; \u0026quot;22fea55\u0026quot; \u0026quot;3854af2\u0026quot; \u0026quot;93d13c4\u0026quot; \u0026quot;b297079\u0026quot; ## [43] \u0026quot;f8b6836\u0026quot; \u0026quot;d988f5c\u0026quot; \u0026quot;6fc4555\u0026quot; \u0026quot;eabf878\u0026quot; \u0026quot;d6bf55f\u0026quot; \u0026quot;ecc0c17\u0026quot; \u0026quot;322d6d8\u0026quot; ## [50] \u0026quot;b204b83\u0026quot; \u0026quot;604a055\u0026quot; \u0026quot;5b6cd16\u0026quot; \u0026quot;7f3d4e8\u0026quot; \u0026quot;3e3d9d1\u0026quot; \u0026quot;c5c2a6b\u0026quot; \u0026quot;72ecc5a\u0026quot; ## [57] \u0026quot;bd25ad7\u0026quot; \u0026quot;2820840\u0026quot; \u0026quot;f8be89c\u0026quot; \u0026quot;5011495\u0026quot; \u0026quot;b4f159a\u0026quot; \u0026quot;471d45e\u0026quot; \u0026quot;32e03b8\u0026quot; ## [64] \u0026quot;d55b641\u0026quot; \u0026quot;175df3e\u0026quot; \u0026quot;03985bd\u0026quot; \u0026quot;549e2f0\u0026quot; \u0026quot;8effeb6\u0026quot; \u0026quot;e7c1fc3\u0026quot; \u0026quot;19f3bcd\u0026quot; ## [71] \u0026quot;0647521\u0026quot; \u0026quot;5913357\u0026quot; \u0026quot;b146ac2\u0026quot; \u0026quot;494f860\u0026quot; \u0026quot;557bf2a\u0026quot; \u0026quot;2b367c7\u0026quot; \u0026quot;734e099\u0026quot; ## [78] \u0026quot;8ef25d4\u0026quot; \u0026quot;1d949ce\u0026quot; \u0026quot;ed14db3\u0026quot; \u0026quot;ba4694c\u0026quot; \u0026quot;57d5fc6\u0026quot; \u0026quot;1656482\u0026quot; \u0026quot;28d68d7\u0026quot; ## [85] \u0026quot;5b8e92a\u0026quot; \u0026quot;a807aab\u0026quot; \u0026quot;359f06a\u0026quot; \u0026quot;78c3ee3\u0026quot; \u0026quot;defc14f\u0026quot; \u0026quot;ec7e081\u0026quot; \u0026quot;e4c9176\u0026quot; ## [92] \u0026quot;ab502db\u0026quot; \u0026quot;7fe3ee6\u0026quot; \u0026quot;2f97534\u0026quot; \u0026quot;3259f27\u0026quot; \u0026quot;bec13bd\u0026quot; \u0026quot;f3142cc\u0026quot; \u0026quot;2959bf6\u0026quot; ## [99] \u0026quot;b4754c2\u0026quot; \u0026quot;91fe96a\u0026quot; \u0026quot;91bba9b\u0026quot; \u0026quot;071d153\u0026quot; \u0026quot;8e4cce3\u0026quot; \u0026quot;ba09b95\u0026quot; \u0026quot;741632b\u0026quot; ## [106] \u0026quot;3569cdc\u0026quot; \u0026quot;d99c163\u0026quot; \u0026quot;5c135e3\u0026quot; \u0026quot;2671a4b\u0026quot; \u0026quot;2b7d810\u0026quot; \u0026quot;ea7d44d\u0026quot; \u0026quot;6c7656c\u0026quot; ## [113] \u0026quot;e40d5d8\u0026quot; \u0026quot;bb9199d\u0026quot; \u0026quot;ca4e593\u0026quot; \u0026quot;c42c33f\u0026quot; \u0026quot;d17291e\u0026quot; \u0026quot;38d1910\u0026quot; \u0026quot;6bc2299\u0026quot; ## [120] \u0026quot;3131d9d\u0026quot; \u0026quot;5906234\u0026quot; \u0026quot;d355f02\u0026quot; \u0026quot;7a6e215\u0026quot; \u0026quot;c5befba\u0026quot; \u0026quot;b0dba1c\u0026quot; \u0026quot;c1d6342\u0026quot; ## [127] \u0026quot;87f3ceb\u0026quot; \u0026quot;83ca75b\u0026quot; \u0026quot;69e41cf\u0026quot; \u0026quot;f9278c7\u0026quot; \u0026quot;a3ee86e\u0026quot; \u0026quot;816ebb5\u0026quot; \u0026quot;030278d\u0026quot; ## [134] \u0026quot;2d9384b\u0026quot; \u0026quot;fec8391\u0026quot; \u0026quot;83dbb8c\u0026quot; \u0026quot;1210553\u0026quot; \u0026quot;ce35ec0\u0026quot; \u0026quot;ab3c776\u0026quot; \u0026quot;c62ad9f\u0026quot; ## [141] \u0026quot;3148687\u0026quot; \u0026quot;c3621d8\u0026quot; \u0026quot;943687e\u0026quot; Use indexing to isolate only the most recent hash from this vector of results.\nsystem(\u0026quot;git log --pretty=%t\u0026quot;, intern = TRUE)[1] ## [1] \u0026quot;7ee9385\u0026quot; To print the commit message, use --pretty=%s instead.\nsystem(\u0026quot;git log --pretty=%s\u0026quot;, intern = TRUE)[1] ## [1] \u0026quot;home page: no more skills, add interests/ed, new bio \u0026amp; abt page\u0026quot;  R results in the YAML Now that we know which commands to run to get the Git info, how do we get this information into our YAML? We will do this using inline R code chunks. This code block shows what I’ve done for the above paper example, and I describe a few of the tricks I use below.\ndate: | | Commit \\texttt{`r system(\u0026quot;git log --pretty=%t\u0026quot;, intern = TRUE)[1]`} on branch \\texttt{`r system(\u0026quot;git symbolic-ref --short HEAD\u0026quot;, intern = TRUE)`}\\footnote{Commit message: \\texttt{`r system(\u0026quot;git log --pretty=%s\u0026quot;, intern = TRUE)[1]`}} | Compiled `r format(Sys.time(), \u0026#39;%B %d, %Y\u0026#39;)` | Most recent online version [here](https://github.com/mikedecr/causal-bayes/blob/master/writing/causal-bayes-paper.pdf). We use the date variable, but we supply multiple lines of content. To do this, place a pipe | after declaring the date variable, and begin each line with a new pipe |. This will line-break the content in your compiled PDF and let you supply \\(\\mathrm{\\LaTeX}\\) code directly to the variable. To use teletype/fixed-width font, type the \\texttt{} command for \\(\\mathrm{\\LaTeX}\\) directly in Rmarkdown. We can evaluate and print the results of inline R code by including the letter r at the beginning of an inline code chunk (delimited by backticks). This code is evaluated before the document is compiled, so the information being passed to \\texttt{} is the results of the R code rather than the text of the R code itself. Do the same basic setup for the commit hash, commit message (in a footnote), and the compilation date. Note that the formatting of the compilation date gives you prettier results than the Rmarkdown default. Lastly, you can link the reader to the most recent public PDF by linking to your remote master branch. By linking directly to Github (or wherever else you host the remote repository), any time you push an update to remote, your PDF will automatically be up to date. This will be true of any offline PDF, any previous PDF, and any PDF generated on any branch. This is because the URL to your master branch PDF will not change even if the PDF file itself changes!    Caveat When you push to Github, it creates new hashes that differ from your local machine. As a result, you can’t use the hash in the PDF to cross-reference the same hash on Github. This is a shortcoming of the approach, and if I think of a feasible way around it, I will update this post or write a new post altogether.\n ","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"21b4c0614a79806c9204f03239bdf915","permalink":"/post/git-in-papers/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/post/git-in-papers/","section":"post","summary":"Timestamp a paper with a commit hash and branch name\n","tags":["Computational Methods","R","R Markdown"],"title":"Git information in your open-source research paper (with Rmarkdown)\n","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Motivation for this post Over this summer, I have been organizing a reading group on causal inference for students in my department. As someone who sees data analysis problems primarily through Bayesian goggles, I have been doing extra work in my head to make sense of “Bayesian causal inference.” I’m hoping to write some articles about this for political scientists, but the dissertation (rightly) has more of my attention lately.\nWe covered causal mediation this week (Imai et al. 2011 APSR), which I thought would be a good opportunity to explain where my thoughts are going about this. So this post will briefly describe a Bayesian vantage point on causal inference and show how to use Bayesian tools to implement it.\n Posterior Predictive Draws. I mean, “Unobserved Potential Outcomes” It should be noted up front that a Bayesian take on causal inference is not at all new (I will borrow plenty of intuition from, for example, Rubin 1978), but it is pretty unfamiliar to the political science/econ folks I roll with.1 People often ask me, “How can you even have a Bayesian experiment; don’t you already have randomization?” as if the purpose of priors is to fix confounding somehow. In fairness to non-Bayesians, if this is how Bayesian analysis used priors, I would also be mistrusting of Bayes. Luckily, priors are less presumptuous than that. You get a Bayesian experiment (or any other credible research design) by specifying priors on the parameters and obtaining a posterior distribution. It is pretty unremarkable—no different than a Bayesian analysis of a non-causal design. Remember that the causal model (by which I mean, the definition of the potential outcomes) is distinct from the methods used to estimate causal parameters. Bayesian analysis is positioned closer to the estimation end of things, whereas causal modeling is a series of assumptions about identifying variation in the data. In short, you fix confounding with the design, and priors are for improving the estimation.2\nWhile the Bayesian approach may not change the research design or the causal assumptions, it does provide a different—and intuitive, I assert—interpretation of potential outcomes. Ordinarily we write potential outcomes as \\(Y_{i}(T_{i} = t)\\), the outcome value for unit \\(i\\) if it received treatment value \\(t\\). Only one potential outcome per unit is ever observed, so can’t observe the unit-level causal effect \\(\\tau_{i}\\), but we can use a causal identification analysis to lay out the assumptions required to estimate an average effect \\(\\bar{\\tau}\\) for at least some subset of units. If we knew this average effect, we would be able to state, for each observed outcome \\(y_{i}\\), what the expected value of that unit’s unobserved potential outcome would be if we could set \\(T_{i}\\) to some value \\(t\u0026#39;\\) other than what was observed. In this way, the unobserved potential outcome is missing data that we can predict with an estimated the model that generates (potential) outcomes.\nMaybe you can see where I’m going with this.\nBayesian analysis begins with joint model \\(p\\left(y, \\theta \\right)\\) for outcome data \\(y\\) and model parameters \\(\\theta\\). This is equivalently expressed as \\(p(y \\mid \\theta)p(\\theta)\\), which is to say that the distribution of \\(y\\) depends on the value of \\(\\theta\\) and that \\(\\theta\\) has its own distribution. We fit the model by conditioning on the observed \\(y\\) to obtain the posterior distribution \\(p\\left(\\theta \\mid y \\right)\\). It is this updated model that represents our state of information about the process that generates potential outcomes \\(y_{i}(t)\\). If we wanted to make posterior inferences about what \\(y_{i}(t)\\) would have been (in expectation) if we could arbitrarily change \\(t\\), we would simulate the unobserved potential outcomes \\(\\tilde{y}\\) from the model. \\[\\begin{align} p(\\tilde{y} \\mid y) \u0026amp;= \\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y)d\\theta \\end{align}\\] The unobserved potential outcome is expressed as a probability distribution because we don’t know exactly what the unobserved data would be. Its distribution depends on \\(\\theta\\), which itself is conditioned on \\(y\\), and we average over our uncertainty about \\(\\theta\\) by integrating. This gives us a distribution for the unobserved potential outcomes that is marginal of our imperfectly estimated parameters.\nOkay, so? The Bayesian view of potential outcomes is appealing because our state of ignorance about the exact potential outcomes is an explicit feature of the model, rather than a point estimate with a post-hoc standard error. Which is to say, we don’t know what the treatment effect is, and so we don’t know what the potential outcomes are, but we have a range of guesses that that we can directly evaluate using their probability distribution. This approach has a certain philosophical resonance before we get anywhere near the notion of prior information. And to whatever extent researchers already view point estimates and frequentist confidence intervals on treatment effects as “ranges of plausible values” with associated posterior probabilities, they are already doing Bayesian causal inference—just without the benefit of having formally set up the whole model. With the Bayesian approach we are actually allowed to say things like “the data suggest that this treatment effect is most likely positive” or what have you.\n Causal Mediation Causal mediation analysis is concerned with a causal graph where a treatment \\(T\\) affects an outcome \\(Y\\), and the effect flows at least partially through a mediator \\(M\\). Potential outcomes are expressed as \\(Y_{i}(T_{i}, M_{i}(T_{i}))\\), where the value of \\(Y\\) depends both on the treatment assignment \\(T_{i} = t\\) and the resulting value of the mediator \\(M_{i}(t)\\), which is itself affected by the treatment. The causal effects are a decomposition of the total (average) treatment effect.\n The total treatment effect: how much total change in \\(Y\\) is owed to setting the value of \\(T\\)? Written as \\(Y(1, M(1)) - Y(0, M(0))\\). The causal mediation effect: how much of the total change in \\(Y\\) is attributed to \\(T\\)’s effect on \\(M\\), which also affects \\(Y\\)? Or, how much change in \\(Y\\) is owed to the fact that \\(M\\) changed, as opposed to not changing? Written as \\(Y(t, M(1)) - Y(t, M(0))\\). The direct effect: how much of the change in \\(Y\\) is not flowing through \\(M\\)? In other words, how would \\(Y\\) be different even if \\(T\\) had no effect on \\(M\\)? Written as \\(Y(1, M(t)) - Y(0, M(t))\\).  Imai et al. present an algorithm to estimate these quantities. We need models to describe how \\(M(T)\\) and \\(Y(T, M(T))\\) are generated, but the form of these models does not affect the intuition of the algorithm. It’s like this:\nEstimate mediator as a function of treatment and pre-treatment covariates: \\(M_{i} = f(T_{i}, X_{i})\\) Estimate the outcome as a function of the treatment, the observed mediator, and pre-treatment covariates. \\(Y_{i} = g(T_{i}, M_{i}, X_{i})\\). Using \\(f()\\), generate predicted values \\(\\hat{M}\\) for all \\(t\\). Using \\(g()\\), predicted values \\(\\hat{Y}\\) for all potential outcomes \\(y(t, M(t\u0026#39;))\\) Use the appropriate \\(\\hat{Y}\\) values to calculate average total, direct, and mediation effects.   Doing it Imai et al. demonstrate their method using (in part) an experimental study by Brader, Valentino, and Suhay 2008 on the way news stories affect immigration attitudes through specific emotional mechanisms. Let’s do the outcome where \\(Y\\) represents a participant’s decision to send an anti-immigrant message to their Congressperson (\\(Y\\)), which is affected by a cue in the story about a hypothetical immigrant’s ethnicity (\\(T\\)), and moderated by the emotion of anxiety (\\(M\\)).\nFirst, Imai et al. use an OLS model to predict respondent’s anxiety in response to treatment, with pre-treatment covariates \\(X_{i}\\) and coefficients \\(\\zeta_{1}\\). Parameters are subscripted \\(1\\) for the “first stage” of the estimation. \\[\\begin{align} M_{i} \u0026amp;= \\alpha_{1} + T_{i}\\beta_{1} + X_{i}\\zeta{1} + \\epsilon_{i} \\end{align}\\] They then use a probit model to estimate the outcome variable, the “second stage” (subscripted 2). This model includes the mediator with coefficient \\(\\gamma\\). \\[\\begin{align} p(Y_{i} = 1) \u0026amp;= \\Phi\\left(\\alpha_{2} + T_{i}\\beta_{2} + M_{i}\\gamma + X_{i}\\zeta_{2}\\right) \\end{align}\\] You should be able to code to implement this routine in R here, which calls this Stan file.\nIn Stan, it is easy to generate posterior quantities of interest in the generated quantities block of a Stan file. For example, generating posterior predictions for mediator values at \\(T \\in \\{0, 1\\}\\) is as easy as…\nm0 = alpha_m + (0 * beta_m) + (X * zeta_m); m1 = alpha_m + (1 * beta_m) + (X * zeta_m); Because alpha_m, beta_m, and zeta_m are all uncertain parameters, what we are actually doing is generating m0 and m1 in each iteration of the sampler, thus creating a distribution of predicted mediator values. In the integral notation from above, what we’re actually doing is generating a distribution \\(p\\left(M(t)\\right)\\) by marginalizing over all of the parameters (except for the error term, which is presumably fixed in the counterfactual case). \\[\\begin{align} p\\left(\\tilde{M}(t)\\right) \u0026amp;= \\int p\\left(\\tilde{M}(t) \\mid \\alpha_{1}, \\beta_{1}, \\zeta_{1}\\right) p(\\alpha_1, \\beta_1, \\zeta_1 \\mid M)d\\alpha_1 d\\beta_1 d\\zeta_1 \\end{align}\\] Hopefully I haven’t messed up the integral.\nPosterior predictions for new potential outcome observation \\(\\tilde{Y}(t, M(t\u0026#39;))\\) would be… \\[\\begin{align} p\\left(\\tilde{Y}(t, m(t\u0026#39;))\\right) \u0026amp;= \\int p\\left(\\tilde{Y}(t, M(t\u0026#39;)) \\mid \\alpha_2, \\beta_2, \\tilde{M}(t\u0026#39;), \\gamma, \\zeta_2 \\right) \\times \\\\[6pt]\u0026amp;\\qquad p\\left(\\alpha_2, \\beta_2, \\tilde{M}(t\u0026#39;), \\gamma, \\zeta_2 \\mid Y\\right) d\\alpha_2 d\\beta_2 d\\tilde{M}(t) d\\gamma d\\zeta_2. \\end{align}\\] This expression is also marginalizing over the simulated mediator value \\(M(t\u0026#39;)\\). Because the simulated mediator is a function of random variables, it itself is also a random variable with a probability distribution.\nIn order to get total, direct, and mediation effects, we calculate each comparison of potential outcomes using the posterior predictive draws, and then average over each observation in the data. Here are the posterior samples for each treatment effect component.\n## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: ## method from ## +.gg ggplot2 ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. And here is a comparison to the {mediation} package by the Imai et al. team. We can see that, because the posterior distributions for the ACMEs are not symmetrical, there is some difference between the {mediation} estimates (which come from an maximum likelihood model) and the Bayesian estimate, which is a posterior mean.\n Other things to think about Imai et al. propose a sensitivity analysis to measure “how much” post-treatment confounding among mediators would be enough to change your inference about causal mediation effects. While I won’t do this now, it would be possible to specify a prior on the sensitivity parameter. Such a move would let the researcher evaluate the mediation effect marginal of a distribution of potential confounding, rather than merely conditional on one fixed level of confounding. This would let us make a probabilistic statement about the threat of confounding rather than a hypothetical statement. It’s of course subject to the prior, but most researchers substantively interpret their results assuming that confounding is zero, so we can think about the prior as actually relaxing an assumption of zero confounding rather than “adding a new assumption.”\n   There are a few examples of it in political science, but the Bayesian component is used mostly for computation (MCMC) rather than for the Bayesian ideas themselves. Meanwhile Bayes-for-its-own-sake seems far more prevalent in fields like psychology, epidemiology, and biostatistics.↩\n  Ugh, caveating. It would be possible to represent identification assumptions as special cases of prior distributions, where the parameters of the prior can be manipulated to “relax” the assumption. For example, unconfoundedness or exclusion restrictions imply a model that contains additional covariates that each have priors that stack all probability density at exactly zero. This exercise is actually very similar to the specification of the “sensitivity parameter” in the Imai et al. mediation analysis routine.↩\n   ","date":1560902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560902400,"objectID":"0c67a89a4cd18647521f55b9d6bc0d46","permalink":"/post/bayes-mediation/","publishdate":"2019-06-19T00:00:00Z","relpermalink":"/post/bayes-mediation/","section":"post","summary":"This is what we were already doing","tags":["Computational Methods","Bayesian Statistics","Causal Inference"],"title":"Causal Mediation, Bayesianly","type":"post"},{"authors":["Michael DeCrescenzo"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577903234,"objectID":"3284c416b2bffe9b57b68dae508493f9","permalink":"/example-publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/example-publication/preprint/","section":"example-publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"example-publication"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" I’ve been reading about Bayesian causal inference for a paper I’m hoping to write, and this has led me to dig into the work by Gerber, Green, and Kaplan (hereafter “GGK”) about the “Illusion of Learning from Observational Research.” In it, they put forth a model to describe how much you “update” your information about causal effects from experimental vs. observational research.\nThe intuition of the model? Suppose that we want to learn about some true causal effect \\(\\tau\\). When we conduct a study, our estimate \\(\\hat{T}\\) reflects the true effect \\(\\tau\\), plus bias \\(\\beta\\), plus error \\(\\epsilon\\): \\[\\begin{align} \\hat{T} \u0026amp;= \\tau + \\beta + \\epsilon, \\end{align}\\] What we want to do is update our information about \\(\\tau\\). Under what conditions do we learn a lot about it?\nSuppose we collect a ton of data, so the variance of \\(\\hat{T}\\) shrinks to be very small. This is similar to shrinking \\(\\epsilon\\) toward zero, so we’re pretty certain what \\(\\tau + \\beta\\) is, but we’ve only identified their sum. We don’t know how much of \\(\\hat{T}\\) is \\(\\tau\\) and how much is \\(\\beta\\).\nHere’s the thrust of their argument. If we want to update our priors about \\(\\tau\\) specifically, we need a study design where we have clear priors about \\(\\beta\\). If our study is experimental, we can assume a priori that the size of the bias \\(\\beta\\) is either zero or very small. This way, learning about \\(\\hat{T}\\) allows us to learn a lot about \\(\\tau\\). When we are in an observational study, we don’t know as much about the size of the bias \\(\\beta\\), and the data don’t allow us to update those priors independently of \\(\\tau\\). As a result, the data can’t tell us as much about \\(\\tau\\), only \\(\\tau + \\beta\\).\nThis model makes a lot of sense. When you have vague priors about the bias in a study, you don’t know how to interpret its findings. Fair!\nBut… The namesake of GGK’s theoretical model, the Illusion of Learning from Observational Research, is the model’s result where we have flat priors about the bias \\(\\beta\\). Under a flat prior, we learn nothing from having conducted an observational study because the bias could be anything.\nWhat I want to assert is that the “flat priors” result is a degenerate case. It does not accurately characterize observational research, and it never really occurs in real life. As a result, we need to be careful interpreting the model to avoid overstating what it actually says about real-world observational research.\nSuppose we conduct an observational study and detect an effect of size \\(1.0\\). If we are concerned about uncorrected bias in the study design, practically speaking we are mainly concerned with the possibility that the effect is over-estimated—unobserved confounders drive self-selection into treatment, inflating the causal effect estimate. This would give us a prior that the true effect most likely falls somewhere between 0 and the observational effect size. Meaning, we actually have pretty specific priors about the bias. Would we put much probability on the possibility that the true effect is double the estimated effect, or greater? Probably not, since most of the time we are worried about self-selection into treatment. Do we think that the true effect is just as big as the estimated effect, but in the exact opposite direction? Again, probably not, especially if we have theorized carefully about our expectations for the study.\nWe can see it better by rearranging terms. Setting \\(\\epsilon\\) to \\(0\\) for a moment… \\[\\begin{align} \\beta \u0026amp;= \\hat{T} - \\tau \\end{align}\\] The bias \\(\\beta\\) is the difference between the true and observed effects, so our prior for \\(\\beta\\) is the difference in our priors for the true and estimated effects. If we have some expectation about the true and estimated effects—and we normally do…—then we have reasonably clear priors about the size of the bias in observational studies.\nWhat does a Bayesian say? Our priors aren’t flat, but so the “Illusion” of learning in observational studies is an overstatement (or even “sleight of hand”). I worry how many people have read the original GGK piece and come away with the impression that flat priors about \\(\\beta\\) is a fair or accurate representation of observational research. I don’t mean this as a defense of biased research—rather, it is a criticism of flat priors. I don’t know if GGK intended for the flat priors case to be interpreted as “realistic”—on the one hand, the flat-priors result is the namesake of the piece, but their numerical example uses a non-flat prior for the bias term—but if a reader isn’t already thinking hard about their priors, then it’s easy to see how they might not catch this.\n Zooming out There are a few mid-level lessons we could reinforce by thinking about our priors about bias in observational studies.\nFirst, there’s never a bad time to remember that (improper) flat priors are unrealistic. Inferences under flat priors can look like inferences under informed priors if the data are strong enough, but we should worry about any exercise where some theoretical result necessarily depends on an assumption of flat priors. In thought experiments and in real data analysis, you can always do better than a flat prior.\nRelatedly, Bayesians are keen to highlight areas where informed priors provide important stability to some result that would have looked like nonsense under flat priors. This is one of those cases. Flat priors lead you in an unstable direction assessing the information conveyed by research. It’s only in a case where you have more informed priors about the terms in the GGK model where the results conform to how we actually think about research findings.1\nLastly, I am increasingly preoccupied by the way Bayes’ theorem is routinely used in theoretical models to convey important intuitions about causal inference and yet there is so little formal incorporation of Bayesian priors in applied data analysis of credible research designs (in political science at least; other fields mix these things much more). I’m trying to write a paper about doing applied Bayesian analysis in causal inference, and I hope the causal inference crowd can be convinced to legalize it!\n   Furthermore, it is worth emphasizing that flat priors are only flat with respect to a likelihood. As soon as you have a quantity that is a function of multiple parameters, the prior for this resulting quantity will not be flat even if the priors for each parameter are flat. If you have ever demonstrated the Central Limit Theorem by summing a bunch of uniform random variables that results in a bell curve, you have already seen this in action!↩\n   ","date":1550880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550880000,"objectID":"637d6006a04b9713167609db170ffd33","permalink":"/post/ggk-flat-priors/","publishdate":"2019-02-23T00:00:00Z","relpermalink":"/post/ggk-flat-priors/","section":"post","summary":"Experimentalists agree! When flat priors lead to worse learning\n","tags":["Causal Inference","Bayesian Statistics"],"title":"Unrealistic Priors and the \"Illusion of Learning from Observational Research\"\n","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Michael DeCrescenzo"],"categories":["Methods"],"content":" Background A few days ago, Andrew Heiss was looking for a way to visualize multiple regression with an emphasis on one predictor, without 3(+)-dimensional plots. He works through a method and posts this cool animation, which shows the changing relationship between \\(x\\) and \\(y\\) when adding controls, superimposed over the scatterplot of the raw data. (He credits Pete Mohanty with the shifting abline idea.)\n Helpful animated #dataviz showing what happens to the slope of one coefficient in a model when controlling for other variables in multiple regression\n(#rstats code: https://t.co/yhVLj325Oh) pic.twitter.com/2foYfXDo28 — 🎃 Andrew Heiss, scary PhD 🦇 (@andrewheiss) October 18, 2018    This is cool, but based on Andrew’s initial question, I had something a little different come to mind. I thought we’d be seeing the impact of the regression in both the regression line and in the data. So I tried to make that (starting with his code)…\n Ok fixed. In this fig, y is (beta * humidity), plus the regression residual. This is equivalent to starting with the fully estimated regression and subtracting out terms for every other covariate pic.twitter.com/fLs4WxHTaK — Michael DeCrescenzo (@mikedecr) October 18, 2018    which he liked and asked to see the code for.\nSo I will deliver. Here is a gist containing an example, and below is some explanation.\n Intuition Some math will help. Let’s start by writing the regression equation to suit the task at hand: although we include multiple predictors, we only want to highlight one of them, putting the other predictors into a black box “vector of controls.” Andrew’s example uses Dark Sky data on weather in Provo, UT, highlighting the relationship between humidity and a daily temperature high for each day \\(i\\)… \\[\\begin{align} \\mathit{HighTemp}_{i} \u0026amp;= \\alpha + \\beta\\left(\\mathit{Humidity}_{i}\\right) + \\mathbf{x}_{i}^{T}\\gamma + \\varepsilon_{i} \\end{align}\\] where \\(\\alpha\\) is the constant, \\(\\mathbf{x}_{i}\\) is a column-vector of covariate observations for unit \\(i\\) (everything but humidity), and \\(\\gamma\\) is a vector of coefficients for all non-humidity predictors.\nOperationally, what we want to do is show how \\(\\beta\\) changes with the inclusion of additional controls. Andrew’s example shows this by plotting different regressions overtop the raw data. If we run the code1 from his Gist:\nThe line being plotted starts with \\(\\hat{\\mathit{High}}_{i} = {\\alpha} + {\\beta}(\\mathit{Humidity}_{i})\\) and adds additional covariates one at a time. The data remain intact.\n Variation But let’s say that we wanted to see the effect of controls in the data as well. This is, I think, where the real umph from this kind of visualization would be; after all, we have already told students that including other predictors will affect the line.\nThinking about the math, this is as easy as doing to the raw data what we’ve already done to the regression line: subtract out the effect of the covariates. That is, purge the effect of other variables from the raw data. Start with the fully specified regression model… \\[\\begin{align} \\mathit{High}_{i} \u0026amp;= \\alpha + \\beta\\left(\\mathit{Humidity}_{i}\\right) + \\mathbf{x}_{i}^{T}\\gamma + \\varepsilon_{i} \\end{align}\\] …and then subtract out the influence of variables in \\(\\mathbf{x}_{i}\\). \\[\\begin{align} \\label{eq:sub} \\mathit{High}_{i} - \\mathbf{x}^{T}\\gamma \u0026amp;= \\alpha + \\beta\\left(\\mathit{Humidity}_{i}\\right) + \\varepsilon_{i} \\end{align}\\]\nWe still have to decide what to do with the constant. We could…\n Leave the constant there, which is probably undesirable because the value of the constant reflects the scaling of other covariates. Start by setting all covariates equal to their means. This would give us a prediction that is no longer subject to the scaling of the covariates but the covariates still affect the mean of \\(y\\) overall. This works but I think we can make it simpler. Subtract the constant along with the covariates. This leaves us with only the predicted partial effect of humidity (plus error). This is what we’ll do, because it zooms in only on the predictor that we care about.   Implementation Now we will create the revised gif.\nFirst we start with the original Heiss data and code.\n# ---- Heiss code ----------------------- library(\u0026quot;magrittr\u0026quot;) library(\u0026quot;tidyverse\u0026quot;) library(\u0026quot;lubridate\u0026quot;) library(\u0026quot;broom\u0026quot;) library(\u0026quot;scales\u0026quot;) library(\u0026quot;gganimate\u0026quot;) # Load and clean data # This data comes from Dark Sky\u0026#39;s API weather_provo_raw \u0026lt;- read_csv(\u0026quot;https://andhs.co/provoweather\u0026quot;) # clean dates and precip weather_provo_2017 \u0026lt;- weather_provo_raw %\u0026gt;% mutate( month = month(date, label = TRUE, abbr = FALSE), month_number = month(date, label = FALSE), weekday = wday(date, label = TRUE, abbr = FALSE), weekday_number = wday(date, label = FALSE), precipType = ifelse(is.na(precipType), \u0026quot;none\u0026quot;, precipType) ) %\u0026gt;% select( date, month, month_number, weekday, weekday_number, sunriseTime, sunsetTime, moonPhase, precipProbability, precipType, temperatureHigh, temperatureLow, dewPoint, humidity, pressure, windSpeed, cloudCover, visibility, uvIndex ) # keep winter and spring, scale vars winter_spring \u0026lt;- weather_provo_2017 %\u0026gt;% filter(month_number \u0026lt;= 5) %\u0026gt;% mutate(month = factor(month, ordered = FALSE)) %\u0026gt;% mutate( humidity = humidity * 100, cloudCover = cloudCover * 100, precipProbability = precipProbability * 100 ) We combine several model formulas into a data frame and estimate each regression using purrr::map(). We’ve added the results from broom::augment() because we want the residuals from each model to create the “noise” in the data for the graphic.\n# ---- mike decrescenzo modifications begin ----------------------- # Run all these models in one data frame (purrr::map) # add the data as a list column because we\u0026#39;ll want it later models \u0026lt;- tribble( ~formula, \u0026quot;temperatureHigh ~ humidity\u0026quot;, \u0026quot;temperatureHigh ~ humidity + windSpeed\u0026quot;, \u0026quot;temperatureHigh ~ humidity + windSpeed + cloudCover\u0026quot;, \u0026quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability\u0026quot;, \u0026quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability + visibility\u0026quot;) %\u0026gt;% # data in a list column mutate(spring_data = list(winter_spring)) %\u0026gt;% # Run a model in each row mutate(model = map2(formula, spring_data, ~ lm(.x, data = .y))) %\u0026gt;% # Extract model elements mutate( model_tidy = map(model, tidy, conf.int = TRUE), model_glance = map(model, glance), model_fits = map(model, augment) ) We calculate the impact of humidity on the high temperature by extracting the humidity coefficient from each model and multiplying it by the raw humidity data (which comes in the augment results). We will lazily refer to this as the humidity’s “partial prediction” of temperature (thanks to Gina Reynolds for feedback on what this should be called). As a bonus, we will also save the upper and lower bounds of the humidity beta confidence interval.\n# compute the partial effect of humidity: beta * humidity model_partials \u0026lt;- models %\u0026gt;% # get the humidity beta and bounds mutate( humidity_beta = map(model_tidy, ~ filter(.x, term == \u0026quot;humidity\u0026quot;)$estimate) %\u0026gt;% as.numeric(), beta_low = map(model_tidy, ~ filter(.x, term == \u0026quot;humidity\u0026quot;)$conf.low) %\u0026gt;% as.numeric(), beta_high = map(model_tidy, ~ filter(.x, term == \u0026quot;humidity\u0026quot;)$conf.high) %\u0026gt;% as.numeric() ) %\u0026gt;% # calculate partial effect of humidity and keep the residual unnest(model_fits) %\u0026gt;% mutate(partial = humidity * humidity_beta) %\u0026gt;% select(formula, humidity, contains(\u0026quot;beta\u0026quot;), partial, .resid) # get the beta for label plotting model_beta \u0026lt;- model_partials %\u0026gt;% select(formula, contains(\u0026quot;beta\u0026quot;)) %\u0026gt;% distinct()  Now we create the figure. The horizontal axis is the raw humidity data. The vertical axis is the humidity effect (\\(\\beta \\times \\mathit{Humidity}_{i}\\)) plus the regression residual \\(\\varepsilon_{i}\\). The regression line is simply \\(\\hat{\\mathit{High}}_{i} = \\hat{\\beta}\\mathit{Humidity}_{i}\\) with a constant of zero. That is, on a given day, a humidity level of \\(z\\) exerts a negative impact on temperature amounting to \\(\\beta z\\), setting other factors aside. Conveniently, we don’t have to manually subtract the other covariates because we already know how to calculate the vertical axis using the partial effect and the residual (thanks to the math above).\n# animate ggplot(data = model_partials, aes(x = humidity, y = partial + .resid)) + geom_point(color = \u0026quot;gray\u0026quot;) + geom_abline( data = model_beta, aes(intercept = 0, slope = humidity_beta, group = formula) ) + geom_abline( data = model_beta, aes(intercept = 0, slope = beta_low, group = formula), linetype = 3 ) + geom_abline( data = model_beta, aes(intercept = 0, slope = beta_high, group = formula), linetype = 3 ) + theme_minimal(base_family = \u0026quot;Fira Sans\u0026quot;) + geom_label( data = model_beta, aes(x = 35, y = -70, label = paste0(\u0026quot;beta: \u0026quot;, round(humidity_beta, 3)), group = formula), parse = TRUE, family = \u0026quot;Fira Sans\u0026quot;, size = 4) + labs( x = \u0026quot;Humidity\u0026quot;, y = \u0026quot;Partial Predicted High Temperature (plus residual, °F)\u0026quot;, subtitle = \u0026quot;{closest_state}\u0026quot; ) + transition_states(formula, transition_length = 0.25, state_length = 0.5) + enter_fade() + ease_aes(\u0026#39;sine-in-out\u0026#39;) Two notes about the confidence interval\nThe confidence intervals here don’t look like the ordinary hourglass-shaped intervals in linear regression. This is because the hourglass shape comes from uncertainty in both the constant and coefficients. However, the constant has been subtracted out of these predictions, so uncertainty in this visualization only reflects uncertainty in the humidity effect. I would show confidence intervals with geom_ribbon(), except I can’t get ribbons to animate because of some weird stuff that’s interfering with transformr during animation.   That’s it I don’t have comments enabled on the website but get at me on Twitter.\n  Maybe it’s a dev version thing, but scales::degree_format() doesn’t work for me, so I removed it.↩\n   ","date":1539907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539907200,"objectID":"2d1337e2100c2ba1c8e8c94a0e46cf7a","permalink":"/post/viz-partials/","publishdate":"2018-10-19T00:00:00Z","relpermalink":"/post/viz-partials/","section":"post","summary":"Partial effects without high dimensions","tags":["Visualization","R"],"title":"A Visualization of Partial Effects in Multiple Regression","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561762571,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Michael DeCrescenzo","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577903234,"objectID":"133bdda310b422087175414dccc2c689","permalink":"/example-publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/example-publication/journal-article/","section":"example-publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"example-publication"},{"authors":["Michael DeCrescenzo","Robert Ford"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577903234,"objectID":"3b73c52a141c990bbfdbdd288a27bb19","permalink":"/example-publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/example-publication/conference-paper/","section":"example-publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"example-publication"}]