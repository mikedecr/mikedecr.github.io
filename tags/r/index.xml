<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Michael DeCrescenzo</title>
    <link>/tags/r/</link>
    <description>Recent content in R on Michael DeCrescenzo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Michael DeCrescenzo {year}</copyright>
    <lastBuildDate>Sun, 26 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How do I reinstall my packages for R 4.0 when many of them came from Github?</title>
      <link>/post/package-reinstall/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/package-reinstall/</guid>
      <description>


&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;R 4.0 &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;is here&lt;/a&gt;, and that’s great. Be warned that if you install it, you will lose all of your installed packages. How should you reinstall them?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First,&lt;/strong&gt; there’s something to be said for installing as you go rather than inheriting all of your packages from your past self. A major upgrade can be a good excuse to clean house, trim unnecessary stuff from your computer, and &lt;a href=&#34;https://twitter.com/hadleywickham/status/1254387031842701312&#34;&gt;install packages from scratch&lt;/a&gt;. As much as I like to take this approach myself, I have to teach next week, so I want my R environment established quickly.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you don’t particularly want to install-as-you-go, one way to revitalize your R environment is to &lt;em&gt;record which packages you have installed&lt;/em&gt; and automate their re-installation with some code. You can find several helpful online guides that &lt;a href=&#34;https://rstats.wtf/maintaining-r.html#how-to-transfer-your-library-when-updating-r&#34;&gt;walk through the main idea&lt;/a&gt;: save the names of installed packages as a vector, and then pass these names to &lt;code&gt;install.packages()&lt;/code&gt; to do a batch installation from CRAN.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But what about the packages that don’t live on CRAN?&lt;/strong&gt; My experiments with R have led me to install several packages from Github: maybe they were “development versions” that had yet to be published on CRAN, or they will never be on CRAN due to policy incompatibilities or the wishes of the package developer. (See Reason #5 in &lt;a href=&#34;https://twitter.com/hadleywickham/status/1254421747014737920&#34;&gt;this tweet&lt;/a&gt;.) How can we automate the reinstall process when these packages have different online sources? This post walks through a process for doing that, adapted from this &lt;a href=&#34;https://gist.github.com/mikedecr/6fd2e7855c71e4358534d5af3b72a03d&#34;&gt;Gist&lt;/a&gt; that I &lt;a href=&#34;https://twitter.com/mikedecr/status/1254152352241782787&#34;&gt;shared on Twitter&lt;/a&gt;. I adapted the whole thing to go into a little &lt;a href=&#34;https://github.com/mikedecr/update-R&#34;&gt;Github repository&lt;/a&gt; if you’d like to fork/clone that instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roadmap&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Roadmap&lt;/h1&gt;
&lt;p&gt;The routine has the following main ideas.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What packages have I installed, and where did they come from?&lt;/li&gt;
&lt;li&gt;Check my local package versions against the CRAN versions. Are the most recent versions on CRAN, or was I using a Github version that I should keep using?&lt;/li&gt;
&lt;li&gt;Render unto Caesar: install the packages from CRAN that makes sense to get from CRAN, but install the packages from Github that makes sense to get from Github.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-already-installed&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is already installed?&lt;/h1&gt;
&lt;p&gt;First we collect information on the packages we already have installed. I am working out of a project directory that manages all of my R updating business, so I &lt;a href=&#34;https://rstats.wtf/safe-paths.html&#34;&gt;use the &lt;code&gt;{here}&lt;/code&gt; package&lt;/a&gt; consistent with a &lt;a href=&#34;https://rstats.wtf/project-oriented-workflow.html&#34;&gt;project-oriented R workflow&lt;/a&gt;. We will also use tidyverse-style data manipulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;here&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)
## Warning: package &amp;#39;tibble&amp;#39; was built under R version 4.0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;installed.packages()&lt;/code&gt; function returns a table of package information for all packages in your library. I convert this to a tibble to make things easier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data frame of all installed packages
local_pkgs &amp;lt;- installed.packages() %&amp;gt;%
  as_tibble() %&amp;gt;%
  print()
## # A tibble: 392 x 16
##    Package LibPath Version Priority Depends Imports LinkingTo Suggests Enhances
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   
##  1 abind   /Libra… 1.4-5   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… method… &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;    
##  2 adagio  /Libra… 0.7.1   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… graphi… &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;    
##  3 ade4    /Libra… 1.7-15  &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… graphi… &amp;lt;NA&amp;gt;      &amp;quot;ade4Tk… &amp;lt;NA&amp;gt;    
##  4 AER     /Libra… 1.2-9   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… stats,… &amp;lt;NA&amp;gt;      &amp;quot;boot, … &amp;lt;NA&amp;gt;    
##  5 Amelia  /Libra… 1.7.6   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… foreig… Rcpp (&amp;gt;=… &amp;quot;tcltk,… &amp;lt;NA&amp;gt;    
##  6 arrayh… /Libra… 1.1-0   &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;   method… &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;    
##  7 askpass /Libra… 1.1     &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;   sys (&amp;gt;… &amp;lt;NA&amp;gt;      &amp;quot;testth… &amp;lt;NA&amp;gt;    
##  8 assert… /Libra… 0.2.1   &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;   tools   &amp;lt;NA&amp;gt;      &amp;quot;testth… &amp;lt;NA&amp;gt;    
##  9 audio   /Libra… 0.1-7   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;    
## 10 backpo… /Libra… 1.1.7   &amp;lt;NA&amp;gt;     &amp;quot;R (&amp;gt;=… utils   &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;    &amp;lt;NA&amp;gt;    
## # … with 382 more rows, and 7 more variables: License &amp;lt;chr&amp;gt;,
## #   License_is_FOSS &amp;lt;chr&amp;gt;, License_restricts_use &amp;lt;chr&amp;gt;, OS_type &amp;lt;chr&amp;gt;,
## #   MD5sum &amp;lt;chr&amp;gt;, NeedsCompilation &amp;lt;chr&amp;gt;, Built &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a start, but I also want to know if I got these packages from CRAN or from Github. I can do this with &lt;code&gt;sessioninfo::package_info()&lt;/code&gt;, passing a vector of package names to the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get source details (cran, github...) from package_info()
local_details &amp;lt;- 
  sessioninfo::package_info(pkgs = local_pkgs$Package) %&amp;gt;%
  as_tibble() %&amp;gt;%
  select(package, local_version = ondiskversion, source) %&amp;gt;%
  print()
## # A tibble: 378 x 3
##    package      local_version source        
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;         
##  1 abind        1.4-5         CRAN (R 4.0.0)
##  2 adagio       0.7.1         CRAN (R 4.0.0)
##  3 ade4         1.7-15        CRAN (R 4.0.0)
##  4 AER          1.2-9         CRAN (R 4.0.0)
##  5 Amelia       1.7.6         CRAN (R 4.0.0)
##  6 arrayhelpers 1.1-0         CRAN (R 4.0.0)
##  7 askpass      1.1           CRAN (R 4.0.0)
##  8 assertthat   0.2.1         CRAN (R 4.0.0)
##  9 audio        0.1-7         CRAN (R 4.0.0)
## 10 backports    1.1.7         CRAN (R 4.0.0)
## # … with 368 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this new table has 14 fewer rows. That’s because &lt;code&gt;sessioninfo::package_info()&lt;/code&gt; isn’t returning the base packages that show up in &lt;code&gt;installed.packages()&lt;/code&gt;. That’s fine, since those will come with R 4.0 anyway.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;source&lt;/code&gt; column in this new table shows us what we want to know. For instance, let’s look at packages that I have from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(local_details, str_detect(source, &amp;quot;Github&amp;quot;))
## # A tibble: 8 x 3
##   package       local_version source                                   
##   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;                                    
## 1 colorout      1.2-2         Github (jalvesaq/colorout@726d681)       
## 2 emo           0.0.0.9000    Github (hadley/emo@3f03b11)              
## 3 ggkeyboard    0.0.0.9009    Github (sharlagelfand/ggkeyboard@b1a965d)
## 4 mRkov         0.0.0.9000    Github (serrat839/mRkov@0f520e8)         
## 5 rethinking    2.00          Github (rmcelreath/rethinking@f393f30)   
## 6 Statamarkdown 0.4.5         Github (hemken/Statamarkdown@506cfc9)    
## 7 texreg        1.36.28       Github (leifeld/texreg@c1da5c8)          
## 8 waffle        1.0.1         Github (hrbrmstr/waffle@3f61463)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;determining-install-source-by-comparing-package-versions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Determining install source by comparing package versions&lt;/h1&gt;
&lt;p&gt;Before you update R, you may be using packages installed from Github, even if those packages are also on CRAN. We want to compare our locally installed package versions against the versions on CRAN. If the CRAN versions are more recent, we can go ahead and get those packages from CRAN. If the Github versions are still the most recent (or the &lt;em&gt;only&lt;/em&gt;) versions of some packages, we want to get them from Github.&lt;/p&gt;
&lt;p&gt;We will want to get a table of data on CRAN package versions. The &lt;code&gt;available.packages()&lt;/code&gt; function returns info for &lt;strong&gt;all&lt;/strong&gt; packages on CRAN.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# available.packages() returns pkg info for ALL pkgs on CRAN.
cran_pkgs &amp;lt;- available.packages() %&amp;gt;% 
  as_tibble(.name_repair = tolower) %&amp;gt;%
  print()
## # A tibble: 16,578 x 17
##    package version priority depends imports linkingto suggests enhances license
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  
##  1 A3      1.0.0   &amp;lt;NA&amp;gt;     R (&amp;gt;= …  &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;      randomF… &amp;lt;NA&amp;gt;     GPL (&amp;gt;…
##  2 aaSEA   1.1.0   &amp;lt;NA&amp;gt;     R(&amp;gt;= 3… &amp;quot;DT(&amp;gt;=… &amp;lt;NA&amp;gt;      knitr, … &amp;lt;NA&amp;gt;     GPL-3  
##  3 AATtoo… 0.0.1   &amp;lt;NA&amp;gt;     R (&amp;gt;= … &amp;quot;magri… &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     GPL-3  
##  4 ABACUS  1.0.0   &amp;lt;NA&amp;gt;     R (&amp;gt;= … &amp;quot;ggplo… &amp;lt;NA&amp;gt;      rmarkdo… &amp;lt;NA&amp;gt;     GPL-3  
##  5 abbyyR  0.5.5   &amp;lt;NA&amp;gt;     R (&amp;gt;= … &amp;quot;httr,… &amp;lt;NA&amp;gt;      testtha… &amp;lt;NA&amp;gt;     MIT + …
##  6 abc     2.1     &amp;lt;NA&amp;gt;     R (&amp;gt;= …  &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     GPL (&amp;gt;…
##  7 abc.da… 1.0     &amp;lt;NA&amp;gt;     R (&amp;gt;= …  &amp;lt;NA&amp;gt;   &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     GPL (&amp;gt;…
##  8 ABC.RAP 0.9.0   &amp;lt;NA&amp;gt;     R (&amp;gt;= … &amp;quot;graph… &amp;lt;NA&amp;gt;      knitr, … &amp;lt;NA&amp;gt;     GPL-3  
##  9 abcADM  1.0     &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;    &amp;quot;Rcpp … Rcpp, BH  &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     GPL-3  
## 10 ABCana… 1.2.1   &amp;lt;NA&amp;gt;     R (&amp;gt;= … &amp;quot;plotr… &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;     GPL-3  
## # … with 16,568 more rows, and 8 more variables: license_is_foss &amp;lt;chr&amp;gt;,
## #   license_restricts_use &amp;lt;chr&amp;gt;, os_type &amp;lt;chr&amp;gt;, archs &amp;lt;chr&amp;gt;, md5sum &amp;lt;chr&amp;gt;,
## #   needscompilation &amp;lt;chr&amp;gt;, file &amp;lt;chr&amp;gt;, repository &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We only care about the packages in this table that we have already installed, so we will narrow the table down using a join.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;slimmer_frame &amp;lt;- 
  left_join(
    x = select(local_details, package, local_version, source),
    y = select(cran_pkgs, package, cran_version = version)
  ) %&amp;gt;%
  print()
## # A tibble: 378 x 4
##    package      local_version source         cran_version
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;       
##  1 abind        1.4-5         CRAN (R 4.0.0) 1.4-5       
##  2 adagio       0.7.1         CRAN (R 4.0.0) 0.7.1       
##  3 ade4         1.7-15        CRAN (R 4.0.0) 1.7-16      
##  4 AER          1.2-9         CRAN (R 4.0.0) 1.2-9       
##  5 Amelia       1.7.6         CRAN (R 4.0.0) 1.7.6       
##  6 arrayhelpers 1.1-0         CRAN (R 4.0.0) 1.1-0       
##  7 askpass      1.1           CRAN (R 4.0.0) 1.1         
##  8 assertthat   0.2.1         CRAN (R 4.0.0) 0.2.1       
##  9 audio        0.1-7         CRAN (R 4.0.0) 0.1-7       
## 10 backports    1.1.7         CRAN (R 4.0.0) 1.2.0       
## # … with 368 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using this slimmer table, we categorize the sources of these packages and where we want to reinstall them from. Here is the basic idea: &lt;strong&gt;We want to install from Github only if our local Github version is more recent than the CRAN version.&lt;/strong&gt; This also applies when there is no version of a package on CRAN.&lt;/p&gt;
&lt;p&gt;Stated another way, we install a package from CRAN in any case that the CRAN version is more recent than the local version. This is true even if the local version was installed from Github! Remember, we don’t install from Github simply because we did so in the past. We install from Github if there is no better choice.&lt;/p&gt;
&lt;p&gt;There are edge cases to be aware of: we may find that the CRAN version of a package is behind our local version, &lt;em&gt;even if&lt;/em&gt; the local version was installed from CRAN. This happens for (at least) two reasons: if a package version was reverted on CRAN (which appeared to happen in the case of &lt;a href=&#34;https://github.com/cran/StanHeaders/commit/2294a66cb1876568b6af74a6c4a1233bf6b6e00f&#34;&gt;StanHeaders&lt;/a&gt;), or if the package is currently unavailable for installation from CRAN (due to some incompatibility, perhaps).&lt;/p&gt;
&lt;p&gt;The code below does this categorization using the &lt;code&gt;utils::compareVersion()&lt;/code&gt; function, which interprets the version numbers so we don’t have to. We do an additional step to note the Github repostory for any package that we still want to obtain from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compare_frame &amp;lt;- slimmer_frame %&amp;gt;%
  group_by(package) %&amp;gt;% 
  mutate(
    source_locale = case_when(
      compareVersion(local_version, cran_version) == 1 &amp;amp;
        str_detect(source, &amp;quot;Github&amp;quot;) ~ &amp;quot;Github&amp;quot;,
      compareVersion(local_version, cran_version) == 1 &amp;amp;
        is.na(cran_version) &amp;amp;
        str_detect(source, &amp;quot;CRAN&amp;quot;) ~ &amp;quot;Unavailable on CRAN&amp;quot;,
      compareVersion(local_version, cran_version) == 1 &amp;amp;
        (is.na(cran_version) == FALSE) &amp;amp;
        str_detect(source, &amp;quot;CRAN&amp;quot;) ~ &amp;quot;Downgraded on CRAN&amp;quot;,
      compareVersion(local_version, cran_version) %in% c(-1, 0) ~ &amp;quot;CRAN&amp;quot;
    ),
    github_repo = case_when(
      source_locale == &amp;quot;Github&amp;quot; ~ 
        str_split(string = source, pattern = &amp;quot;@&amp;quot;, simplify = TRUE)[,1] %&amp;gt;%
        str_replace(&amp;quot;Github \\(&amp;quot;, &amp;quot;&amp;quot;),
      TRUE ~ as.character(NA)
    ),
  ) %&amp;gt;%
  ungroup() %&amp;gt;%
  print()
## # A tibble: 378 x 6
##    package     local_version source       cran_version source_locale github_repo
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;      
##  1 abind       1.4-5         CRAN (R 4.0… 1.4-5        CRAN          &amp;lt;NA&amp;gt;       
##  2 adagio      0.7.1         CRAN (R 4.0… 0.7.1        CRAN          &amp;lt;NA&amp;gt;       
##  3 ade4        1.7-15        CRAN (R 4.0… 1.7-16       CRAN          &amp;lt;NA&amp;gt;       
##  4 AER         1.2-9         CRAN (R 4.0… 1.2-9        CRAN          &amp;lt;NA&amp;gt;       
##  5 Amelia      1.7.6         CRAN (R 4.0… 1.7.6        CRAN          &amp;lt;NA&amp;gt;       
##  6 arrayhelpe… 1.1-0         CRAN (R 4.0… 1.1-0        CRAN          &amp;lt;NA&amp;gt;       
##  7 askpass     1.1           CRAN (R 4.0… 1.1          CRAN          &amp;lt;NA&amp;gt;       
##  8 assertthat  0.2.1         CRAN (R 4.0… 0.2.1        CRAN          &amp;lt;NA&amp;gt;       
##  9 audio       0.1-7         CRAN (R 4.0… 0.1-7        CRAN          &amp;lt;NA&amp;gt;       
## 10 backports   1.1.7         CRAN (R 4.0… 1.2.0        CRAN          &amp;lt;NA&amp;gt;       
## # … with 368 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;updating-r-and-reinstalling-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Updating R and reinstalling packages&lt;/h1&gt;
&lt;p&gt;When we are satisfied with our decisions about where to install a package from, save this comparison table to file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# output data location
dir.create(here(&amp;quot;data&amp;quot;))

# output file
out_file &amp;lt;- as.character(str_glue(&amp;quot;pkg-data_{Sys.Date()}.rds&amp;quot;))
write_rds(compare_frame, here(&amp;quot;data&amp;quot;, out_file))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After updating, we reopen R and install our packages according to our classifications. We want to begin by installing &lt;code&gt;{remotes}&lt;/code&gt; to enable installation from Github.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to install from github
install.packages(&amp;quot;remotes&amp;quot;)

# should still be operating in your working directory
# so downloading {here} makes sense also
install.packages(&amp;quot;here&amp;quot;)

# read package data
pkgs &amp;lt;- readRDS(here::here(&amp;quot;data&amp;quot;, &amp;quot;pkg-data_2020-04-25.rds&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything that we want to install from Github, we install by iterating &lt;code&gt;remotes::install_github&lt;/code&gt; over the github repository slugs that we saved previously. This requires us to write the code in “hard mode” because we aren’t using tidyverse dialect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from github repos
github_pkgs &amp;lt;- pkgs[pkgs$source_locale == &amp;quot;Github&amp;quot;, ][[&amp;quot;github_repo&amp;quot;]]

remotes::install_github(github_pkgs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, everything that we said we wouldn’t get from Github, we get by using plain ol’ &lt;code&gt;install.packages()&lt;/code&gt;. Before doing this, you may find it beneficial to filter out some of the packages that you don’t use anymore or that maybe we only installed as dependencies for other packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install from cran with remaining package names
cran_pkgs &amp;lt;- pkgs[pkgs$source_locale != &amp;quot;Github&amp;quot;, ][[&amp;quot;package&amp;quot;]]

install.packages(cran_pkgs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should get you (more or less) up and running with R 4.0 and all of your old packages.&lt;/p&gt;
&lt;p&gt;Fair warning: I’m already getting burned by some C++ configuration problems for packages that want to compile from source. I think this is particular to my own computer and the klugey fixes I undertook to set up &lt;code&gt;{rstan}&lt;/code&gt; with MacOS Catalina. I remember reading somewhere that R 4.0 fixed some of the Stan x Catalina problems, so maybe I will confront these choices again soon, but I will cross that bridge when I get to it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;
And maybe I will do a fresh install of &lt;em&gt;everything&lt;/em&gt; once the semester is over.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting What Matters</title>
      <link>/post/visualizing-what-matters/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-what-matters/</guid>
      <description>


&lt;div id=&#34;this-is-a-post-about-temptation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;This is a post about temptation&lt;/h2&gt;
&lt;p&gt;Or, resisting temptations when presenting statistical results.&lt;/p&gt;
&lt;p&gt;When you build a model to answer a question, it is often tempting to make a graphic about the &lt;em&gt;coolest thing about the model&lt;/em&gt;. Maybe you learned something new to build the model, or you noticed and corrected an important structure in the data, so naturally you want to show off your good work. The purpose of this post is to &lt;strong&gt;reflect on why this practice isn’t useful for communicating statistical results&lt;/strong&gt;. Instead, we should be communicating the information that will help the audience grasp the important takeaways of the analysis. A different focus entirely.&lt;/p&gt;
&lt;p&gt;This is almost too obvious, but it is easier said than done. Researchers wrestle with it in different ways based on our audiences, our professional goals, and (to be honest) our insecurities. Speaking for myself, I need to grapple with my biases and how they manifest in my work product. As a PhD student, I sometimes feel like academia cultivates incentives to convince our colleagues that we are Very Smart&lt;sup&gt;TM&lt;/sup&gt;, which is a distinct goal from doing good work (however defined). Cool graphics can be a way to show how much thought and work we put into something—a way of signaling that we belong. Understandable, but not always useful. I also like to use Bayesian methods, but I feel constant pressure to justify the Bayesian approach to audiences that I (rightly or wrongly) assume will be hostile to that choice. As a result, I feel tempted to plot something that would be impossible without Bayes—a way of saying, “Get off my back!” as if it ultimately mattered for what I’m trying to communicate with my analysis overall. Sometimes it does matter, but the way that it matters won’t be so simple as “just plot the flashy thing.”&lt;/p&gt;
&lt;p&gt;This post unpacks this using a recent example from a grad methods course that I am TA’ing. The assignment requires students to write a policy memo informed by a statistical analysis. The statistical model contains an important component that students are learning about in the course, but that component &lt;strong&gt;isn’t actually important to communicate in the policy memo&lt;/strong&gt;. So the assignment challenges students both on model-building but also communication skills: exercising restraint and judgment about what is and is not important to communicate about the details of the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The setting&lt;/h2&gt;
&lt;p&gt;The assignment for students to complete lays out the following scenario.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are an advisor to a newly elected mayor of Smallville. During the campaign, the mayor-elect charged that the Sanitation Department was being grossly mismanaged. Last year it cost &lt;span class=&#34;math inline&#34;&gt;\(\$ 48.50\)&lt;/span&gt; per household for once-a-week curbside waste pick-up. A private contractor has made an informal bid of &lt;span class=&#34;math inline&#34;&gt;\(\$ 40.60\)&lt;/span&gt; per household for collection services, but this would require eliminating Sanitation Department jobs, which would be difficult and politically costly. Before switching to private contracting, the mayor would like to know how much costs might be reduced with the appointment of a more competent Sanitation Department supervisor.&lt;/p&gt;
&lt;p&gt;Prepare a memorandum to the mayor advising her about the potential gains from better management. The mayor has had little statistical training, so be sure to explain your empirical work clearly and carefully.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Students are given a dataset of 30 other municipalities in the region, simulated from a model that they don’t directly see.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 5
##    hholds density  wage snowdays cost_per_household
##     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;
##  1   3.35    565.  18.2        3               29.4
##  2  11.2     740.  15.7       10               52.0
##  3   9.48    540.  17.4        3               34.4
##  4   9.43    629.  19.6        1               41.5
##  5  11.3     685.  20.6        6               63.4
##  6   6.18    605.  20.2        6               30.0
##  7   2.82    510.  16.5        4               20.8
##  8   2.95    459.  15.8        2               12.9
##  9   6.98    507.  16.5        3               21.8
## 10   7.89    524.  19.9        2               37.8
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables are described as follows.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cost_per_household&lt;/code&gt;: cost per household (&lt;span class=&#34;math inline&#34;&gt;\(\$\)&lt;/span&gt; U.S.) of once weekly curbside refuse pickup for last year.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hholds&lt;/code&gt;: number of households served in 10,000s. &lt;strong&gt;(Note: previous studies suggest refuse collection may involve non-constant returns to scale; that is, there may be some number of households at which the cost per household is minimized; communities with smaller or larger numbers of households have higher costs per household.)&lt;/strong&gt; Value for your city: 6.28.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;density&lt;/code&gt;: density of households per square mile. Value for your city: 620.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wage&lt;/code&gt;: average hourly wage in dollars for collection workers. Value for your city: 19.50.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snowdays&lt;/code&gt;: number of snow emergency days last year; may raise costs by interfering with regular schedule. Value for your city: 5.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;So what’s the objective?&lt;/strong&gt; Students are supposed to use the data to build a predictive (OLS) model for &lt;code&gt;cost_per_household&lt;/code&gt;, and then interpret the model to advise the mayor about the choice between (a) enlisting the private sanitation contractor or (b) replacing the supervisor of the Sanitation Department.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;source-of-temptation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Source of temptation&lt;/h2&gt;
&lt;p&gt;Students typically begin with a simple model where every variable is linearly related to the outcome variable…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathtt{cost\_per\_household}_{i} &amp;amp;= 
    \alpha + 
    \beta_{1} \mathtt{hholds} +
    \beta_{2} \mathtt{density} \\
  &amp;amp; \quad 
    + \beta_{3} \mathtt{wage} +
    \beta_{4} \mathtt{snowdays} +
    \varepsilon_{i}
\end{align}\]&lt;/span&gt;
…with a normally distributed error. But they should find that the simple model violates OLS assumptions by producing residuals with a curvilinear pattern. If students inspect the data more, they detect that the likely culprit is the &lt;code&gt;hholds&lt;/code&gt; variable, the number of households in the municipality. This is consistent with the hint in the variable descriptions above, that there may be some number of households that minimize sanitation costs per household. So they build a model with a quadratic term for &lt;code&gt;hholds&lt;/code&gt;.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathtt{cost\_per\_household}_{i} &amp;amp;= 
    \alpha + 
    \beta_{1} \mathtt{hholds} +
    \beta_{2} \mathtt{hholds}^{2} +
    \beta_{3} \mathtt{density} \\
  &amp;amp; \quad 
    + \beta_{4} \mathtt{wage} +
    \beta_{5} \mathtt{snowdays} +
    \varepsilon_{i}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the purposes of the assignment, this is the “correct” model, and the residuals look better than they did before (see below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/vizualizing-what-matters/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have the appropriate model, how do we visualize it? I’ll tell you what I did when I was a student in this course (many years ago): I plotted the nonlinear relationship between the number of households and the outcome variable. It the most interesting part of the model, and it was hidden in the data…how could it not be the important thing that I should focus on? I think I did something like this: generate model predictions with other variables fixed at their means, and then plot those model predictions alongside my city’s data (Smallville) and the private contractor’s proposal for reducing the curbside pickup costs per household.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/vizualizing-what-matters/index_files/figure-html/wrong-plot-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this graphic, we could reason that even if we enlist the private contractor, their bid does not get us close to what we would expect from the model. This leads us the direction of saying that maybe we expect more savings from better mismanagement of the Sanitation Department than what the current supervisor is delivering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disciplined-plotting-choices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Disciplined plotting choices&lt;/h2&gt;
&lt;p&gt;There are a few things that are misguided about the above approach.&lt;/p&gt;
&lt;p&gt;First, the comparison in this above graphic isn’t actually the relevant comparison. It is typical to teach students to visualize partial relationships in regression by varying one predictor and fixing other predictors to their means. But we don’t always want to compare against a typical observation. In this example, we would be more interested in holding covariates to the same values as we observe for our city. Applied situations like this remind us that many of the problems that we encounter aren’t questions about “typical” observations at all.&lt;/p&gt;
&lt;p&gt;Second, and more importantly, the only reason why we make the previous mistake is because we think that the quadratic relationship is the important thing to visualize. It’s understandable that we’re distracted by the quadratic relationship because it was initially a challenge to discover, but if we discipline ourselves about what is important to communicate to our audience (in this case, the mayor of “Smallville”), we would see that the nonlinearity is irrelevant to visualize. All that matters is comparing our city’s data and the contractor’s proposal to a specific model-based prediction for our city. Exploring how the prediction changes as we arbitrarily assign other values on key covariates doesn’t help us make a policy recommendation. We aren’t in a position to intervene on those variables (and we aren’t confident that our model identifies causal effects anyway), so we shouldn’t distract the mayor by presenting irrelevant information that draws focus away from the key insights.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What is a simpler way to show the model’s key takeaway? Here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/vizualizing-what-matters/index_files/figure-html/plot-smallville-1.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The takeaway from this figure is similar as it was above. We show a model-based prediction for a city with the same observable characteristics as Smallville (a point estimate, 95% confidence interval, and 95% prediction interval) alongside Smallville’s current costs and the contractor’s proposal.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
We see that the contractor’s proposal reduces sanitation costs, but they still get us nowhere near a level that we should expect given the characteristics of our city. If we are confident that replacing the Sanitation Department supervisor would make our costs “representative” of other similar towns, we would save a lot more money by replacing the supervisor than we would by hiring the contractor.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-do-better-visualize-cost-savings-directly&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We can do better: visualize cost-savings directly&lt;/h2&gt;
&lt;p&gt;Even though the above graphic does a little better than what we were working with before, it still doesn’t directly communicate the aggregate financial impact of the policy. The audience has to do that work on their own still. If we really wanted to communicate the insights of the model, we could translate these predictions directly into something that mayors, comptrollers, and so on really understand: dollars.&lt;/p&gt;
&lt;p&gt;Here’s the idea. In terms of annual cost cost-per-household, we know our city’s current value, the contractor’s bid, and a distribution of model-based estimates for a city with our data. We also know the total number of households in our city, so it is straightforward to calculate the total costs from each of these per-household figures:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \text{Total annual cost} &amp;amp;= \text{Cost per household} \times \text{Number of households}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we calculate annual costs for the current per-household rate, the contractor’s bid, and three model-based scenarios&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;
that result from replacing the Sanitation Dept. supervisor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An “average” scenario: plugging in the point estimate from the model as the annual cost per household under a new supervisor.&lt;/li&gt;
&lt;li&gt;An “optimistic” scenario: plugging in the 10th percentile of the predictive distribution as the estimated cost per household.&lt;/li&gt;
&lt;li&gt;A “pessimistic” scenario: plugging in the 90th percentile of the predictive distribution as the annual cost per household.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We plot two quantities for each scenario. First, how much money is saved annually by replacing the supervisor brings pickup costs to each of these benchmarks? And second, how much &lt;em&gt;more&lt;/em&gt; does each benchmark save us when we compare to the private contractor’s bid?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/vizualizing-what-matters/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The comparison against the contractor bid isn’t just for show! It gives decision-makers valuable information because it represents a “budget” for replacing the supervisor. How so? Replacing the supervisor wouldn’t be costless: the replacement may require a higher salary than the current supervisor, and the job search will involve some fixed costs. Comparing each scenario’s savings to the amount saved from the enlisting the contractor conveys how much money we can &lt;em&gt;invest&lt;/em&gt; in a new supervisor while still saving more money than hiring the contractor. Visualizing the results this way shows exactly how much money we’re working with and how much we stand to save by hiring supervisors with different salary expectations.&lt;/p&gt;
&lt;p&gt;By doing some further processing of the model’s insights, and in-turn moving &lt;em&gt;farther&lt;/em&gt; from the technical details of the model, we actually learn more about the consequences of our choices. We see that even under the pessimistic cost-savings scenario, we still have a roughly half-million dollar cushion before the decision to replace the supervisor starts to look like the wrong choice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reframing-what-to-be-proud-of&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reframing what to be “proud” of&lt;/h2&gt;
&lt;p&gt;We feel proud when we build a technologically complex model. It makes us feel valuable when we work through something challenging, so we want to show it off. When we need to hide these technical details in order to communicate the results to non-experts, the choice is painful at first, especially for someone like me who takes a lot of pride in the time and effort that I invest in improving my statistical skills. This is a psychological game that we are playing with ourselves.&lt;/p&gt;
&lt;p&gt;But the “difficulty-level” of our work isn’t the only skill to take pride in. Distilling essential information out of a complicated piece of machinery is valuable, and we can see it as a distinct source of pride when we do it well. Better still, doing an effective job summarizing the important takeaways of an analysis makes it all the more rewarding to describe the technical backend, since the technical backend has a much stronger clarity-of-purpose after if is introduced effectively.&lt;/p&gt;
&lt;p&gt;Just to drive the point home, that last graph was a bar graph. I kinda hate bar graphs. But I don’t hate the fact that I recognized a context where it made something simpler to communicate. That’s valuable.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;
At this point you may wonder if we can intervene on the system by reducing the wages of sanitation workers to make up the cost difference. First of all, how dare you. Secondly, even though we know that lowering wages should have a causal effect on pickup costs, we don’t know that the coefficient for &lt;code&gt;wage&lt;/code&gt; in the model represents the causal effect of wages—certainly it doesn’t. Third, even if we could make that assumption, the numerical impact of decreasing sanitation worker wages to (say) the median wage for other cities would amount to only $5.54 saved per household, so we save even less than we would save from hiring the contractor.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;
The language used in the graphic (“95% range of predictions”) is doing a little violence against the meaning of a frequentist confidence interval. If I were implementing this analysis in real life, I would build a Bayesian model that lets me say, “This is the likely range of scenarios, as the data suggest,” because that’s conveniently what a posterior distribution actually means!&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;
Once again, interpreting the prediction interval as “possible scenarios” is more closely tied to Bayes than to a frequentist model. The more I think about the value of communicating model predictions as “possible scenarios,” the more I think this warrants its own blog post. &lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Git information in your open-source research paper (with Rmarkdown)
</title>
      <link>/post/git-in-papers/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/git-in-papers/</guid>
      <description>


&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;One benefit of open-source research is that it is possible to trace the history of a research product through its (potentially many) iterations using a versioning system such as Git. This is great for readers who encounter the project’s remote repository, but it’s more likely the case that readers will encounter only a PDF of your paper in an email or through a preprint archive. While services like ArXiv will watermark your paper, it (or so it seems) only includes information about the paper’s history in ArXiv specifically, rather its history in your Git repository. This post describes how you can use Rmarkdown to include Git information into a working draft of your research paper.&lt;/p&gt;
&lt;p&gt;What exactly do I mean? Your paper typically includes the date of compilation, but you could also include the current commit hash, the branch of the current commit, and so on. Why would you want to do this?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A compilation system like &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; can print the date of compilation, but it is often the case that documents are re-compiled without any real changes. This means the compilation date can be a deceiving signal about when the paper was most recently modified. You may want to “timestamp” a version of your paper in a way that is robust to re-compilation at an arbitrary future time.&lt;/li&gt;
&lt;li&gt;As you develop your paper locally, you may commit several small changes between major versions of your paper. To prevent your “in-development” copy from being confused for a major version of the paper, you may want to note which commit generated the current PDF and perhaps link to a more stable “for public eyes” version of the paper elsewhere.&lt;/li&gt;
&lt;li&gt;A more general case of the previous point: suppose you develop your project across multiple branches (e.g. as with &lt;a href=&#34;https://datasift.github.io/gitflow/IntroducingGitFlow.html&#34;&gt;“Git flow”&lt;/a&gt;). You may reserve your “master” branch for major versions of the project while iteratively developing the project (and compiling the document) on a non-master branch. In this case, you might want to know if a PDF was compiled from source code on the master branch (i.e. “Am I looking at a major version of the paper”) or on an in-development branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example from one of my in-progress papers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;git-date.png&#34; width=&#34;732&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This setup prioritizes the commit hash over the compilation date as a method for “dating” your paper. The branch name is included in cases where the PDF is generated on a development branch instead of on the master/public branch. The footnote corresponding to the commit information contains the commit message (not shown). And lastly, the link to the public version takes you to the master branch PDF on Github—the most recent major version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-do-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to do it&lt;/h2&gt;
&lt;p&gt;Setting this up consists of essentially two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Learn to print Git commands to the console using R.&lt;/li&gt;
&lt;li&gt;Place that R code in your &lt;code&gt;.Rmd&lt;/code&gt; document’s YAML header.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;console-commands-with-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Console commands with R&lt;/h3&gt;
&lt;p&gt;We can run console commands within R using the &lt;code&gt;system()&lt;/code&gt; function. Ordinarily the results of the commands merely print to the console instead of being treated as objects, but we want to make these objects be accessible in the R environment using the &lt;code&gt;intern = TRUE&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Here are some examples that will display Git information for my website repo (where this code is currently being evaluated).&lt;/p&gt;
&lt;p&gt;For instance, how can we print the branch name?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;git symbolic-ref --short HEAD&amp;quot;, intern = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;master&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print only the hashes in your Git log, you can supply &lt;code&gt;%t&lt;/code&gt; to the the &lt;code&gt;--pretty&lt;/code&gt; argument of &lt;code&gt;git log&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;git log --pretty=%t&amp;quot;, intern = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;7ee9385&amp;quot; &amp;quot;2441a5b&amp;quot; &amp;quot;d17d13e&amp;quot; &amp;quot;04293bd&amp;quot; &amp;quot;d9c823e&amp;quot; &amp;quot;c5fcaa0&amp;quot; &amp;quot;d3b381a&amp;quot;
##   [8] &amp;quot;ad7ee18&amp;quot; &amp;quot;c5fcaa0&amp;quot; &amp;quot;533e975&amp;quot; &amp;quot;2e7c7a8&amp;quot; &amp;quot;eaee307&amp;quot; &amp;quot;43bfc74&amp;quot; &amp;quot;1856a0b&amp;quot;
##  [15] &amp;quot;5823e01&amp;quot; &amp;quot;66e52d7&amp;quot; &amp;quot;0f043a4&amp;quot; &amp;quot;af6087c&amp;quot; &amp;quot;56bc096&amp;quot; &amp;quot;ebd98df&amp;quot; &amp;quot;3074778&amp;quot;
##  [22] &amp;quot;94bcada&amp;quot; &amp;quot;fa8f4fa&amp;quot; &amp;quot;1079fd7&amp;quot; &amp;quot;1a3553b&amp;quot; &amp;quot;f352285&amp;quot; &amp;quot;0e7c4b4&amp;quot; &amp;quot;3be8c27&amp;quot;
##  [29] &amp;quot;2a80372&amp;quot; &amp;quot;950065a&amp;quot; &amp;quot;b8d24f2&amp;quot; &amp;quot;9da4ece&amp;quot; &amp;quot;8cccbf9&amp;quot; &amp;quot;45d0315&amp;quot; &amp;quot;9d26ba5&amp;quot;
##  [36] &amp;quot;6ebb3b0&amp;quot; &amp;quot;af8a617&amp;quot; &amp;quot;2c6ec85&amp;quot; &amp;quot;22fea55&amp;quot; &amp;quot;3854af2&amp;quot; &amp;quot;93d13c4&amp;quot; &amp;quot;b297079&amp;quot;
##  [43] &amp;quot;f8b6836&amp;quot; &amp;quot;d988f5c&amp;quot; &amp;quot;6fc4555&amp;quot; &amp;quot;eabf878&amp;quot; &amp;quot;d6bf55f&amp;quot; &amp;quot;ecc0c17&amp;quot; &amp;quot;322d6d8&amp;quot;
##  [50] &amp;quot;b204b83&amp;quot; &amp;quot;604a055&amp;quot; &amp;quot;5b6cd16&amp;quot; &amp;quot;7f3d4e8&amp;quot; &amp;quot;3e3d9d1&amp;quot; &amp;quot;c5c2a6b&amp;quot; &amp;quot;72ecc5a&amp;quot;
##  [57] &amp;quot;bd25ad7&amp;quot; &amp;quot;2820840&amp;quot; &amp;quot;f8be89c&amp;quot; &amp;quot;5011495&amp;quot; &amp;quot;b4f159a&amp;quot; &amp;quot;471d45e&amp;quot; &amp;quot;32e03b8&amp;quot;
##  [64] &amp;quot;d55b641&amp;quot; &amp;quot;175df3e&amp;quot; &amp;quot;03985bd&amp;quot; &amp;quot;549e2f0&amp;quot; &amp;quot;8effeb6&amp;quot; &amp;quot;e7c1fc3&amp;quot; &amp;quot;19f3bcd&amp;quot;
##  [71] &amp;quot;0647521&amp;quot; &amp;quot;5913357&amp;quot; &amp;quot;b146ac2&amp;quot; &amp;quot;494f860&amp;quot; &amp;quot;557bf2a&amp;quot; &amp;quot;2b367c7&amp;quot; &amp;quot;734e099&amp;quot;
##  [78] &amp;quot;8ef25d4&amp;quot; &amp;quot;1d949ce&amp;quot; &amp;quot;ed14db3&amp;quot; &amp;quot;ba4694c&amp;quot; &amp;quot;57d5fc6&amp;quot; &amp;quot;1656482&amp;quot; &amp;quot;28d68d7&amp;quot;
##  [85] &amp;quot;5b8e92a&amp;quot; &amp;quot;a807aab&amp;quot; &amp;quot;359f06a&amp;quot; &amp;quot;78c3ee3&amp;quot; &amp;quot;defc14f&amp;quot; &amp;quot;ec7e081&amp;quot; &amp;quot;e4c9176&amp;quot;
##  [92] &amp;quot;ab502db&amp;quot; &amp;quot;7fe3ee6&amp;quot; &amp;quot;2f97534&amp;quot; &amp;quot;3259f27&amp;quot; &amp;quot;bec13bd&amp;quot; &amp;quot;f3142cc&amp;quot; &amp;quot;2959bf6&amp;quot;
##  [99] &amp;quot;b4754c2&amp;quot; &amp;quot;91fe96a&amp;quot; &amp;quot;91bba9b&amp;quot; &amp;quot;071d153&amp;quot; &amp;quot;8e4cce3&amp;quot; &amp;quot;ba09b95&amp;quot; &amp;quot;741632b&amp;quot;
## [106] &amp;quot;3569cdc&amp;quot; &amp;quot;d99c163&amp;quot; &amp;quot;5c135e3&amp;quot; &amp;quot;2671a4b&amp;quot; &amp;quot;2b7d810&amp;quot; &amp;quot;ea7d44d&amp;quot; &amp;quot;6c7656c&amp;quot;
## [113] &amp;quot;e40d5d8&amp;quot; &amp;quot;bb9199d&amp;quot; &amp;quot;ca4e593&amp;quot; &amp;quot;c42c33f&amp;quot; &amp;quot;d17291e&amp;quot; &amp;quot;38d1910&amp;quot; &amp;quot;6bc2299&amp;quot;
## [120] &amp;quot;3131d9d&amp;quot; &amp;quot;5906234&amp;quot; &amp;quot;d355f02&amp;quot; &amp;quot;7a6e215&amp;quot; &amp;quot;c5befba&amp;quot; &amp;quot;b0dba1c&amp;quot; &amp;quot;c1d6342&amp;quot;
## [127] &amp;quot;87f3ceb&amp;quot; &amp;quot;83ca75b&amp;quot; &amp;quot;69e41cf&amp;quot; &amp;quot;f9278c7&amp;quot; &amp;quot;a3ee86e&amp;quot; &amp;quot;816ebb5&amp;quot; &amp;quot;030278d&amp;quot;
## [134] &amp;quot;2d9384b&amp;quot; &amp;quot;fec8391&amp;quot; &amp;quot;83dbb8c&amp;quot; &amp;quot;1210553&amp;quot; &amp;quot;ce35ec0&amp;quot; &amp;quot;ab3c776&amp;quot; &amp;quot;c62ad9f&amp;quot;
## [141] &amp;quot;3148687&amp;quot; &amp;quot;c3621d8&amp;quot; &amp;quot;943687e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use indexing to isolate only the most recent hash from this vector of results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;git log --pretty=%t&amp;quot;, intern = TRUE)[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;7ee9385&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the commit message, use &lt;code&gt;--pretty=%s&lt;/code&gt; instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;git log --pretty=%s&amp;quot;, intern = TRUE)[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;home page: no more skills, add interests/ed, new bio &amp;amp; abt page&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;r-results-in-the-yaml&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R results in the YAML&lt;/h3&gt;
&lt;p&gt;Now that we know which commands to run to get the Git info, how do we get this information into our YAML? We will do this using inline R code chunks. This code block shows what I’ve done for the above paper example, and I describe a few of the tricks I use below.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;date: |
  | Commit \texttt{`r system(&amp;quot;git log --pretty=%t&amp;quot;, intern = TRUE)[1]`} on branch \texttt{`r system(&amp;quot;git symbolic-ref --short HEAD&amp;quot;, intern = TRUE)`}\footnote{Commit message: \texttt{`r system(&amp;quot;git log --pretty=%s&amp;quot;, intern = TRUE)[1]`}}
  | Compiled `r format(Sys.time(), &amp;#39;%B %d, %Y&amp;#39;)`
  | Most recent online version [here](https://github.com/mikedecr/causal-bayes/blob/master/writing/causal-bayes-paper.pdf).&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We use the &lt;code&gt;date&lt;/code&gt; variable, but we supply multiple lines of content. To do this, place a pipe &lt;code&gt;|&lt;/code&gt; after declaring the &lt;code&gt;date&lt;/code&gt; variable, and begin each line with a new pipe &lt;code&gt;|&lt;/code&gt;. This will line-break the content in your compiled PDF and let you supply &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; code directly to the variable.&lt;/li&gt;
&lt;li&gt;To use teletype/fixed-width font, type the &lt;code&gt;\texttt{}&lt;/code&gt; command for &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{\LaTeX}\)&lt;/span&gt; directly in Rmarkdown.&lt;/li&gt;
&lt;li&gt;We can evaluate and print the results of inline R code by including the letter &lt;code&gt;r&lt;/code&gt; at the beginning of an inline code chunk (delimited by backticks). This code is evaluated before the document is compiled, so the information being passed to &lt;code&gt;\texttt{}&lt;/code&gt; is the &lt;em&gt;results&lt;/em&gt; of the R code rather than the text of the R code itself.&lt;/li&gt;
&lt;li&gt;Do the same basic setup for the commit hash, commit message (in a footnote), and the compilation date. Note that the formatting of the compilation date gives you prettier results than the Rmarkdown default.&lt;/li&gt;
&lt;li&gt;Lastly, you can link the reader to the most recent public PDF by linking to your remote master branch. By linking directly to Github (or wherever else you host the remote repository), any time you push an update to remote, your PDF will automatically be up to date. This will be true of any offline PDF, any previous PDF, and any PDF generated on any branch. This is because the URL to your master branch PDF will not change even if the PDF file itself changes!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;caveat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveat&lt;/h2&gt;
&lt;p&gt;When you push to Github, it creates new hashes that differ from your local machine.
As a result, you can’t use the hash in the PDF to cross-reference the same hash on Github.
This is a shortcoming of the approach, and if I think of a feasible way around it, I will update this post or write a new post altogether.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Visualization of Partial Effects in Multiple Regression</title>
      <link>/post/viz-partials/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/viz-partials/</guid>
      <description>


&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;A few days ago, &lt;a href=&#34;https://www.andrewheiss.com/&#34;&gt;Andrew Heiss&lt;/a&gt; was &lt;a href=&#34;https://twitter.com/andrewheiss/status/1052232993723494400?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1052232993723494400&amp;amp;ref_url=http%3A%2F%2F127.0.0.1%3A4321%2F2018%2F2018-10-19-partialling-out%2F&#34;&gt;looking&lt;/a&gt; for a way to visualize multiple regression with an emphasis on one predictor, without 3(+)-dimensional plots. He works through a method and posts this cool animation, which shows the changing relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; when adding controls, superimposed over the scatterplot of the raw data. (He credits &lt;a href=&#34;https://twitter.com/petemohanty&#34;&gt;Pete Mohanty&lt;/a&gt; with the shifting abline idea.)&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Helpful animated &lt;a href=&#34;https://twitter.com/hashtag/dataviz?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#dataviz&lt;/a&gt; showing what happens to the slope of one coefficient in a model when controlling for other variables in multiple regression&lt;br&gt;&lt;br&gt;(&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; code: &lt;a href=&#34;https://t.co/yhVLj325Oh&#34;&gt;https://t.co/yhVLj325Oh&lt;/a&gt;) &lt;a href=&#34;https://t.co/2foYfXDo28&#34;&gt;pic.twitter.com/2foYfXDo28&lt;/a&gt;
&lt;/p&gt;
— 🎃 Andrew Heiss, scary PhD 🦇 (&lt;span class=&#34;citation&#34;&gt;@andrewheiss&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/andrewheiss/status/1052978108255498240?ref_src=twsrc%5Etfw&#34;&gt;October 18, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;This is cool, but based on Andrew’s initial question, I had something a little different come to mind. I thought we’d be seeing the impact of the regression in both the regression line &lt;em&gt;and&lt;/em&gt; in the data. So I tried to make that (starting with &lt;a href=&#34;https://t.co/yhVLj325Oh&#34;&gt;his code&lt;/a&gt;)…&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Ok fixed. &lt;br&gt;&lt;br&gt;In this fig, y is (beta * humidity), plus the regression residual. This is equivalent to starting with the fully estimated regression and subtracting out terms for every other covariate &lt;a href=&#34;https://t.co/fLs4WxHTaK&#34;&gt;pic.twitter.com/fLs4WxHTaK&lt;/a&gt;
&lt;/p&gt;
— Michael DeCrescenzo (&lt;span class=&#34;citation&#34;&gt;@mikedecr&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/mikedecr/status/1053028075170975744?ref_src=twsrc%5Etfw&#34;&gt;October 18, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;which he &lt;a href=&#34;https://twitter.com/andrewheiss/status/1053031575770718208&#34;&gt;liked&lt;/a&gt; and asked to see the code for.&lt;/p&gt;
&lt;p&gt;So I will deliver. &lt;a href=&#34;https://gist.github.com/mikedecr/f6ffdb716d62af32e701f95231f00bee&#34;&gt;Here&lt;/a&gt; is a gist containing an example, and below is some explanation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intuition&lt;/h1&gt;
&lt;p&gt;Some math will help. Let’s start by writing the regression equation to suit the task at hand: although we include multiple predictors, we only want to highlight one of them, putting the other predictors into a black box “vector of controls.” Andrew’s example uses Dark Sky data on weather in Provo, UT, highlighting the relationship between humidity and a daily temperature high for each day &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathit{HighTemp}_{i} &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \mathbf{x}_{i}^{T}\gamma + \varepsilon_{i}
\end{align}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is the constant, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt; is a column-vector of covariate observations for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (everything but humidity), and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a vector of coefficients for all non-humidity predictors.&lt;/p&gt;
&lt;p&gt;Operationally, what we want to do is show how &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; changes with the inclusion of additional controls. Andrew’s example shows this by plotting different regressions overtop the raw data. If we run the code&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; from his &lt;a href=&#34;https://gist.github.com/andrewheiss/5e162c836575721d1dd53ec2af38753c&#34;&gt;Gist&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/partialling-out/index_files/figure-html/andrew-animate-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The line being plotted starts with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathit{High}}_{i} = {\alpha} + {\beta}(\mathit{Humidity}_{i})\)&lt;/span&gt; and adds additional covariates one at a time. The data remain intact.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variation&lt;/h1&gt;
&lt;p&gt;But let’s say that we wanted to see the effect of controls in the data as well. This is, I think, where the real umph from this kind of visualization would be; after all, we have already told students that including other predictors will affect the line.&lt;/p&gt;
&lt;p&gt;Thinking about the math, this is as easy as doing to the raw data what we’ve already done to the regression line: subtract out the effect of the covariates. That is, purge the effect of other variables from the raw data. Start with the fully specified regression model…
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \mathit{High}_{i} &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \mathbf{x}_{i}^{T}\gamma + \varepsilon_{i}
\end{align}\]&lt;/span&gt;
…and then subtract out the influence of variables in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{i}\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \label{eq:sub} \mathit{High}_{i} - \mathbf{x}^{T}\gamma &amp;amp;= \alpha + \beta\left(\mathit{Humidity}_{i}\right) + \varepsilon_{i}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We still have to decide what to do with the constant. We could…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leave the constant there, which is probably undesirable because the value of the constant reflects the scaling of other covariates.&lt;/li&gt;
&lt;li&gt;Start by setting all covariates equal to their means. This would give us a prediction that is no longer subject to the &lt;em&gt;scaling&lt;/em&gt; of the covariates but the covariates still affect the mean of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; overall. This works but I think we can make it simpler.&lt;/li&gt;
&lt;li&gt;Subtract the constant along with the covariates. This leaves us with only the predicted partial effect of humidity (plus error). This is what we’ll do, because it zooms in only on the predictor that we care about.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Now we will create the revised gif.&lt;/p&gt;
&lt;p&gt;First we start with the original Heiss data and code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ---- Heiss code -----------------------

library(&amp;quot;magrittr&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;lubridate&amp;quot;)
library(&amp;quot;broom&amp;quot;)
library(&amp;quot;scales&amp;quot;)
library(&amp;quot;gganimate&amp;quot;)

# Load and clean data
# This data comes from Dark Sky&amp;#39;s API
weather_provo_raw &amp;lt;- read_csv(&amp;quot;https://andhs.co/provoweather&amp;quot;)

# clean dates and precip
weather_provo_2017 &amp;lt;- weather_provo_raw %&amp;gt;% 
  mutate(
    month = month(date, label = TRUE, abbr = FALSE),
    month_number = month(date, label = FALSE),
    weekday = wday(date, label = TRUE, abbr = FALSE),
    weekday_number = wday(date, label = FALSE),
    precipType = ifelse(is.na(precipType), &amp;quot;none&amp;quot;, precipType)
  ) %&amp;gt;% 
  select(
    date, month, month_number, weekday, weekday_number,
    sunriseTime, sunsetTime, moonPhase, 
    precipProbability, precipType, temperatureHigh, temperatureLow, dewPoint, 
    humidity, pressure, windSpeed, cloudCover, visibility, uvIndex
  )

# keep winter and spring, scale vars
winter_spring &amp;lt;- weather_provo_2017 %&amp;gt;% 
  filter(month_number &amp;lt;= 5) %&amp;gt;% 
  mutate(month = factor(month, ordered = FALSE)) %&amp;gt;% 
  mutate(
    humidity = humidity * 100, 
    cloudCover = cloudCover * 100, 
    precipProbability = precipProbability * 100
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We combine several model formulas into a data frame and estimate each regression using &lt;code&gt;purrr::map()&lt;/code&gt;. We’ve added the results from &lt;code&gt;broom::augment()&lt;/code&gt; because we want the residuals from each model to create the “noise” in the data for the graphic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ---- mike decrescenzo modifications begin -----------------------

# Run all these models in one data frame (purrr::map)
# add the data as a list column because we&amp;#39;ll want it later
models &amp;lt;- 
  tribble(
    ~formula,
    &amp;quot;temperatureHigh ~ humidity&amp;quot;,
    &amp;quot;temperatureHigh ~ humidity + windSpeed&amp;quot;,
    &amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover&amp;quot;,
    &amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability&amp;quot;,
    &amp;quot;temperatureHigh ~ humidity + windSpeed + cloudCover + precipProbability + visibility&amp;quot;) %&amp;gt;%
  # data in a list column
  mutate(spring_data = list(winter_spring)) %&amp;gt;%
  # Run a model in each row
  mutate(model = map2(formula, spring_data, ~ lm(.x, data = .y))) %&amp;gt;%
  # Extract model elements
  mutate(
    model_tidy = map(model, tidy, conf.int = TRUE), 
    model_glance = map(model, glance), 
    model_fits = map(model, augment)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We calculate the impact of humidity on the high temperature by extracting the humidity coefficient from each model and multiplying it by the raw humidity data (which comes in the &lt;code&gt;augment&lt;/code&gt; results). We will lazily refer to this as the humidity’s “partial prediction” of temperature (thanks to &lt;a href=&#34;https://twitter.com/EvaMaeRey&#34;&gt;Gina Reynolds&lt;/a&gt; for feedback on what this should be called). As a bonus, we will also save the upper and lower bounds of the humidity beta confidence interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute the partial effect of humidity: beta * humidity
model_partials &amp;lt;- models %&amp;gt;%
  # get the humidity beta and bounds
  mutate(
    humidity_beta = 
      map(model_tidy, ~ filter(.x, term == &amp;quot;humidity&amp;quot;)$estimate) %&amp;gt;% 
                        as.numeric(),
    beta_low = 
      map(model_tidy, ~ filter(.x, term == &amp;quot;humidity&amp;quot;)$conf.low) %&amp;gt;% 
                        as.numeric(),
    beta_high = 
      map(model_tidy, ~ filter(.x, term == &amp;quot;humidity&amp;quot;)$conf.high) %&amp;gt;%
                        as.numeric()
  ) %&amp;gt;% 
  # calculate partial effect of humidity and keep the residual
  unnest(model_fits) %&amp;gt;%
  mutate(partial = humidity * humidity_beta) %&amp;gt;%
  select(formula, humidity, contains(&amp;quot;beta&amp;quot;), partial, .resid) 

# get the beta for label plotting
model_beta &amp;lt;- model_partials %&amp;gt;%
  select(formula, contains(&amp;quot;beta&amp;quot;)) %&amp;gt;%
  distinct() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create the figure. The horizontal axis is the raw humidity data. The vertical axis is the humidity effect (&lt;span class=&#34;math inline&#34;&gt;\(\beta \times \mathit{Humidity}_{i}\)&lt;/span&gt;) plus the regression residual &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt;. The regression line is simply &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathit{High}}_{i} = \hat{\beta}\mathit{Humidity}_{i}\)&lt;/span&gt; with a constant of zero. That is, on a given day, a humidity level of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; exerts a negative impact on temperature amounting to &lt;span class=&#34;math inline&#34;&gt;\(\beta z\)&lt;/span&gt;, setting other factors aside. Conveniently, we don’t have to manually subtract the other covariates because we already know how to calculate the vertical axis using the partial effect and the residual (thanks to the math above).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# animate 
ggplot(data = model_partials, aes(x = humidity, y = partial + .resid)) +
  geom_point(color = &amp;quot;gray&amp;quot;) +
  geom_abline(
    data = model_beta, 
    aes(intercept = 0, slope = humidity_beta, group = formula)
  ) +
  geom_abline(
    data = model_beta, 
    aes(intercept = 0, slope = beta_low, group = formula), 
    linetype = 3
  ) +
  geom_abline(
    data = model_beta, 
    aes(intercept = 0, slope = beta_high, group = formula), 
    linetype = 3
    ) +
  theme_minimal(base_family = &amp;quot;Fira Sans&amp;quot;) +
  geom_label(
    data = model_beta, 
    aes(x = 35, y = -70, 
        label = paste0(&amp;quot;beta: &amp;quot;, round(humidity_beta, 3)), 
        group = formula), 
    parse = TRUE, 
    family = &amp;quot;Fira Sans&amp;quot;, 
    size = 4) +
  labs(
    x = &amp;quot;Humidity&amp;quot;, 
    y = &amp;quot;Partial Predicted High Temperature (plus residual, °F)&amp;quot;, 
    subtitle = &amp;quot;{closest_state}&amp;quot;
  ) +
  transition_states(formula, transition_length = 0.25, state_length = 0.5) +
  enter_fade() +
  ease_aes(&amp;#39;sine-in-out&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/partialling-out/index_files/figure-html/animated-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Two notes about the confidence interval&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The confidence intervals here don’t look like the ordinary hourglass-shaped intervals in linear regression. This is because the hourglass shape comes from uncertainty in both the constant and coefficients. However, the constant has been subtracted out of these predictions, so uncertainty in this visualization only reflects uncertainty in the humidity effect.&lt;/li&gt;
&lt;li&gt;I would show confidence intervals with &lt;code&gt;geom_ribbon()&lt;/code&gt;, except I can’t get ribbons to animate because of some weird stuff that’s interfering with &lt;a href=&#34;https://github.com/thomasp85/transformr&#34;&gt;transformr&lt;/a&gt; during animation.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;thats-it&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;That’s it&lt;/h1&gt;
&lt;p&gt;I don’t have comments enabled on the website but get at me on &lt;a href=&#34;https://twitter.com/mikedecr&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Maybe it’s a dev version thing, but &lt;code&gt;scales::degree_format()&lt;/code&gt; doesn’t work for me, so I removed it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
